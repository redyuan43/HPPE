#!/usr/bin/env python3
"""
LLM å¼•æ“æ€§èƒ½åŸºå‡†æµ‹è¯•

æµ‹è¯•å†…å®¹ï¼š
1. æ¨ç†å»¶è¿Ÿï¼ˆP50/P95/P99ï¼‰
2. GPUå†…å­˜å ç”¨
3. ååé‡ï¼ˆRPSï¼‰
4. æ‰¹å¤„ç†æ€§èƒ½
"""

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # ä½¿ç”¨GPU1é¿å…ä¸è®­ç»ƒå†²çª

import sys
from pathlib import Path
import time
import statistics
import json
from typing import List, Dict
from datetime import datetime

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "src"))

import pytest
from hppe.engines.llm import QwenFineTunedEngine
from hppe.engines.llm.recognizers import FineTunedLLMRecognizer

# GPUç›‘æ§
try:
    import pynvml
    HAS_NVML = True
except ImportError:
    HAS_NVML = False
    print("âš ï¸  pynvml not installed. GPU monitoring disabled.")


class GPUMonitor:
    """GPUå†…å­˜å’Œåˆ©ç”¨ç‡ç›‘æ§"""

    def __init__(self):
        self.enabled = HAS_NVML
        if self.enabled:
            pynvml.nvmlInit()
            # è·å–GPU1ï¼ˆCUDA_VISIBLE_DEVICES=1ä¼šæ˜ å°„ä¸ºè®¾å¤‡0ï¼‰
            self.handle = pynvml.nvmlDeviceGetHandleByIndex(1)

    def get_memory_usage(self) -> Dict:
        """è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if not self.enabled:
            return {"used_mb": 0, "total_mb": 0, "utilization_percent": 0}

        mem_info = pynvml.nvmlDeviceGetMemoryInfo(self.handle)
        return {
            "used_mb": mem_info.used / 1024 / 1024,
            "total_mb": mem_info.total / 1024 / 1024,
            "utilization_percent": (mem_info.used / mem_info.total) * 100
        }

    def get_gpu_utilization(self) -> int:
        """è·å–GPUè®¡ç®—åˆ©ç”¨ç‡"""
        if not self.enabled:
            return 0

        util = pynvml.nvmlDeviceGetUtilizationRates(self.handle)
        return util.gpu

    def __del__(self):
        if self.enabled:
            pynvml.nvmlShutdown()


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""

    def __init__(self, model_path: str, device: str = "cuda"):
        """
        åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•

        Args:
            model_path: æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ï¼ˆcuda/cpuï¼‰
        """
        self.model_path = model_path
        self.device = device
        self.engine = None
        self.recognizer = None
        self.gpu_monitor = GPUMonitor()

        # æµ‹è¯•æ–‡æœ¬é›†åˆï¼ˆä¸åŒé•¿åº¦ï¼‰
        self.test_texts = self._prepare_test_texts()

    def _prepare_test_texts(self) -> Dict[str, List[str]]:
        """å‡†å¤‡ä¸åŒé•¿åº¦çš„æµ‹è¯•æ–‡æœ¬"""
        return {
            "short": [
                "æˆ‘æ˜¯å¼ ä¸‰",
                "ç”µè¯13812345678",
                "é‚®ç®±test@example.com",
                "åŒ—äº¬å¸‚æµ·æ·€åŒº",
                "é˜¿é‡Œå·´å·´å…¬å¸",
            ],
            "medium": [
                "æˆ‘æ˜¯å¼ ä¸‰ï¼Œç”µè¯13812345678ï¼Œé‚®ç®±zhangsan@example.com",
                "è”ç³»äººï¼šæå››ï¼Œæ‰‹æœºï¼š13900139000ï¼Œå·¥ä½œå•ä½ï¼šè…¾è®¯ç§‘æŠ€",
                "æ”¶ä»¶åœ°å€ï¼šä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒºé™†å®¶å˜´ç¯è·¯1000å·",
                "ç‹äº”å…ˆç”Ÿå±…ä½åœ¨å¹¿å·å¸‚å¤©æ²³åŒºç æ±Ÿæ–°åŸ",
                "å®¢æˆ·ä¿¡æ¯ï¼šèµµå…­ï¼Œèº«ä»½è¯å·ï¼š110101199001011234",
            ],
            "long": [
                "å°Šæ•¬çš„å¼ ä¸‰å…ˆç”Ÿï¼Œæ‚¨å¥½ï¼æˆ‘ä»¬æ˜¯é˜¿é‡Œå·´å·´å…¬å¸ï¼Œåœ°å€ä½äºæ­å·å¸‚ä½™æ­åŒºæ–‡ä¸€è¥¿è·¯969å·ã€‚"
                "æ‚¨çš„è®¢å•å·²å‘è´§ï¼Œå¿«é€’å•å·ï¼šSF1234567890ï¼Œæ”¶è´§åœ°å€ä¸ºåŒ—äº¬å¸‚æœé˜³åŒºå»ºå›½è·¯88å·SOHOç°ä»£åŸã€‚"
                "å¦‚æœ‰é—®é¢˜è¯·è”ç³»å®¢æœï¼Œç”µè¯ï¼š400-123-4567ï¼Œé‚®ç®±ï¼šservice@example.comã€‚",

                "å®¢æˆ·æ¡£æ¡ˆï¼šæå››ï¼Œæ€§åˆ«ç”·ï¼Œèº«ä»½è¯å·ï¼š310101198506151234ï¼Œæ‰‹æœºï¼š13900139000ï¼Œ"
                "é‚®ç®±ï¼šlisi@company.comï¼Œå…¬å¸ï¼šè…¾è®¯ç§‘æŠ€æœ‰é™å…¬å¸ï¼Œåœ°å€ï¼šæ·±åœ³å¸‚å—å±±åŒºç§‘æŠ€å›­ã€‚"
                "ç´§æ€¥è”ç³»äººï¼šç‹äº”ï¼Œå…³ç³»ï¼šé…å¶ï¼Œç”µè¯ï¼š13800138000ã€‚",

                "ä¼šè®®é€šçŸ¥ï¼šå®šäº2025å¹´10æœˆ20æ—¥ä¸Šåˆ10:00åœ¨ä¸Šæµ·å¸‚é»„æµ¦åŒºäººæ°‘å¹¿åœº100å·ä¼šè®®å®¤å¬å¼€ã€‚"
                "å‚ä¼šäººå‘˜ï¼šå¼ ä¸‰ï¼ˆé˜¿é‡Œå·´å·´ï¼‰ã€æå››ï¼ˆè…¾è®¯ï¼‰ã€ç‹äº”ï¼ˆå­—èŠ‚è·³åŠ¨ï¼‰ã€‚"
                "ä¼šè®®ä¸»é¢˜ï¼šéšç§ä¿æŠ¤æŠ€æœ¯ç ”è®¨ã€‚è”ç³»äººï¼šèµµå…­ï¼Œç”µè¯ï¼š021-12345678ã€‚",
            ]
        }

    def setup(self):
        """åˆå§‹åŒ–å¼•æ“ï¼ˆæµ‹é‡åŠ è½½æ—¶é—´ï¼‰"""
        print("\nğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        start_time = time.time()

        self.engine = QwenFineTunedEngine(
            model_path=self.model_path,
            device=self.device,
            load_in_4bit=True
        )

        self.recognizer = FineTunedLLMRecognizer(
            llm_engine=self.engine,
            confidence_threshold=0.75
        )

        load_time = time.time() - start_time

        # é¢„çƒ­ï¼ˆé¦–æ¬¡æ¨ç†è¾ƒæ…¢ï¼‰
        print("ğŸ”¥ é¢„çƒ­æ¨¡å‹...")
        warmup_start = time.time()
        _ = self.recognizer.detect("æµ‹è¯•æ–‡æœ¬")
        warmup_time = time.time() - warmup_start

        mem_usage = self.gpu_monitor.get_memory_usage()

        return {
            "load_time_sec": load_time,
            "warmup_time_sec": warmup_time,
            "gpu_memory_mb": mem_usage["used_mb"],
            "gpu_memory_percent": mem_usage["utilization_percent"]
        }

    def benchmark_latency(self, text_category: str = "medium", num_runs: int = 20) -> Dict:
        """
        æµ‹è¯•æ¨ç†å»¶è¿Ÿ

        Args:
            text_category: æ–‡æœ¬ç±»åˆ«ï¼ˆshort/medium/longï¼‰
            num_runs: è¿è¡Œæ¬¡æ•°

        Returns:
            å»¶è¿Ÿç»Ÿè®¡æ•°æ®
        """
        texts = self.test_texts[text_category]
        latencies = []

        print(f"\nâ±ï¸  æµ‹è¯•å»¶è¿Ÿï¼ˆ{text_category}æ–‡æœ¬ï¼Œ{num_runs}æ¬¡è¿è¡Œï¼‰...")

        for i in range(num_runs):
            text = texts[i % len(texts)]

            start_time = time.perf_counter()
            _ = self.recognizer.detect(text)
            end_time = time.perf_counter()

            latency_ms = (end_time - start_time) * 1000
            latencies.append(latency_ms)

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        latencies_sorted = sorted(latencies)

        return {
            "text_category": text_category,
            "num_runs": num_runs,
            "min_ms": min(latencies),
            "max_ms": max(latencies),
            "mean_ms": statistics.mean(latencies),
            "median_ms": statistics.median(latencies),
            "p50_ms": latencies_sorted[int(len(latencies) * 0.50)],
            "p95_ms": latencies_sorted[int(len(latencies) * 0.95)],
            "p99_ms": latencies_sorted[int(len(latencies) * 0.99)],
            "stdev_ms": statistics.stdev(latencies) if len(latencies) > 1 else 0,
            "all_latencies": latencies
        }

    def benchmark_throughput(self, duration_sec: int = 30, text_category: str = "medium") -> Dict:
        """
        æµ‹è¯•ååé‡ï¼ˆRPSï¼‰

        Args:
            duration_sec: æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
            text_category: æ–‡æœ¬ç±»åˆ«

        Returns:
            ååé‡æ•°æ®
        """
        texts = self.test_texts[text_category]

        print(f"\nğŸ“Š æµ‹è¯•ååé‡ï¼ˆæŒç»­{duration_sec}ç§’ï¼‰...")

        start_time = time.time()
        end_time = start_time + duration_sec

        request_count = 0
        total_entities = 0
        gpu_utils = []

        while time.time() < end_time:
            text = texts[request_count % len(texts)]
            entities = self.recognizer.detect(text)

            request_count += 1
            total_entities += len(entities)

            # é‡‡æ ·GPUåˆ©ç”¨ç‡
            if request_count % 5 == 0:
                gpu_utils.append(self.gpu_monitor.get_gpu_utilization())

        actual_duration = time.time() - start_time
        rps = request_count / actual_duration

        return {
            "duration_sec": actual_duration,
            "total_requests": request_count,
            "total_entities_detected": total_entities,
            "requests_per_second": rps,
            "avg_gpu_utilization": statistics.mean(gpu_utils) if gpu_utils else 0
        }

    def benchmark_memory(self) -> Dict:
        """
        æµ‹è¯•å†…å­˜å ç”¨

        Returns:
            å†…å­˜ä½¿ç”¨æ•°æ®
        """
        print("\nğŸ’¾ æµ‹è¯•å†…å­˜å ç”¨...")

        # ç©ºé—²çŠ¶æ€
        mem_idle = self.gpu_monitor.get_memory_usage()

        # å•æ¬¡æ¨ç†
        _ = self.recognizer.detect(self.test_texts["medium"][0])
        mem_single = self.gpu_monitor.get_memory_usage()

        # æ‰¹é‡æ¨ç†
        for text in self.test_texts["medium"]:
            _ = self.recognizer.detect(text)
        mem_batch = self.gpu_monitor.get_memory_usage()

        return {
            "idle": mem_idle,
            "single_inference": mem_single,
            "batch_inference": mem_batch
        }

    def run_full_benchmark(self) -> Dict:
        """
        è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•

        Returns:
            å®Œæ•´çš„æµ‹è¯•ç»“æœ
        """
        print("=" * 70)
        print("ğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 70)

        # 1. æ¨¡å‹åŠ è½½å’Œåˆå§‹åŒ–
        setup_results = self.setup()

        # 2. å»¶è¿Ÿæµ‹è¯•ï¼ˆä¸åŒæ–‡æœ¬é•¿åº¦ï¼‰
        latency_results = {}
        for category in ["short", "medium", "long"]:
            latency_results[category] = self.benchmark_latency(
                text_category=category,
                num_runs=20
            )

        # 3. ååé‡æµ‹è¯•
        throughput_results = self.benchmark_throughput(duration_sec=30)

        # 4. å†…å­˜æµ‹è¯•
        memory_results = self.benchmark_memory()

        # æ±‡æ€»ç»“æœ
        results = {
            "timestamp": datetime.now().isoformat(),
            "model_path": self.model_path,
            "device": self.device,
            "setup": setup_results,
            "latency": latency_results,
            "throughput": throughput_results,
            "memory": memory_results,
        }

        # æ‰“å°æ‘˜è¦
        self._print_summary(results)

        return results

    def _print_summary(self, results: Dict):
        """æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦"""
        print("\n" + "=" * 70)
        print("ğŸ“ˆ æ€§èƒ½æµ‹è¯•ç»“æœæ‘˜è¦")
        print("=" * 70)

        # åˆå§‹åŒ–
        setup = results["setup"]
        print(f"\nğŸ”§ æ¨¡å‹åŠ è½½:")
        print(f"  åŠ è½½æ—¶é—´: {setup['load_time_sec']:.2f}ç§’")
        print(f"  é¢„çƒ­æ—¶é—´: {setup['warmup_time_sec']:.2f}ç§’")
        print(f"  GPUå†…å­˜: {setup['gpu_memory_mb']:.0f}MB ({setup['gpu_memory_percent']:.1f}%)")

        # å»¶è¿Ÿ
        print(f"\nâ±ï¸  æ¨ç†å»¶è¿Ÿ:")
        for category, data in results["latency"].items():
            print(f"  {category.upper()} æ–‡æœ¬:")
            print(f"    P50: {data['p50_ms']:.1f}ms")
            print(f"    P95: {data['p95_ms']:.1f}ms")
            print(f"    P99: {data['p99_ms']:.1f}ms")
            print(f"    å¹³å‡: {data['mean_ms']:.1f}ms Â± {data['stdev_ms']:.1f}ms")

        # ååé‡
        throughput = results["throughput"]
        print(f"\nğŸ“Š ååé‡:")
        print(f"  RPS: {throughput['requests_per_second']:.2f}")
        print(f"  æ€»è¯·æ±‚æ•°: {throughput['total_requests']}")
        print(f"  æ£€æµ‹å®ä½“æ•°: {throughput['total_entities_detected']}")
        print(f"  å¹³å‡GPUåˆ©ç”¨ç‡: {throughput['avg_gpu_utilization']:.1f}%")

        # å†…å­˜
        memory = results["memory"]
        print(f"\nğŸ’¾ å†…å­˜å ç”¨:")
        print(f"  ç©ºé—²: {memory['idle']['used_mb']:.0f}MB")
        print(f"  å•æ¬¡æ¨ç†: {memory['single_inference']['used_mb']:.0f}MB")
        print(f"  æ‰¹é‡æ¨ç†: {memory['batch_inference']['used_mb']:.0f}MB")

        print("\n" + "=" * 70)


# ==================== Pytest æµ‹è¯•ç”¨ä¾‹ ====================

@pytest.fixture(scope="module")
def benchmark():
    """åˆ›å»ºæ€§èƒ½æµ‹è¯•å®ä¾‹"""
    model_path = "models/pii_qwen4b_unsloth/final"
    bench = PerformanceBenchmark(model_path=model_path, device="cuda")
    return bench


@pytest.mark.benchmark
def test_model_loading_time(benchmark):
    """æµ‹è¯•æ¨¡å‹åŠ è½½æ—¶é—´"""
    results = benchmark.setup()

    # éªŒè¯åŠ è½½æ—¶é—´ < 60ç§’
    assert results["load_time_sec"] < 60, f"æ¨¡å‹åŠ è½½æ—¶é—´è¿‡é•¿: {results['load_time_sec']:.2f}s"

    # éªŒè¯GPUå†…å­˜å ç”¨ < 8GB
    assert results["gpu_memory_mb"] < 8192, f"GPUå†…å­˜å ç”¨è¿‡é«˜: {results['gpu_memory_mb']:.0f}MB"

    print(f"âœ… æ¨¡å‹åŠ è½½: {results['load_time_sec']:.2f}s, GPU: {results['gpu_memory_mb']:.0f}MB")


@pytest.mark.benchmark
def test_inference_latency_short(benchmark):
    """æµ‹è¯•çŸ­æ–‡æœ¬æ¨ç†å»¶è¿Ÿ"""
    if benchmark.engine is None:
        benchmark.setup()

    results = benchmark.benchmark_latency(text_category="short", num_runs=20)

    # éªŒè¯P50å»¶è¿Ÿ < 200ms
    assert results["p50_ms"] < 200, f"P50å»¶è¿Ÿè¿‡é«˜: {results['p50_ms']:.1f}ms"

    # éªŒè¯P99å»¶è¿Ÿ < 500ms
    assert results["p99_ms"] < 500, f"P99å»¶è¿Ÿè¿‡é«˜: {results['p99_ms']:.1f}ms"

    print(f"âœ… çŸ­æ–‡æœ¬å»¶è¿Ÿ: P50={results['p50_ms']:.1f}ms, P99={results['p99_ms']:.1f}ms")


@pytest.mark.benchmark
def test_inference_latency_medium(benchmark):
    """æµ‹è¯•ä¸­ç­‰æ–‡æœ¬æ¨ç†å»¶è¿Ÿ"""
    if benchmark.engine is None:
        benchmark.setup()

    results = benchmark.benchmark_latency(text_category="medium", num_runs=20)

    # éªŒè¯P50å»¶è¿Ÿ < 300ms
    assert results["p50_ms"] < 300, f"P50å»¶è¿Ÿè¿‡é«˜: {results['p50_ms']:.1f}ms"

    print(f"âœ… ä¸­ç­‰æ–‡æœ¬å»¶è¿Ÿ: P50={results['p50_ms']:.1f}ms, P99={results['p99_ms']:.1f}ms")


@pytest.mark.benchmark
def test_throughput(benchmark):
    """æµ‹è¯•ååé‡"""
    if benchmark.engine is None:
        benchmark.setup()

    results = benchmark.benchmark_throughput(duration_sec=15)

    # éªŒè¯ååé‡ > 5 RPS
    assert results["requests_per_second"] > 5, f"ååé‡è¿‡ä½: {results['requests_per_second']:.2f} RPS"

    print(f"âœ… ååé‡: {results['requests_per_second']:.2f} RPS")


# ==================== å‘½ä»¤è¡Œè¿è¡Œ ====================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="LLMå¼•æ“æ€§èƒ½åŸºå‡†æµ‹è¯•")
    parser.add_argument(
        "--model-path",
        default="models/pii_qwen4b_unsloth/final",
        help="æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--output",
        default="benchmark_results.json",
        help="ç»“æœè¾“å‡ºæ–‡ä»¶"
    )
    parser.add_argument(
        "--duration",
        type=int,
        default=30,
        help="ååé‡æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰"
    )

    args = parser.parse_args()

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark = PerformanceBenchmark(model_path=args.model_path)
    results = benchmark.run_full_benchmark()

    # ä¿å­˜ç»“æœ
    output_path = Path(args.output)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print("\nâœ… åŸºå‡†æµ‹è¯•å®Œæˆï¼")
