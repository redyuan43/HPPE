#!/usr/bin/env python3
"""
åˆ†ææ¼æ£€æ ·æœ¬ï¼Œæ‰¾å‡ºRecallä½çš„æ ¹æœ¬åŸå› 
"""
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from collections import defaultdict

MODEL_PATH = "models/pii_qwen4b_unsloth/checkpoint-1562"
BASE_MODEL = "/home/ivan/.cache/modelscope/hub/Qwen/Qwen3-4B"
TEST_DATA = "data/merged_pii_dataset_test.jsonl"
SAMPLE_SIZE = 100  # å¢åŠ æ ·æœ¬é‡

print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
    device_map="auto",
)
model = PeftModel.from_pretrained(model, MODEL_PATH)
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")

# åŠ è½½æµ‹è¯•æ•°æ®
test_samples = []
with open(TEST_DATA, 'r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        if i >= SAMPLE_SIZE:
            break
        test_samples.append(json.loads(line))

def extract_entities(text):
    try:
        start_idx = text.find('{"entities"')
        if start_idx == -1:
            return []
        end_marker = '<|im_end|>'
        end_idx = text.find(end_marker, start_idx)
        if end_idx == -1:
            json_str = text[start_idx:].strip()
        else:
            json_str = text[start_idx:end_idx].strip()
        result = json.loads(json_str)
        return result.get("entities", [])
    except:
        return []

# ç»Ÿè®¡æ¼æ£€æƒ…å†µ
missed_by_type = defaultdict(lambda: {"total": 0, "missed": 0, "examples": []})
total_fn = 0
total_entities = 0

print("ğŸ” åˆ†ææ¼æ£€æ¨¡å¼...")
for idx, sample in enumerate(test_samples):
    input_text = sample["input"]
    expected_entities = sample.get("output", {}).get("entities", [])
    
    # æ¨ç†
    prompt = (
        f"<|im_start|>system\n"
        f"ä½ æ˜¯ PII æ£€æµ‹ä¸“å®¶ã€‚æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚<|im_end|>\n"
        f"<|im_start|>user\n{input_text}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )
    
    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    predicted_entities = extract_entities(response)
    
    # åˆ†ææ¼æ£€
    expected_set = set((e["type"], e["value"]) for e in expected_entities)
    predicted_set = set((e["type"], e["value"]) for e in predicted_entities)
    
    missed = expected_set - predicted_set
    
    for entity_type, entity_value in missed:
        missed_by_type[entity_type]["missed"] += 1
        if len(missed_by_type[entity_type]["examples"]) < 3:
            missed_by_type[entity_type]["examples"].append({
                "text": input_text[:100],
                "missed_value": entity_value
            })
    
    for entity in expected_entities:
        missed_by_type[entity["type"]]["total"] += 1
        total_entities += 1
    
    total_fn += len(missed)
    
    if (idx + 1) % 10 == 0:
        print(f"  å·²å¤„ç† {idx+1}/{SAMPLE_SIZE} æ ·æœ¬...")

# è¾“å‡ºåˆ†ææŠ¥å‘Š
print(f"\n{'='*70}")
print(f"ğŸ“Š æ¼æ£€åˆ†ææŠ¥å‘Š (åŸºäº{SAMPLE_SIZE}æ ·æœ¬)")
print(f"{'='*70}")
print(f"æ€»å®ä½“æ•°: {total_entities}")
print(f"æ€»æ¼æ£€æ•°: {total_fn}")
print(f"æ€»ä½“Recall: {(total_entities - total_fn) / total_entities * 100:.2f}%\n")

print(f"{'ç±»å‹':<15} {'æ€»æ•°':>6} {'æ¼æ£€':>6} {'Recall':>8} {'æ¼æ£€ç‡':>8}")
print(f"{'-'*70}")

sorted_types = sorted(missed_by_type.items(), 
                      key=lambda x: x[1]["missed"] / max(x[1]["total"], 1), 
                      reverse=True)

for entity_type, stats in sorted_types:
    total = stats["total"]
    missed = stats["missed"]
    recall = (total - missed) / total * 100 if total > 0 else 0
    miss_rate = missed / total * 100 if total > 0 else 0
    
    print(f"{entity_type:<15} {total:>6} {missed:>6} {recall:>7.1f}% {miss_rate:>7.1f}%")
    
    if stats["examples"]:
        print(f"  æ¼æ£€ç¤ºä¾‹:")
        for ex in stats["examples"][:2]:
            print(f"    - æ–‡æœ¬: {ex['text']}...")
            print(f"      æ¼æ£€: {ex['missed_value']}")

print(f"\n{'='*70}")
print(f"ğŸ’¡ å…³é”®å‘ç°")
print(f"{'='*70}")

# æ‰¾å‡ºRecallæœ€ä½çš„3ä¸ªç±»å‹
worst_types = sorted_types[:3]
print(f"\nRecallæœ€ä½çš„PIIç±»å‹:")
for entity_type, stats in worst_types:
    total = stats["total"]
    missed = stats["missed"]
    recall = (total - missed) / total * 100 if total > 0 else 0
    print(f"  {entity_type}: Recall {recall:.1f}% (æ¼æ£€{missed}/{total})")

# ä¿å­˜è¯¦ç»†ç»“æœ
with open("logs/missed_cases_analysis.json", 'w', encoding='utf-8') as f:
    json.dump({
        "total_entities": total_entities,
        "total_missed": total_fn,
        "overall_recall": (total_entities - total_fn) / total_entities,
        "by_type": {k: v for k, v in missed_by_type.items()}
    }, f, indent=2, ensure_ascii=False)

print(f"\nâœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: logs/missed_cases_analysis.json")
