#!/usr/bin/env python3
"""
GPU1è®­ç»ƒè„šæœ¬ - 5ç§PII
VEHICLE_PLATE, DRIVER_LICENSE, SOCIAL_SECURITY_CARD, UNIFIED_SOCIAL_CREDIT_CODE, IPV6_ADDRESS
"""
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # å¼ºåˆ¶ä½¿ç”¨GPU1

import json
import sys
from pathlib import Path
from datasets import Dataset
from transformers import TrainingArguments, Trainer
import torch
from datetime import datetime

print("="*70)
print("ğŸš€ GPU1 è®­ç»ƒä»»åŠ¡å¯åŠ¨")
print("="*70)
print(f"è®­ç»ƒPIIç±»å‹: VEHICLE_PLATE, DRIVER_LICENSE, SOCIAL_SECURITY_CARD, UNIFIED_SOCIAL_CREDIT_CODE, IPV6_ADDRESS")
print(f"GPU: 1")
print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*70 + "\n")

from unsloth import FastLanguageModel

# é…ç½®
MODEL_NAME = "/home/ivan/.cache/modelscope/hub/Qwen/Qwen3-4B"
TRAIN_DATA = "data/gpu1_train.jsonl"
OUTPUT_DIR = "models/pii_11new_gpu1"
MAX_SEQ_LENGTH = 512
LORA_R = 16
LORA_ALPHA = 32
BATCH_SIZE = 12
GRADIENT_ACCUMULATION = 3
EPOCHS = 2
LR = 1.5e-4

# åŠ è½½æ¨¡å‹
print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_NAME,
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=None,
    load_in_4bit=True,
)

model = FastLanguageModel.get_peft_model(
    model,
    r=LORA_R,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=LORA_ALPHA,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",
)

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")

# åŠ è½½æ•°æ®å¹¶tokenize
print(f"ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {TRAIN_DATA}")

def load_and_tokenize_jsonl(path: str):
    """åŠ è½½JSONLæ•°æ®å¹¶tokenize"""
    all_input_ids = []
    all_attention_mask = []
    all_labels = []

    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            sample = json.loads(line)

            # æå–è¾“å…¥å’Œå®ä½“
            text = sample["input"]
            entities = sample.get("output", {}).get("entities", [])

            # æ„é€ å®Œæ•´çš„è®­ç»ƒæ–‡æœ¬
            full_text = (
                f"<|im_start|>system\n"
                f"ä½ æ˜¯ PII æ£€æµ‹ä¸“å®¶ã€‚æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚<|im_end|>\n"
                f"<|im_start|>user\n{text}<|im_end|>\n"
                f"<|im_start|>assistant\n"
                f'{{"entities": {json.dumps(entities, ensure_ascii=False)}}}<|im_end|>'
            )

            # Tokenize
            tokenized = tokenizer(
                full_text,
                max_length=MAX_SEQ_LENGTH,
                truncation=True,
                padding="max_length",
                return_tensors=None,
            )

            # è®¾ç½® labels = input_ids
            tokenized["labels"] = tokenized["input_ids"].copy()

            all_input_ids.append(tokenized["input_ids"])
            all_attention_mask.append(tokenized["attention_mask"])
            all_labels.append(tokenized["labels"])

    print(f"âœ… åŠ è½½äº† {len(all_input_ids)} ä¸ªè®­ç»ƒæ ·æœ¬\n")

    return Dataset.from_dict({
        "input_ids": all_input_ids,
        "attention_mask": all_attention_mask,
        "labels": all_labels,
    })

train_dataset = load_and_tokenize_jsonl(TRAIN_DATA)

# è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    num_train_epochs=EPOCHS,
    learning_rate=LR,
    warmup_steps=100,
    logging_steps=50,
    save_strategy="epoch",
    fp16=not torch.cuda.is_bf16_supported(),
    bf16=torch.cuda.is_bf16_supported(),
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="linear",
    seed=42,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=tokenizer,
)

# å¼€å§‹è®­ç»ƒ
print("ğŸš€ å¼€å§‹è®­ç»ƒ...\n")
start_time = datetime.now()
trainer.train()
end_time = datetime.now()

# ä¿å­˜æ¨¡å‹
print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
model.save_pretrained(f"{OUTPUT_DIR}/final")
tokenizer.save_pretrained(f"{OUTPUT_DIR}/final")

duration = (end_time - start_time).total_seconds() / 60
print(f"\n{'='*70}")
print(f"âœ… GPU1 è®­ç»ƒå®Œæˆï¼")
print(f"{'='*70}")
print(f"è®­ç»ƒæ—¶é•¿: {duration:.1f} åˆ†é’Ÿ")
print(f"æ¨¡å‹ä¿å­˜è‡³: {OUTPUT_DIR}/final")
print(f"å®Œæˆæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"{'='*70}")
