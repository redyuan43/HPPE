#!/usr/bin/env python3
"""
æ¨¡å‹é›†æˆéªŒè¯ï¼šEpoch1 + Epoch2
ä½¿ç”¨æŠ•ç¥¨æœºåˆ¶æå‡Recall
"""
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # ä¿®æ”¹ä¸ºGPU0

import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from tqdm import tqdm

BASE_MODEL = "/home/ivan/.cache/modelscope/hub/Qwen/Qwen3-4B"
MODEL1_PATH = "models/pii_qwen4b_unsloth/checkpoint-781"   # Epoch 1
MODEL2_PATH = "models/pii_qwen4b_unsloth/checkpoint-1562"  # Epoch 2
TEST_DATA = "data/merged_pii_dataset_test.jsonl"
SAMPLE_SIZE = 100

print("="*70)
print("ğŸ”„ æ¨¡å‹é›†æˆéªŒè¯")
print("="*70)
print(f"æ¨¡å‹1: {MODEL1_PATH} (Epoch 1)")
print(f"æ¨¡å‹2: {MODEL2_PATH} (Epoch 2)")
print(f"ç­–ç•¥: å¹¶é›†ï¼ˆä»»ä¸€æ¨¡å‹æ£€å‡ºå³ç®—æ£€å‡ºï¼‰")
print("="*70 + "\n")

# åŠ è½½ä¸¤ä¸ªæ¨¡å‹
print("ğŸ“¦ åŠ è½½æ¨¡å‹1 (Epoch 1)...")
base_model1 = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
    device_map="auto",
)
model1 = PeftModel.from_pretrained(base_model1, MODEL1_PATH)
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
print("âœ… æ¨¡å‹1åŠ è½½å®Œæˆ\n")

print("ğŸ“¦ åŠ è½½æ¨¡å‹2 (Epoch 2)...")
base_model2 = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
    device_map="auto",
)
model2 = PeftModel.from_pretrained(base_model2, MODEL2_PATH)
print("âœ… æ¨¡å‹2åŠ è½½å®Œæˆ\n")

# åŠ è½½æµ‹è¯•æ•°æ®
test_samples = []
with open(TEST_DATA, 'r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        if i >= SAMPLE_SIZE:
            break
        test_samples.append(json.loads(line))

print(f"ğŸ“Š åŠ è½½äº† {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬\n")

def extract_entities(text):
    """æå–å®ä½“"""
    try:
        start_idx = text.find('{"entities"')
        if start_idx == -1:
            return []
        end_marker = '<|im_end|>'
        end_idx = text.find(end_marker, start_idx)
        if end_idx == -1:
            json_str = text[start_idx:].strip()
        else:
            json_str = text[start_idx:end_idx].strip()
        result = json.loads(json_str)
        return result.get("entities", [])
    except:
        return []

def predict_entities(model, tokenizer, text):
    """å•ä¸ªæ¨¡å‹é¢„æµ‹"""
    prompt = (
        f"<|im_start|>system\n"
        f"ä½ æ˜¯ PII æ£€æµ‹ä¸“å®¶ã€‚æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚<|im_end|>\n"
        f"<|im_start|>user\n{text}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )

    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    return extract_entities(response)

# æ‰§è¡Œé›†æˆéªŒè¯
print("ğŸš€ å¼€å§‹é›†æˆéªŒè¯...\n")

# å•æ¨¡å‹ç»Ÿè®¡
model1_tp = model1_fp = model1_fn = 0
model2_tp = model2_fp = model2_fn = 0
ensemble_tp = ensemble_fp = ensemble_fn = 0

for sample in tqdm(test_samples, desc="éªŒè¯è¿›åº¦"):
    input_text = sample["input"]
    expected_entities = set(
        (e["type"], e["value"])
        for e in sample.get("output", {}).get("entities", [])
    )

    # æ¨¡å‹1é¢„æµ‹
    pred1 = predict_entities(model1, tokenizer, input_text)
    pred1_set = set((e["type"], e["value"]) for e in pred1)

    # æ¨¡å‹2é¢„æµ‹
    pred2 = predict_entities(model2, tokenizer, input_text)
    pred2_set = set((e["type"], e["value"]) for e in pred2)

    # é›†æˆé¢„æµ‹ï¼ˆå¹¶é›†ï¼‰
    ensemble_set = pred1_set | pred2_set

    # ç»Ÿè®¡æ¨¡å‹1
    model1_tp += len(expected_entities & pred1_set)
    model1_fp += len(pred1_set - expected_entities)
    model1_fn += len(expected_entities - pred1_set)

    # ç»Ÿè®¡æ¨¡å‹2
    model2_tp += len(expected_entities & pred2_set)
    model2_fp += len(pred2_set - expected_entities)
    model2_fn += len(expected_entities - pred2_set)

    # ç»Ÿè®¡é›†æˆ
    ensemble_tp += len(expected_entities & ensemble_set)
    ensemble_fp += len(ensemble_set - expected_entities)
    ensemble_fn += len(expected_entities - ensemble_set)

# è®¡ç®—æŒ‡æ ‡
def calc_metrics(tp, fp, fn):
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    return precision, recall, f1

model1_p, model1_r, model1_f1 = calc_metrics(model1_tp, model1_fp, model1_fn)
model2_p, model2_r, model2_f1 = calc_metrics(model2_tp, model2_fp, model2_fn)
ensemble_p, ensemble_r, ensemble_f1 = calc_metrics(ensemble_tp, ensemble_fp, ensemble_fn)

# è¾“å‡ºç»“æœ
print(f"\n{'='*70}")
print(f"ğŸ“Š é›†æˆéªŒè¯ç»“æœ (åŸºäº {SAMPLE_SIZE} æ ·æœ¬)")
print(f"{'='*70}\n")

print(f"{'æ¨¡å‹':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}")
print(f"{'-'*70}")
print(f"{'Epoch 1':<15} {model1_p*100:<11.2f}% {model1_r*100:<11.2f}% {model1_f1*100:<11.2f}%")
print(f"{'Epoch 2':<15} {model2_p*100:<11.2f}% {model2_r*100:<11.2f}% {model2_f1*100:<11.2f}%")
print(f"{'-'*70}")
print(f"{'ğŸ”„ é›†æˆæ¨¡å‹':<15} {ensemble_p*100:<11.2f}% {ensemble_r*100:<11.2f}% {ensemble_f1*100:<11.2f}%")
print(f"{'='*70}\n")

print(f"ğŸ“ˆ é›†æˆæ•ˆæœæå‡:")
print(f"  Precision: {model2_p*100:.2f}% â†’ {ensemble_p*100:.2f}% ({(ensemble_p-model2_p)*100:+.2f}%)")
print(f"  Recall:    {model2_r*100:.2f}% â†’ {ensemble_r*100:.2f}% ({(ensemble_r-model2_r)*100:+.2f}%)")
print(f"  F1-Score:  {model2_f1*100:.2f}% â†’ {ensemble_f1*100:.2f}% ({(ensemble_f1-model2_f1)*100:+.2f}%)")

print(f"\nè¯¦ç»†ç»Ÿè®¡:")
print(f"  é›†æˆæ¨¡å‹ - TP: {ensemble_tp}, FP: {ensemble_fp}, FN: {ensemble_fn}")

# ä¿å­˜ç»“æœ
result = {
    "sample_size": SAMPLE_SIZE,
    "model1": {
        "path": MODEL1_PATH,
        "precision": model1_p,
        "recall": model1_r,
        "f1": model1_f1,
        "tp": model1_tp,
        "fp": model1_fp,
        "fn": model1_fn
    },
    "model2": {
        "path": MODEL2_PATH,
        "precision": model2_p,
        "recall": model2_r,
        "f1": model2_f1,
        "tp": model2_tp,
        "fp": model2_fp,
        "fn": model2_fn
    },
    "ensemble": {
        "precision": ensemble_p,
        "recall": ensemble_r,
        "f1": ensemble_f1,
        "tp": ensemble_tp,
        "fp": ensemble_fp,
        "fn": ensemble_fn
    }
}

with open("logs/ensemble_validation_result.json", 'w', encoding='utf-8') as f:
    json.dump(result, f, indent=2, ensure_ascii=False)

print(f"\nâœ… ç»“æœå·²ä¿å­˜è‡³: logs/ensemble_validation_result.json")

# åˆ¤æ–­æ˜¯å¦è¾¾æ ‡
if ensemble_r >= 0.90:
    print(f"\nğŸ‰ æ­å–œï¼é›†æˆæ¨¡å‹Recallå·²è¾¾æ ‡: {ensemble_r*100:.2f}% â‰¥ 90%")
else:
    gap = 0.90 - ensemble_r
    print(f"\nâš ï¸ è·ç¦»ç›®æ ‡è¿˜å·®: {gap*100:.2f}%")
    print(f"å»ºè®®: ç»§ç»­æ‰§è¡Œæ•°æ®å¢å¼ºç­–ç•¥")
