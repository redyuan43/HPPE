#!/usr/bin/env python3
"""
6ç§PIIæ¨¡å‹ vs 17ç§PIIæ¨¡å‹å¯¹æ¯”éªŒè¯

åŠŸèƒ½ï¼š
1. åŠ è½½6ç§å’Œ17ç§PIIæ¨¡å‹
2. åœ¨ç›¸åŒæµ‹è¯•é›†ä¸Šè¿è¡Œ
3. è®¡ç®—Precision/Recall/F1
4. ç”Ÿæˆè¯¦ç»†å¯¹æ¯”æŠ¥å‘Š
"""

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # ä½¿ç”¨GPU1

import sys
from pathlib import Path
import json
from typing import List, Dict, Set, Tuple
from collections import defaultdict
from datetime import datetime

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from hppe.engines.llm import QwenFineTunedEngine
from hppe.engines.llm.recognizers import FineTunedLLMRecognizer
from hppe.models.entity import Entity


class ModelComparator:
    """æ¨¡å‹å¯¹æ¯”éªŒè¯å™¨"""

    def __init__(
        self,
        model_6pii_path: str,
        model_17pii_path: str,
        test_data_path: str
    ):
        """
        åˆå§‹åŒ–å¯¹æ¯”éªŒè¯å™¨

        Args:
            model_6pii_path: 6ç§PIIæ¨¡å‹è·¯å¾„
            model_17pii_path: 17ç§PIIæ¨¡å‹è·¯å¾„
            test_data_path: æµ‹è¯•æ•°æ®è·¯å¾„
        """
        self.model_6pii_path = model_6pii_path
        self.model_17pii_path = model_17pii_path
        self.test_data_path = test_data_path

        self.engine_6pii = None
        self.engine_17pii = None
        self.recognizer_6pii = None
        self.recognizer_17pii = None

        self.test_cases = []

    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {self.test_data_path}")

        with open(self.test_data_path, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    self.test_cases.append(json.loads(line))

        print(f"âœ… åŠ è½½ {len(self.test_cases)} æ¡æµ‹è¯•ç”¨ä¾‹")

    def load_6pii_model(self):
        """åŠ è½½6ç§PIIæ¨¡å‹"""
        print(f"\nğŸ“¦ åŠ è½½6ç§PIIæ¨¡å‹: {self.model_6pii_path}")

        self.engine_6pii = QwenFineTunedEngine(
            model_path=self.model_6pii_path,
            device="cuda",
            load_in_4bit=True
        )

        self.recognizer_6pii = FineTunedLLMRecognizer(
            llm_engine=self.engine_6pii,
            confidence_threshold=0.7
        )

        # é¢„çƒ­
        _ = self.recognizer_6pii.detect("æµ‹è¯•")

        print(f"âœ… 6ç§PIIæ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"   æ”¯æŒç±»å‹: {self.engine_6pii.get_supported_pii_types()}")

    def load_17pii_model(self):
        """åŠ è½½17ç§PIIæ¨¡å‹"""
        print(f"\nğŸ“¦ åŠ è½½17ç§PIIæ¨¡å‹: {self.model_17pii_path}")

        # å…ˆå¸è½½6ç§æ¨¡å‹é‡Šæ”¾å†…å­˜
        if self.engine_6pii is not None:
            del self.engine_6pii
            del self.recognizer_6pii
            import torch
            torch.cuda.empty_cache()
            print("   6ç§æ¨¡å‹å·²å¸è½½ï¼Œå†…å­˜å·²é‡Šæ”¾")

        self.engine_17pii = QwenFineTunedEngine(
            model_path=self.model_17pii_path,
            device="cuda",
            load_in_4bit=True
        )

        self.recognizer_17pii = FineTunedLLMRecognizer(
            llm_engine=self.engine_17pii,
            confidence_threshold=0.7
        )

        # é¢„çƒ­
        _ = self.recognizer_17pii.detect("æµ‹è¯•")

        print(f"âœ… 17ç§PIIæ¨¡å‹åŠ è½½å®Œæˆ")
        print(f"   æ”¯æŒç±»å‹: {self.engine_17pii.get_supported_pii_types()}")

    def evaluate_model(
        self,
        recognizer: FineTunedLLMRecognizer,
        model_name: str
    ) -> Dict:
        """
        è¯„ä¼°å•ä¸ªæ¨¡å‹

        Args:
            recognizer: è¯†åˆ«å™¨
            model_name: æ¨¡å‹åç§°

        Returns:
            è¯„ä¼°ç»“æœ
        """
        print(f"\nğŸ§ª è¯„ä¼° {model_name}...")

        # ç»Ÿè®¡å˜é‡
        total_tp = 0  # True Positives
        total_fp = 0  # False Positives
        total_fn = 0  # False Negatives

        # æŒ‰ç±»å‹ç»Ÿè®¡
        type_stats = defaultdict(lambda: {"tp": 0, "fp": 0, "fn": 0})

        detailed_results = []

        for i, test_case in enumerate(self.test_cases, 1):
            text = test_case["text"]
            ground_truth = test_case["entities"]

            # è¿è¡Œæ£€æµ‹
            predicted = recognizer.detect(text)

            # è½¬æ¢ä¸ºé›†åˆï¼ˆç”¨äºæ¯”è¾ƒï¼‰
            gt_set = {
                (e["type"], e["value"])
                for e in ground_truth
            }

            pred_set = {
                (e.entity_type, e.value)
                for e in predicted
            }

            # è®¡ç®—TP, FP, FN
            tp_set = gt_set & pred_set
            fp_set = pred_set - gt_set
            fn_set = gt_set - pred_set

            total_tp += len(tp_set)
            total_fp += len(fp_set)
            total_fn += len(fn_set)

            # æŒ‰ç±»å‹ç»Ÿè®¡
            for entity_type, value in tp_set:
                type_stats[entity_type]["tp"] += 1

            for entity_type, value in fp_set:
                type_stats[entity_type]["fp"] += 1

            for entity_type, value in fn_set:
                type_stats[entity_type]["fn"] += 1

            # ä¿å­˜è¯¦ç»†ç»“æœ
            detailed_results.append({
                "case_id": i,
                "text": text,
                "ground_truth": list(gt_set),
                "predicted": list(pred_set),
                "tp": list(tp_set),
                "fp": list(fp_set),
                "fn": list(fn_set),
            })

            # æ‰“å°è¿›åº¦
            if i % 10 == 0:
                print(f"  è¿›åº¦: {i}/{len(self.test_cases)}")

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        # è®¡ç®—æ¯ç§ç±»å‹çš„æŒ‡æ ‡
        type_metrics = {}
        for entity_type, stats in type_stats.items():
            tp = stats["tp"]
            fp = stats["fp"]
            fn = stats["fn"]

            prec = tp / (tp + fp) if (tp + fp) > 0 else 0
            rec = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1_type = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0

            type_metrics[entity_type] = {
                "precision": prec,
                "recall": rec,
                "f1": f1_type,
                "tp": tp,
                "fp": fp,
                "fn": fn,
                "support": tp + fn  # çœŸå®æ ·æœ¬æ•°
            }

        print(f"âœ… {model_name} è¯„ä¼°å®Œæˆ")
        print(f"   Precision: {precision:.2%}")
        print(f"   Recall: {recall:.2%}")
        print(f"   F1-Score: {f1:.2%}")

        return {
            "model_name": model_name,
            "overall": {
                "precision": precision,
                "recall": recall,
                "f1": f1,
                "tp": total_tp,
                "fp": total_fp,
                "fn": total_fn
            },
            "by_type": type_metrics,
            "detailed_results": detailed_results
        }

    def compare_models(self) -> Dict:
        """
        å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹

        Returns:
            å¯¹æ¯”ç»“æœ
        """
        print("=" * 70)
        print("ğŸ†š æ¨¡å‹å¯¹æ¯”éªŒè¯")
        print("=" * 70)

        # åŠ è½½æµ‹è¯•æ•°æ®
        self.load_test_data()

        # è¯„ä¼°6ç§PIIæ¨¡å‹
        self.load_6pii_model()
        results_6pii = self.evaluate_model(self.recognizer_6pii, "6-PII Model")

        # è¯„ä¼°17ç§PIIæ¨¡å‹
        self.load_17pii_model()
        results_17pii = self.evaluate_model(self.recognizer_17pii, "17-PII Model")

        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        comparison = self._generate_comparison(results_6pii, results_17pii)

        return comparison

    def _generate_comparison(
        self,
        results_6pii: Dict,
        results_17pii: Dict
    ) -> Dict:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        print("\n" + "=" * 70)
        print("ğŸ“Š å¯¹æ¯”ç»“æœ")
        print("=" * 70)

        # æ€»ä½“å¯¹æ¯”
        print("\næ€»ä½“æ€§èƒ½:")
        print(f"{'æŒ‡æ ‡':<15} {'6-PII':>12} {'17-PII':>12} {'æå‡':>12}")
        print("-" * 55)

        metrics = ["precision", "recall", "f1"]
        for metric in metrics:
            val_6 = results_6pii["overall"][metric]
            val_17 = results_17pii["overall"][metric]
            delta = val_17 - val_6
            delta_str = f"+{delta:.2%}" if delta >= 0 else f"{delta:.2%}"

            print(f"{metric.capitalize():<15} {val_6:>11.2%} {val_17:>11.2%} {delta_str:>12}")

        # æŒ‰ç±»å‹å¯¹æ¯”
        print("\næŒ‰PIIç±»å‹å¯¹æ¯”:")
        print(f"{'ç±»å‹':<20} {'6-PII F1':>12} {'17-PII F1':>12} {'æå‡':>12}")
        print("-" * 60)

        all_types = set(results_6pii["by_type"].keys()) | set(results_17pii["by_type"].keys())

        for pii_type in sorted(all_types):
            f1_6 = results_6pii["by_type"].get(pii_type, {}).get("f1", 0)
            f1_17 = results_17pii["by_type"].get(pii_type, {}).get("f1", 0)

            delta = f1_17 - f1_6
            delta_str = f"+{delta:.2%}" if delta >= 0 else f"{delta:.2%}"

            # æ ‡è®°æ–°å¢ç±»å‹
            type_str = pii_type
            if f1_6 == 0 and f1_17 > 0:
                type_str += " *"

            print(f"{type_str:<20} {f1_6:>11.2%} {f1_17:>11.2%} {delta_str:>12}")

        print("\n* è¡¨ç¤º17-PIIæ¨¡å‹æ–°å¢æ”¯æŒçš„ç±»å‹")

        # æ±‡æ€»
        comparison = {
            "timestamp": datetime.now().isoformat(),
            "test_dataset": self.test_data_path,
            "num_test_cases": len(self.test_cases),
            "model_6pii": {
                "path": self.model_6pii_path,
                "results": results_6pii
            },
            "model_17pii": {
                "path": self.model_17pii_path,
                "results": results_17pii
            },
            "improvement": {
                "precision": results_17pii["overall"]["precision"] - results_6pii["overall"]["precision"],
                "recall": results_17pii["overall"]["recall"] - results_6pii["overall"]["recall"],
                "f1": results_17pii["overall"]["f1"] - results_6pii["overall"]["f1"]
            }
        }

        return comparison

    def save_results(self, comparison: Dict, output_path: str):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        output_file = Path(output_path)

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(comparison, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="6ç§ vs 17ç§PIIæ¨¡å‹å¯¹æ¯”éªŒè¯")
    parser.add_argument(
        "--model-6pii",
        default="models/pii_qwen4b_unsloth/final",
        help="6ç§PIIæ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--model-17pii",
        default="models/pii_qwen4b_17types/final",
        help="17ç§PIIæ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--test-data",
        default="data/test_datasets/17pii_test_cases.jsonl",
        help="æµ‹è¯•æ•°æ®è·¯å¾„"
    )
    parser.add_argument(
        "--output",
        default="comparison_6vs17_report.json",
        help="è¾“å‡ºæŠ¥å‘Šè·¯å¾„"
    )

    args = parser.parse_args()

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.model_6pii).exists():
        print(f"âŒ é”™è¯¯: 6ç§PIIæ¨¡å‹ä¸å­˜åœ¨: {args.model_6pii}")
        return

    if not Path(args.test_data).exists():
        print(f"âŒ é”™è¯¯: æµ‹è¯•æ•°æ®ä¸å­˜åœ¨: {args.test_data}")
        return

    # æ£€æŸ¥17ç§æ¨¡å‹ï¼ˆå¯èƒ½è¿˜åœ¨è®­ç»ƒä¸­ï¼‰
    if not Path(args.model_17pii).exists():
        print(f"âš ï¸  è­¦å‘Š: 17ç§PIIæ¨¡å‹ä¸å­˜åœ¨: {args.model_17pii}")
        print("    è®­ç»ƒå¯èƒ½è¿˜åœ¨è¿›è¡Œä¸­ï¼Œå°†åªè¯„ä¼°6ç§PIIæ¨¡å‹")

        # åªè¯„ä¼°6ç§æ¨¡å‹
        comparator = ModelComparator(
            model_6pii_path=args.model_6pii,
            model_17pii_path=args.model_17pii,  # ä¿ç•™è·¯å¾„ï¼Œç¨åå¤„ç†
            test_data_path=args.test_data
        )

        comparator.load_test_data()
        comparator.load_6pii_model()
        results_6pii = comparator.evaluate_model(comparator.recognizer_6pii, "6-PII Model")

        # ä¿å­˜6ç§æ¨¡å‹çš„ç»“æœ
        output_6pii = {
            "timestamp": datetime.now().isoformat(),
            "model_path": args.model_6pii,
            "results": results_6pii
        }

        output_file = Path(args.output).parent / "6pii_baseline_results.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output_6pii, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ 6ç§PIIæ¨¡å‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        print("\nâœ… åŸºçº¿è¯„ä¼°å®Œæˆï¼ç­‰å¾…17ç§æ¨¡å‹è®­ç»ƒå®Œæˆåå†æ¬¡è¿è¡Œå¯¹æ¯”ã€‚")
        return

    # ä¸¤ä¸ªæ¨¡å‹éƒ½å­˜åœ¨ï¼Œè¿›è¡Œå¯¹æ¯”
    comparator = ModelComparator(
        model_6pii_path=args.model_6pii,
        model_17pii_path=args.model_17pii,
        test_data_path=args.test_data
    )

    comparison = comparator.compare_models()
    comparator.save_results(comparison, args.output)

    print("\nâœ… å¯¹æ¯”éªŒè¯å®Œæˆï¼")


if __name__ == "__main__":
    main()
