#!/usr/bin/env python3
"""
å¿«é€Ÿæ¨¡å‹éªŒè¯è„šæœ¬ - ä¼˜åŒ–ç‰ˆæœ¬
ä¸“æ³¨äºå¿«é€Ÿè·å–æ¨¡å‹æ€§èƒ½æŒ‡æ ‡,ä¸è¿½æ±‚å®Œç¾è¯„ä¼°
"""

import json
import sys
import time
from pathlib import Path
from typing import Dict, Any, List
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from tqdm import tqdm

class QuickEvaluator:
    """å¿«é€Ÿè¯„ä¼°å™¨ - ä¼˜åŒ–æ€§èƒ½"""

    def __init__(self, model_path: str, device: str = "auto"):
        self.model_path = Path(model_path)
        self.device = self._setup_device(device)

        print(f"\nğŸš€ å¿«é€Ÿæ¨¡å‹éªŒè¯")
        print(f"=" * 70)
        print(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
        print(f"è®¾å¤‡: {self.device}")
        print(f"=" * 70)

        self.tokenizer, self.model = self._load_model()

    def _setup_device(self, device: str) -> str:
        """è®¾ç½®è®¾å¤‡"""
        if device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return device

    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print("\n[1/3] åŠ è½½æ¨¡å‹...")

        # è¯»å– adapter_config è·å–åŸºç¡€æ¨¡å‹è·¯å¾„
        adapter_config_path = self.model_path / "adapter_config.json"
        with open(adapter_config_path) as f:
            adapter_config = json.load(f)

        base_model_path = adapter_config["base_model_name_or_path"]
        print(f"  åŸºç¡€æ¨¡å‹: {base_model_path}")

        # åŠ è½½ tokenizer
        print("  åŠ è½½ tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            str(self.model_path),
            trust_remote_code=True
        )

        # åŠ è½½åŸºç¡€æ¨¡å‹
        print("  åŠ è½½åŸºç¡€æ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map=self.device,
            trust_remote_code=True
        )

        # åŠ è½½ LoRA adapter
        print("  åŠ è½½ LoRA adapter...")
        model = PeftModel.from_pretrained(base_model, str(self.model_path))

        # åˆå¹¶æƒé‡ä»¥åŠ é€Ÿæ¨ç†
        print("  åˆå¹¶æƒé‡...")
        model = model.merge_and_unload()
        model.eval()

        print("  âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")

        return tokenizer, model

    def detect_pii(self, text: str) -> Dict[str, Any]:
        """æ£€æµ‹ PII"""
        # æ ¼å¼åŒ–æç¤ºè¯
        prompt = (
            f"<|im_start|>system\n"
            f"ä½ æ˜¯ PII æ£€æµ‹ä¸“å®¶ã€‚æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚<|im_end|>\n"
            f"<|im_start|>user\n"
            f"{text}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )

        # åˆ†è¯ (æˆªæ–­åˆ°256ä»¥åŠ é€Ÿ)
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=256
        ).to(self.device)

        # ç”Ÿæˆ (é™åˆ¶åˆ°128 tokensä»¥åŠ é€Ÿ)
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=128,  # é™ä½åˆ°128åŠ é€Ÿ
                do_sample=False,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

        # è§£ç 
        output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=False)

        # æå– JSON
        try:
            assistant_marker = "<|im_start|>assistant\n"
            if assistant_marker in output_text:
                output_text = output_text.split(assistant_marker)[-1]

            json_start = output_text.find("{")
            json_end = output_text.rfind("}") + 1

            if json_start != -1 and json_end > 0:
                json_str = output_text[json_start:json_end]
                result = json.loads(json_str)
                return result
        except:
            pass

        return {"entities": []}

    def evaluate_on_subset(self, test_data_path: str, sample_size: int = 500):
        """åœ¨å­é›†ä¸Šè¯„ä¼°"""
        print(f"\n[2/3] åœ¨æµ‹è¯•å­é›†ä¸Šè¯„ä¼°...")
        print(f"  æµ‹è¯•æ•°æ®: {test_data_path}")
        print(f"  å­é›†å¤§å°: {sample_size}")

        # åŠ è½½æµ‹è¯•æ•°æ®
        samples = []
        with open(test_data_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= sample_size:
                    break
                samples.append(json.loads(line))

        print(f"  âœ“ åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")

        # è¯„ä¼°
        tp, fp, fn = 0, 0, 0
        success_count = 0
        error_count = 0

        print(f"\n  å¼€å§‹æ¨ç†...")
        start_time = time.time()

        for sample in tqdm(samples, desc="  å¤„ç†ä¸­"):
            try:
                # é¢„æµ‹ (inputå­—æ®µæ˜¯æ–‡æœ¬)
                pred_result = self.detect_pii(sample["input"])
                pred_entities = pred_result.get("entities", [])

                # çœŸå®æ ‡ç­¾ (output.entities)
                expected_entities = sample.get("output", {}).get("entities", [])

                # ç®€å•åŒ¹é… (type + value)
                pred_set = {(e["type"], e["value"]) for e in pred_entities if "type" in e and "value" in e}
                expected_set = {(e["type"], e["value"]) for e in expected_entities if "type" in e and "value" in e}

                # è®¡ç®—æŒ‡æ ‡
                tp += len(pred_set & expected_set)
                fp += len(pred_set - expected_set)
                fn += len(expected_set - pred_set)

                success_count += 1

            except Exception as e:
                error_count += 1
                if error_count == 1:  # åªæ‰“å°ç¬¬ä¸€ä¸ªé”™è¯¯
                    print(f"\n  âš ï¸  ç¬¬ä¸€ä¸ªé”™è¯¯: {type(e).__name__}: {str(e)}")

        elapsed = time.time() - start_time

        print(f"\n  âœ“ è¯„ä¼°å®Œæˆ")
        print(f"  è€—æ—¶: {elapsed:.1f}ç§’ ({elapsed/len(samples):.2f}ç§’/æ ·æœ¬)")
        print(f"  æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}")

        return {
            "samples": len(samples),
            "success": success_count,
            "errors": error_count,
            "tp": tp,
            "fp": fp,
            "fn": fn,
            "elapsed": elapsed
        }

    def print_results(self, metrics: Dict[str, Any]):
        """æ‰“å°ç»“æœ"""
        print(f"\n[3/3] éªŒè¯ç»“æœ")
        print(f"=" * 70)

        # è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡
        tp = metrics["tp"]
        fp = metrics["fp"]
        fn = metrics["fn"]

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        f2 = 5 * precision * recall / (4 * precision + recall) if (4 * precision + recall) > 0 else 0

        print(f"\næ··æ·†çŸ©é˜µ:")
        print(f"  TP (æ­£ç¡®æ£€æµ‹): {tp}")
        print(f"  FP (è¯¯æŠ¥): {fp}")
        print(f"  FN (æ¼æŠ¥): {fn}")

        print(f"\nå‡†ç¡®æ€§æŒ‡æ ‡:")
        print(f"  Precision (ç²¾ç¡®ç‡): {precision:.2%}")
        print(f"  Recall (å¬å›ç‡):    {recall:.2%}")
        print(f"  F1-Score:           {f1:.2%}")
        print(f"  F2-Score:           {f2:.2%}")

        print(f"\næ€§èƒ½æŒ‡æ ‡:")
        print(f"  æµ‹è¯•æ ·æœ¬æ•°: {metrics['samples']}")
        print(f"  æ€»è€—æ—¶: {metrics['elapsed']:.1f}ç§’")
        print(f"  å¹³å‡é€Ÿåº¦: {metrics['elapsed']/metrics['samples']:.2f}ç§’/æ ·æœ¬")

        # éªŒè¯é€šè¿‡/å¤±è´¥
        print(f"\n{'=' * 70}")

        target_f1 = 0.875
        target_recall = 0.90

        if f1 >= target_f1 and recall >= target_recall:
            print(f"âœ… éªŒè¯é€šè¿‡!")
            print(f"   F1-Score {f1:.2%} >= ç›®æ ‡ {target_f1:.0%}")
            print(f"   Recall {recall:.2%} >= ç›®æ ‡ {target_recall:.0%}")
            result = "PASS"
        else:
            print(f"âŒ éªŒè¯æœªé€šè¿‡")
            if f1 < target_f1:
                print(f"   F1-Score {f1:.2%} < ç›®æ ‡ {target_f1:.0%} (å·®è·: {(target_f1-f1)*100:.1f}%)")
            if recall < target_recall:
                print(f"   Recall {recall:.2%} < ç›®æ ‡ {target_recall:.0%} (å·®è·: {(target_recall-recall)*100:.1f}%)")
            result = "FAIL"

        print(f"{'=' * 70}")

        return result


def main():
    import argparse

    parser = argparse.ArgumentParser(description="å¿«é€Ÿæ¨¡å‹éªŒè¯")
    parser.add_argument("--model", required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--test-data", required=True, help="æµ‹è¯•æ•°æ®è·¯å¾„")
    parser.add_argument("--sample-size", type=int, default=500, help="æµ‹è¯•æ ·æœ¬æ•° (é»˜è®¤500)")

    args = parser.parse_args()

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = QuickEvaluator(args.model)

    # è¯„ä¼°
    metrics = evaluator.evaluate_on_subset(args.test_data, args.sample_size)

    # æ‰“å°ç»“æœ
    result = evaluator.print_results(metrics)

    # è¿”å›é€€å‡ºç 
    sys.exit(0 if result == "PASS" else 1)


if __name__ == "__main__":
    main()
