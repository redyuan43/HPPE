#!/usr/bin/env python3
"""
é›†æˆæ¨¡å‹ + è§„åˆ™å¢å¼ºéªŒè¯
æµ‹è¯•ï¼šEnsemble (Epoch1+Epoch2) + Rule-based
"""
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"  # ä½¿ç”¨åŒGPU

import sys
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from tqdm import tqdm

# å¯¼å…¥è§„åˆ™æ£€æµ‹å™¨
sys.path.append('scripts')
from rule_enhanced_detector import RuleEnhancedPIIDetector

BASE_MODEL = "/home/ivan/.cache/modelscope/hub/Qwen/Qwen3-4B"
MODEL1_PATH = "models/pii_qwen4b_unsloth/checkpoint-781"
MODEL2_PATH = "models/pii_qwen4b_unsloth/checkpoint-1562"
TEST_DATA = "data/merged_pii_dataset_test.jsonl"
SAMPLE_SIZE = 100

print("="*70)
print("ğŸš€ é›†æˆæ¨¡å‹ + è§„åˆ™å¢å¼ºéªŒè¯")
print("="*70)
print(f"ç­–ç•¥: (Epoch1 âˆª Epoch2) âˆª Rules")
print(f"æ ·æœ¬æ•°: {SAMPLE_SIZE}")
print("="*70 + "\n")

# åŠ è½½æ¨¡å‹
print("ğŸ“¦ åŠ è½½æ¨¡å‹1 (GPU0)...")
base_model1 = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
    device_map={"": 0},
)
model1 = PeftModel.from_pretrained(base_model1, MODEL1_PATH)
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
print("âœ… æ¨¡å‹1å®Œæˆ\n")

print("ğŸ“¦ åŠ è½½æ¨¡å‹2 (GPU1)...")
base_model2 = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
    device_map={"": 1},
)
model2 = PeftModel.from_pretrained(base_model2, MODEL2_PATH)
print("âœ… æ¨¡å‹2å®Œæˆ\n")

# åˆå§‹åŒ–è§„åˆ™æ£€æµ‹å™¨
print("ğŸ“¦ åˆå§‹åŒ–è§„åˆ™æ£€æµ‹å™¨...")
rule_detector = RuleEnhancedPIIDetector()
print("âœ… è§„åˆ™æ£€æµ‹å™¨å®Œæˆ\n")

# åŠ è½½æµ‹è¯•æ•°æ®
test_samples = []
with open(TEST_DATA, 'r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        if i >= SAMPLE_SIZE:
            break
        test_samples.append(json.loads(line))

print(f"ğŸ“Š åŠ è½½äº† {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬\n")

def extract_entities(text):
    try:
        start_idx = text.find('{"entities"')
        if start_idx == -1:
            return []
        end_marker = '<|im_end|>'
        end_idx = text.find(end_marker, start_idx)
        if end_idx == -1:
            json_str = text[start_idx:].strip()
        else:
            json_str = text[start_idx:end_idx].strip()
        result = json.loads(json_str)
        return result.get("entities", [])
    except:
        return []

def predict_entities(model, tokenizer, text):
    prompt = (
        f"<|im_start|>system\n"
        f"ä½ æ˜¯ PII æ£€æµ‹ä¸“å®¶ã€‚æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚<|im_end|>\n"
        f"<|im_start|>user\n{text}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )

    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    return extract_entities(response)

# æ‰§è¡ŒéªŒè¯
print("ğŸš€ å¼€å§‹éªŒè¯...\n")

ensemble_tp = ensemble_fp = ensemble_fn = 0
full_tp = full_fp = full_fn = 0

for sample in tqdm(test_samples, desc="éªŒè¯è¿›åº¦"):
    input_text = sample["input"]
    expected_entities = set(
        (e["type"], e["value"])
        for e in sample.get("output", {}).get("entities", [])
    )

    # æ¨¡å‹1é¢„æµ‹
    pred1 = predict_entities(model1, tokenizer, input_text)
    pred1_set = set((e["type"], e["value"]) for e in pred1)

    # æ¨¡å‹2é¢„æµ‹
    pred2 = predict_entities(model2, tokenizer, input_text)
    pred2_set = set((e["type"], e["value"]) for e in pred2)

    # é›†æˆé¢„æµ‹ï¼ˆå¹¶é›†ï¼‰
    ensemble_set = pred1_set | pred2_set

    # è§„åˆ™æå–
    rule_entities = rule_detector.extract_by_rules(input_text)
    rule_set = set((e["type"], e["value"]) for e in rule_entities)

    # å®Œæ•´é¢„æµ‹ï¼ˆé›†æˆ + è§„åˆ™ï¼‰
    full_set = ensemble_set | rule_set

    # ç»Ÿè®¡é›†æˆæ¨¡å‹
    ensemble_tp += len(expected_entities & ensemble_set)
    ensemble_fp += len(ensemble_set - expected_entities)
    ensemble_fn += len(expected_entities - ensemble_set)

    # ç»Ÿè®¡å®Œæ•´æ–¹æ¡ˆ
    full_tp += len(expected_entities & full_set)
    full_fp += len(full_set - expected_entities)
    full_fn += len(expected_entities - full_set)

# è®¡ç®—æŒ‡æ ‡
def calc_metrics(tp, fp, fn):
    p = tp / (tp + fp) if (tp + fp) > 0 else 0
    r = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0
    return p, r, f1

ens_p, ens_r, ens_f1 = calc_metrics(ensemble_tp, ensemble_fp, ensemble_fn)
full_p, full_r, full_f1 = calc_metrics(full_tp, full_fp, full_fn)

# è¾“å‡ºç»“æœ
print(f"\n{'='*70}")
print(f"ğŸ“Š éªŒè¯ç»“æœå¯¹æ¯” ({SAMPLE_SIZE}æ ·æœ¬)")
print(f"{'='*70}\n")

print(f"{'æ–¹æ¡ˆ':<20} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}")
print(f"{'-'*70}")
print(f"{'é›†æˆæ¨¡å‹':<20} {ens_p*100:<11.2f}% {ens_r*100:<11.2f}% {ens_f1*100:<11.2f}%")
print(f"{'é›†æˆ+è§„åˆ™':<20} {full_p*100:<11.2f}% {full_r*100:<11.2f}% {full_f1*100:<11.2f}%")
print(f"{'='*70}\n")

print(f"ğŸ“ˆ è§„åˆ™å¢å¼ºæ•ˆæœ:")
print(f"  Recall:    {ens_r*100:.2f}% â†’ {full_r*100:.2f}% ({(full_r-ens_r)*100:+.2f}%)")
print(f"  F1-Score:  {ens_f1*100:.2f}% â†’ {full_f1*100:.2f}% ({(full_f1-ens_f1)*100:+.2f}%)")

print(f"\nç»Ÿè®¡:")
print(f"  é›†æˆ: TP={ensemble_tp}, FP={ensemble_fp}, FN={ensemble_fn}")
print(f"  å®Œæ•´: TP={full_tp}, FP={full_fp}, FN={full_fn}")

# ä¿å­˜ç»“æœ
result = {
    "sample_size": SAMPLE_SIZE,
    "ensemble_only": {
        "precision": ens_p,
        "recall": ens_r,
        "f1": ens_f1,
        "tp": ensemble_tp,
        "fp": ensemble_fp,
        "fn": ensemble_fn
    },
    "ensemble_plus_rules": {
        "precision": full_p,
        "recall": full_r,
        "f1": full_f1,
        "tp": full_tp,
        "fp": full_fp,
        "fn": full_fn
    }
}

with open("logs/ensemble_plus_rules_result.json", 'w', encoding='utf-8') as f:
    json.dump(result, f, indent=2, ensure_ascii=False)

print(f"\nâœ… ç»“æœå·²ä¿å­˜è‡³: logs/ensemble_plus_rules_result.json")

if full_r >= 0.90:
    print(f"\nğŸ‰ è¾¾æ ‡ï¼Recall {full_r*100:.2f}% â‰¥ 90%")
else:
    gap = 0.90 - full_r
    print(f"\nâš ï¸ è·ç¦»ç›®æ ‡: {gap*100:.2f}%")
