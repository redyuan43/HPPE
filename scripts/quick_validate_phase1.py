#!/usr/bin/env python3
"""
Phase 1 å¿«é€ŸéªŒè¯è„šæœ¬ - ç®€åŒ–ç‰ˆ
éªŒè¯GPU0å’ŒGPU1æ¨¡å‹å¯¹11ç§æ–°PIIçš„è¯†åˆ«æ•ˆæœ
"""
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

import json
import torch
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import random

print("="*70)
print("ğŸ“Š Phase 1 æ¨¡å‹å¿«é€ŸéªŒè¯")
print("="*70)
print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*70 + "\n")

# é…ç½®
BASE_MODEL = "/home/ivan/.cache/modelscope/hub/Qwen/Qwen3-4B"
GPU0_MODEL = "models/pii_11new_gpu0/final"
GPU1_MODEL = "models/pii_11new_gpu1/final"
OUTPUT_REPORT = "logs/phase1_validation_report.json"

# GPU0è´Ÿè´£çš„PIIç±»å‹
GPU0_TYPES = ["BANK_CARD", "PASSPORT", "HK_MACAU_PASS", "POSTAL_CODE", "IP_ADDRESS", "MAC_ADDRESS"]
# GPU1è´Ÿè´£çš„PIIç±»å‹
GPU1_TYPES = ["VEHICLE_PLATE", "DRIVER_LICENSE", "SOCIAL_SECURITY_CARD", "UNIFIED_SOCIAL_CREDIT_CODE", "IPV6_ADDRESS"]

# ç”Ÿæˆæµ‹è¯•æ ·æœ¬ï¼ˆç®€åŒ–ç‰ˆï¼Œæ¯ç§10ä¸ªï¼‰
def generate_test_samples():
    """ä¸ºæ¯ç§PIIç±»å‹ç”Ÿæˆ10ä¸ªæµ‹è¯•æ ·æœ¬"""
    samples = {}

    print("ğŸ“ ç”Ÿæˆæµ‹è¯•æ ·æœ¬...")

    # BANK_CARD
    samples["BANK_CARD"] = []
    for _ in range(10):
        card = "6217" + "".join([str(random.randint(0, 9)) for _ in range(12)])
        samples["BANK_CARD"].append({
            "text": f"è¯·è½¬è´¦åˆ°é“¶è¡Œå¡{card}",
            "expected_type": "BANK_CARD",
            "expected_value": card
        })

    # PASSPORT
    samples["PASSPORT"] = []
    for _ in range(10):
        passport = "E" + "".join([str(random.randint(0, 9)) for _ in range(8)])
        samples["PASSPORT"].append({
            "text": f"æŠ¤ç…§å·{passport}",
            "expected_type": "PASSPORT",
            "expected_value": passport
        })

    # HK_MACAU_PASS
    samples["HK_MACAU_PASS"] = []
    for _ in range(10):
        pass_id = "C" + "".join([str(random.randint(0, 9)) for _ in range(8)])
        samples["HK_MACAU_PASS"].append({
            "text": f"æ¸¯æ¾³é€šè¡Œè¯{pass_id}",
            "expected_type": "HK_MACAU_PASS",
            "expected_value": pass_id
        })

    # POSTAL_CODE
    samples["POSTAL_CODE"] = []
    for _ in range(10):
        code = "".join([str(random.randint(0, 9)) for _ in range(6)])
        samples["POSTAL_CODE"].append({
            "text": f"é‚®ç¼–{code}",
            "expected_type": "POSTAL_CODE",
            "expected_value": code
        })

    # IP_ADDRESS
    samples["IP_ADDRESS"] = []
    for _ in range(10):
        ip = f"{random.randint(1,255)}.{random.randint(0,255)}.{random.randint(0,255)}.{random.randint(1,254)}"
        samples["IP_ADDRESS"].append({
            "text": f"IPåœ°å€{ip}",
            "expected_type": "IP_ADDRESS",
            "expected_value": ip
        })

    # MAC_ADDRESS
    samples["MAC_ADDRESS"] = []
    for _ in range(10):
        mac = ":".join([f"{random.randint(0,255):02x}" for _ in range(6)])
        samples["MAC_ADDRESS"].append({
            "text": f"MACåœ°å€{mac}",
            "expected_type": "MAC_ADDRESS",
            "expected_value": mac
        })

    # VEHICLE_PLATE
    samples["VEHICLE_PLATE"] = []
    provinces = ["äº¬", "æ²ª", "ç²¤"]
    for _ in range(10):
        plate = random.choice(provinces) + random.choice("ABCDEFGH") + "".join([random.choice("0123456789ABCDEFGH") for _ in range(5)])
        samples["VEHICLE_PLATE"].append({
            "text": f"è½¦ç‰Œ{plate}",
            "expected_type": "VEHICLE_PLATE",
            "expected_value": plate
        })

    # DRIVER_LICENSE
    samples["DRIVER_LICENSE"] = []
    for _ in range(10):
        dl = "".join([str(random.randint(0, 9)) for _ in range(18)])
        samples["DRIVER_LICENSE"].append({
            "text": f"é©¾é©¶è¯{dl}",
            "expected_type": "DRIVER_LICENSE",
            "expected_value": dl
        })

    # SOCIAL_SECURITY_CARD
    samples["SOCIAL_SECURITY_CARD"] = []
    for _ in range(10):
        ssc = "".join([str(random.randint(0, 9)) for _ in range(18)])
        samples["SOCIAL_SECURITY_CARD"].append({
            "text": f"ç¤¾ä¿å¡{ssc}",
            "expected_type": "SOCIAL_SECURITY_CARD",
            "expected_value": ssc
        })

    # UNIFIED_SOCIAL_CREDIT_CODE
    samples["UNIFIED_SOCIAL_CREDIT_CODE"] = []
    for _ in range(10):
        code = "91" + "".join([str(random.randint(0, 9)) for _ in range(16)])
        samples["UNIFIED_SOCIAL_CREDIT_CODE"].append({
            "text": f"ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç {code}",
            "expected_type": "UNIFIED_SOCIAL_CREDIT_CODE",
            "expected_value": code
        })

    # IPV6_ADDRESS
    samples["IPV6_ADDRESS"] = []
    for _ in range(10):
        ipv6 = ":".join([f"{random.randint(0,65535):04x}" for _ in range(8)])
        samples["IPV6_ADDRESS"].append({
            "text": f"IPv6åœ°å€{ipv6}",
            "expected_type": "IPV6_ADDRESS",
            "expected_value": ipv6
        })

    print(f"âœ… ç”Ÿæˆäº† {sum(len(v) for v in samples.values())} ä¸ªæµ‹è¯•æ ·æœ¬\n")
    return samples

# åŠ è½½æ¨¡å‹
def load_model(adapter_path):
    """åŠ è½½æ¨¡å‹"""
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {adapter_path}")

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)

    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )

    model = PeftModel.from_pretrained(base_model, adapter_path)
    model.eval()

    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")
    return model, tokenizer

# æµ‹è¯•å•ä¸ªæ ·æœ¬
def test_sample(model, tokenizer, text, expected_type):
    """æµ‹è¯•å•ä¸ªæ ·æœ¬"""
    prompt = (
        f"<|im_start|>system\n"
        f"ä½ æ˜¯ PII æ£€æµ‹ä¸“å®¶ã€‚æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚<|im_end|>\n"
        f"<|im_start|>user\n{text}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.1,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)

    # åˆ¤æ–­æ˜¯å¦æ£€æµ‹åˆ°æ­£ç¡®ç±»å‹
    correct = expected_type in response

    return correct, response

# æµ‹è¯•æ¨¡å‹
def test_model(model, tokenizer, samples, pii_types, model_name):
    """æµ‹è¯•æ¨¡å‹"""
    print(f"ğŸ” æµ‹è¯•{model_name}...")
    print(f"   PIIç±»å‹: {', '.join(pii_types)}\n")

    results = {}

    for pii_type in pii_types:
        print(f"   æµ‹è¯• {pii_type}...", end=" ")
        correct_count = 0
        total_count = len(samples[pii_type])
        details = []

        for sample in samples[pii_type]:
            correct, response = test_sample(model, tokenizer, sample["text"], sample["expected_type"])
            if correct:
                correct_count += 1
            details.append({
                "text": sample["text"],
                "expected": sample["expected_type"],
                "response": response[:100],  # åªä¿å­˜å‰100å­—ç¬¦
                "correct": correct
            })

        recall = 100 * correct_count / total_count
        print(f"{correct_count}/{total_count} ({recall:.0f}%)")

        results[pii_type] = {
            "correct": correct_count,
            "total": total_count,
            "recall": recall,
            "details": details
        }

    print(f"\nâœ… {model_name}æµ‹è¯•å®Œæˆ\n")
    return results

# ä¸»å‡½æ•°
def main():
    # ç”Ÿæˆæµ‹è¯•æ ·æœ¬
    samples = generate_test_samples()

    # æµ‹è¯•GPU0æ¨¡å‹
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    gpu0_model, gpu0_tokenizer = load_model(GPU0_MODEL)
    gpu0_results = test_model(gpu0_model, gpu0_tokenizer, samples, GPU0_TYPES, "GPU0æ¨¡å‹")
    del gpu0_model, gpu0_tokenizer
    torch.cuda.empty_cache()

    # æµ‹è¯•GPU1æ¨¡å‹
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    gpu1_model, gpu1_tokenizer = load_model(GPU1_MODEL)
    gpu1_results = test_model(gpu1_model, gpu1_tokenizer, samples, GPU1_TYPES, "GPU1æ¨¡å‹")
    del gpu1_model, gpu1_tokenizer
    torch.cuda.empty_cache()

    # åˆå¹¶ç»“æœ
    all_results = {**gpu0_results, **gpu1_results}

    # è®¡ç®—æ•´ä½“æŒ‡æ ‡
    total_correct = sum(r["correct"] for r in all_results.values())
    total_samples = sum(r["total"] for r in all_results.values())
    overall_recall = 100 * total_correct / total_samples

    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("ğŸ“Š éªŒè¯ç»“æœæ±‡æ€»")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
    print(f"æ•´ä½“Recall: {overall_recall:.1f}% ({total_correct}/{total_samples})\n")

    for pii_type in GPU0_TYPES + GPU1_TYPES:
        result = all_results[pii_type]
        print(f"  {pii_type:30s}: {result['recall']:5.1f}% ({result['correct']}/{result['total']})")

    # ä¿å­˜æŠ¥å‘Š
    report = {
        "timestamp": datetime.now().isoformat(),
        "overall": {
            "recall": round(overall_recall, 2),
            "correct": total_correct,
            "total": total_samples
        },
        "by_type": {
            pii_type: {
                "recall": round(r["recall"], 2),
                "correct": r["correct"],
                "total": r["total"]
            }
            for pii_type, r in all_results.items()
        },
        "details": all_results
    }

    with open(OUTPUT_REPORT, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {OUTPUT_REPORT}")
    print("="*70)

    # è¿”å›æ•´ä½“recallç”¨äºåç»­åˆ¤æ–­
    return overall_recall

if __name__ == "__main__":
    recall = main()
    print(f"\næœ€ç»ˆRecall: {recall:.2f}%")
