#!/usr/bin/env python3
"""
æµ‹è¯•Epoch 2æ¨¡å‹å¯¹11ç§æ–°PIIçš„é›¶æ ·æœ¬æ£€æµ‹èƒ½åŠ›
éªŒè¯æ¨¡å‹åœ¨æœªè®­ç»ƒè¿‡è¿™äº›ç±»å‹æ—¶çš„è¡¨ç°
"""
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # åªç”¨GPU0

import torch
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

BASE_MODEL = "/home/ivan/.cache/modelscope/hub/Qwen/Qwen3-4B"
MODEL_PATH = "models/pii_qwen4b_unsloth/checkpoint-1562"  # Epoch 2

print("="*70)
print("ğŸ§ª æµ‹è¯•Epoch 2æ¨¡å‹çš„11ç§æ–°PIIé›¶æ ·æœ¬æ£€æµ‹èƒ½åŠ›")
print("="*70)
print(f"æ¨¡å‹: {MODEL_PATH}")
print(f"æµ‹è¯•æ ·æœ¬: 20ä¸ªï¼ˆæ¶µç›–11ç§æ–°PIIï¼‰")
print("="*70 + "\n")

# åŠ è½½æ¨¡å‹
print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
    device_map="auto",
)
model = PeftModel.from_pretrained(base_model, MODEL_PATH)
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")

# æ„é€ æµ‹è¯•æ ·æœ¬ï¼ˆ11ç§æ–°PIIï¼Œæ¯ç§2ä¸ªæ ·æœ¬ï¼‰
test_samples = [
    # 1. BANK_CARD - é“¶è¡Œå¡å·
    {
        "input": "è¯·å°†æ¬¾é¡¹è½¬è‡³é“¶è¡Œå¡6217002430009876543ï¼Œæˆ·åå¼ ä¸‰ã€‚",
        "expected": [{"type": "BANK_CARD", "value": "6217002430009876543"}, {"type": "PERSON_NAME", "value": "å¼ ä¸‰"}]
    },
    {
        "input": "æˆ‘çš„å·¥èµ„å¡å·æ˜¯6222021234567890123ï¼Œå¼€æˆ·è¡Œæ˜¯å·¥å•†é“¶è¡Œã€‚",
        "expected": [{"type": "BANK_CARD", "value": "6222021234567890123"}]
    },

    # 2. VEHICLE_PLATE - è½¦ç‰Œå·
    {
        "input": "è½¦ç‰Œå·äº¬A12345çš„è½¦è¾†è¿ç« äº†ï¼Œè¯·å°½å¿«å¤„ç†ã€‚",
        "expected": [{"type": "VEHICLE_PLATE", "value": "äº¬A12345"}]
    },
    {
        "input": "åœè½¦åœºç›‘æ§æ˜¾ç¤ºï¼Œæ²ªB88888å·è½¦åœ¨18:30ç¦»å¼€ã€‚",
        "expected": [{"type": "VEHICLE_PLATE", "value": "æ²ªB88888"}]
    },

    # 3. IP_ADDRESS - IPåœ°å€
    {
        "input": "æœåŠ¡å™¨IPåœ°å€192.168.1.100å‡ºç°å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥ã€‚",
        "expected": [{"type": "IP_ADDRESS", "value": "192.168.1.100"}]
    },
    {
        "input": "ç™»å½•è®°å½•æ˜¾ç¤ºï¼Œæ¥è‡ª10.0.0.25çš„è®¿é—®è¢«æ‹’ç»ã€‚",
        "expected": [{"type": "IP_ADDRESS", "value": "10.0.0.25"}]
    },

    # 4. POSTAL_CODE - é‚®æ”¿ç¼–ç 
    {
        "input": "æ”¶ä»¶åœ°å€ï¼šåŒ—äº¬å¸‚æœé˜³åŒºå»ºå›½è·¯1å·ï¼Œé‚®ç¼–100020ã€‚",
        "expected": [{"type": "ADDRESS", "value": "åŒ—äº¬å¸‚æœé˜³åŒºå»ºå›½è·¯1å·"}, {"type": "POSTAL_CODE", "value": "100020"}]
    },
    {
        "input": "è¯·å°†å‘ç¥¨å¯„è‡³ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒºï¼Œé‚®æ”¿ç¼–ç 200120ã€‚",
        "expected": [{"type": "ADDRESS", "value": "ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒº"}, {"type": "POSTAL_CODE", "value": "200120"}]
    },

    # 5. UNIFIED_SOCIAL_CREDIT_CODE - ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç 
    {
        "input": "å…¬å¸ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç ï¼š91110000MA01234567ï¼Œè¯·æ ¸å®ã€‚",
        "expected": [{"type": "UNIFIED_SOCIAL_CREDIT_CODE", "value": "91110000MA01234567"}]
    },
    {
        "input": "ä¼ä¸šä¿¡ç”¨ä»£ç 91310000MA02345678å·²é€šè¿‡å·¥å•†å±€éªŒè¯ã€‚",
        "expected": [{"type": "UNIFIED_SOCIAL_CREDIT_CODE", "value": "91310000MA02345678"}]
    },

    # 6. PASSPORT - æŠ¤ç…§å·
    {
        "input": "æˆ‘çš„æŠ¤ç…§å·æ˜¯E12345678ï¼Œæœ‰æ•ˆæœŸè‡³2030å¹´ã€‚",
        "expected": [{"type": "PASSPORT", "value": "E12345678"}]
    },
    {
        "input": "è¯·å‡ºç¤ºæŠ¤ç…§G98765432åŠç†ç™»æœºæ‰‹ç»­ã€‚",
        "expected": [{"type": "PASSPORT", "value": "G98765432"}]
    },

    # 7. HK_MACAU_PASS - æ¸¯æ¾³é€šè¡Œè¯
    {
        "input": "æ¸¯æ¾³é€šè¡Œè¯C12345678å·²è¿‡æœŸï¼Œéœ€é‡æ–°åŠç†ã€‚",
        "expected": [{"type": "HK_MACAU_PASS", "value": "C12345678"}]
    },
    {
        "input": "è¯·æºå¸¦æ¸¯æ¾³é€šè¡Œè¯H98765432é€šå…³ã€‚",
        "expected": [{"type": "HK_MACAU_PASS", "value": "H98765432"}]
    },

    # 8. DRIVER_LICENSE - é©¾é©¶è¯å·
    {
        "input": "é©¾é©¶è¯å·110101199001011234ï¼Œå‡†é©¾è½¦å‹C1ã€‚",
        "expected": [{"type": "DRIVER_LICENSE", "value": "110101199001011234"}]
    },
    {
        "input": "è¯·æä¾›é©¾é©¶è¯320106198512152345è¿›è¡Œæ ¸æŸ¥ã€‚",
        "expected": [{"type": "DRIVER_LICENSE", "value": "320106198512152345"}]
    },

    # 9. SOCIAL_SECURITY_CARD - ç¤¾ä¿å¡å·
    {
        "input": "ç¤¾ä¿å¡å·110101199001011234ï¼Œå•ä½ç¼´è´¹æ­£å¸¸ã€‚",
        "expected": [{"type": "SOCIAL_SECURITY_CARD", "value": "110101199001011234"}]
    },
    {
        "input": "è¯·æä¾›ç¤¾ä¼šä¿éšœå¡å·310101198001012345æŸ¥è¯¢ã€‚",
        "expected": [{"type": "SOCIAL_SECURITY_CARD", "value": "310101198001012345"}]
    },

    # 10. MAC_ADDRESS - MACåœ°å€
    {
        "input": "ç½‘å¡MACåœ°å€00:1A:2B:3C:4D:5Eå·²ç»‘å®šã€‚",
        "expected": [{"type": "MAC_ADDRESS", "value": "00:1A:2B:3C:4D:5E"}]
    },
    {
        "input": "è®¾å¤‡MACåœ°å€ä¸ºAA:BB:CC:DD:EE:FFï¼Œè¯·è®°å½•ã€‚",
        "expected": [{"type": "MAC_ADDRESS", "value": "AA:BB:CC:DD:EE:FF"}]
    },

    # 11. IPV6_ADDRESS - IPv6åœ°å€
    {
        "input": "IPv6åœ°å€2001:0db8:85a3:0000:0000:8a2e:0370:7334å·²åˆ†é…ã€‚",
        "expected": [{"type": "IPV6_ADDRESS", "value": "2001:0db8:85a3:0000:0000:8a2e:0370:7334"}]
    },
    {
        "input": "æœåŠ¡å™¨IPv6ä¸º2001:db8::1ï¼Œè¯·é…ç½®è·¯ç”±ã€‚",
        "expected": [{"type": "IPV6_ADDRESS", "value": "2001:db8::1"}]
    },
]

def extract_entities(text):
    """æå–å®ä½“"""
    try:
        start_idx = text.find('{"entities"')
        if start_idx == -1:
            return []
        end_marker = '<|im_end|>'
        end_idx = text.find(end_marker, start_idx)
        if end_idx == -1:
            json_str = text[start_idx:].strip()
        else:
            json_str = text[start_idx:end_idx].strip()
        result = json.loads(json_str)
        return result.get("entities", [])
    except:
        return []

def predict_entities(text):
    """é¢„æµ‹å®ä½“"""
    prompt = (
        f"<|im_start|>system\n"
        f"ä½ æ˜¯ PII æ£€æµ‹ä¸“å®¶ã€‚æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚<|im_end|>\n"
        f"<|im_start|>user\n{text}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )

    inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    return extract_entities(response)

# æ‰§è¡Œæµ‹è¯•
print("ğŸš€ å¼€å§‹é›¶æ ·æœ¬æ£€æµ‹æµ‹è¯•...\n")

results = []
for i, sample in enumerate(test_samples, 1):
    input_text = sample["input"]
    expected = sample["expected"]

    print(f"\n{'='*70}")
    print(f"æµ‹è¯•æ ·æœ¬ {i}/{len(test_samples)}")
    print(f"{'='*70}")
    print(f"è¾“å…¥: {input_text}")
    print(f"\næœŸæœ›æ£€æµ‹:")
    for e in expected:
        print(f"  - {e['type']}: {e['value']}")

    # é¢„æµ‹
    predicted = predict_entities(input_text)

    print(f"\nå®é™…æ£€æµ‹:")
    if predicted:
        for e in predicted:
            print(f"  - {e['type']}: {e['value']}")
    else:
        print("  (æœªæ£€æµ‹åˆ°ä»»ä½•å®ä½“)")

    # åˆ†æç»“æœ
    expected_set = set((e["type"], e["value"]) for e in expected)
    predicted_set = set((e["type"], e["value"]) for e in predicted)

    tp = len(expected_set & predicted_set)
    fp = len(predicted_set - expected_set)
    fn = len(expected_set - predicted_set)

    print(f"\nç»“æœåˆ†æ:")
    print(f"  TP (æ­£ç¡®): {tp}")
    print(f"  FP (è¯¯æŠ¥): {fp}")
    print(f"  FN (æ¼æŠ¥): {fn}")

    if tp == len(expected) and fp == 0:
        print(f"  âœ… å®Œå…¨æ­£ç¡®")
    elif tp > 0:
        print(f"  âš ï¸ éƒ¨åˆ†æ­£ç¡®")
    else:
        print(f"  âŒ å®Œå…¨æ¼æ£€")

    results.append({
        "sample_id": i,
        "input": input_text,
        "expected": expected,
        "predicted": predicted,
        "tp": tp,
        "fp": fp,
        "fn": fn
    })

# ç»Ÿè®¡æ€»ä½“ç»“æœ
total_tp = sum(r["tp"] for r in results)
total_fp = sum(r["fp"] for r in results)
total_fn = sum(r["fn"] for r in results)

precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

print(f"\n{'='*70}")
print(f"ğŸ“Š é›¶æ ·æœ¬æ£€æµ‹æ€»ä½“ç»“æœ")
print(f"{'='*70}")
print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_samples)}")
print(f"æœŸæœ›å®ä½“æ•°: {total_tp + total_fn}")
print(f"\nTP (æ­£ç¡®æ£€æµ‹): {total_tp}")
print(f"FP (è¯¯æŠ¥): {total_fp}")
print(f"FN (æ¼æŠ¥): {total_fn}")
print(f"\nPrecision: {precision*100:.2f}%")
print(f"Recall:    {recall*100:.2f}%")
print(f"F1-Score:  {f1*100:.2f}%")

# æŒ‰PIIç±»å‹ç»Ÿè®¡
pii_stats = {}
for result in results:
    for entity in result["expected"]:
        pii_type = entity["type"]
        if pii_type not in pii_stats:
            pii_stats[pii_type] = {"total": 0, "detected": 0}
        pii_stats[pii_type]["total"] += 1

    for entity in result["predicted"]:
        pii_type = entity["type"]
        if pii_type in pii_stats and (pii_type, entity["value"]) in set((e["type"], e["value"]) for e in result["expected"]):
            pii_stats[pii_type]["detected"] += 1

print(f"\næŒ‰PIIç±»å‹ç»Ÿè®¡:")
print(f"{'-'*70}")
print(f"{'PIIç±»å‹':<30} {'æ£€æµ‹æ•°/æ€»æ•°':<15} {'æ£€æµ‹ç‡':<10}")
print(f"{'-'*70}")
for pii_type, stats in sorted(pii_stats.items()):
    rate = stats["detected"] / stats["total"] if stats["total"] > 0 else 0
    print(f"{pii_type:<30} {stats['detected']}/{stats['total']:<13} {rate*100:>6.1f}%")

# ä¿å­˜ç»“æœ
output_file = "logs/zero_shot_11pii_test_result.json"
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump({
        "model_path": MODEL_PATH,
        "sample_size": len(test_samples),
        "overall": {
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "tp": total_tp,
            "fp": total_fp,
            "fn": total_fn
        },
        "by_pii_type": pii_stats,
        "details": results
    }, f, indent=2, ensure_ascii=False)

print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_file}")

# ç»“è®ºå»ºè®®
print(f"\n{'='*70}")
print(f"ğŸ’¡ ç»“è®ºä¸å»ºè®®")
print(f"{'='*70}")

if recall >= 0.50:
    print(f"âœ… æ¨¡å‹å…·å¤‡è¾ƒå¼ºçš„é›¶æ ·æœ¬æ£€æµ‹èƒ½åŠ› (Recall {recall*100:.1f}% â‰¥ 50%)")
    print(f"   å»ºè®®ï¼šå¯ä»¥ç›´æ¥ä½¿ç”¨è§„åˆ™å¢å¼ºæ¥è¡¥å……ï¼Œæ— éœ€å¤§é‡è®­ç»ƒæ•°æ®")
elif recall >= 0.20:
    print(f"âš ï¸ æ¨¡å‹å…·å¤‡ä¸€å®šçš„é›¶æ ·æœ¬æ£€æµ‹èƒ½åŠ› (Recall {recall*100:.1f}% åœ¨20-50%)")
    print(f"   å»ºè®®ï¼šå»ºè®®æ¯ç§PIIç±»å‹å‡†å¤‡500-1000ä¸ªè®­ç»ƒæ ·æœ¬é‡æ–°è®­ç»ƒ")
else:
    print(f"âŒ æ¨¡å‹å‡ ä¹æ— é›¶æ ·æœ¬æ£€æµ‹èƒ½åŠ› (Recall {recall*100:.1f}% < 20%)")
    print(f"   å»ºè®®ï¼šå¿…é¡»ä¸ºæ¯ç§PIIç±»å‹å‡†å¤‡1000-2000ä¸ªè®­ç»ƒæ ·æœ¬")

print(f"\næ£€æµ‹ç‡æœ€ä½çš„3ç§PIIç±»å‹éœ€è¦é‡ç‚¹ä¼˜åŒ–ï¼š")
sorted_pii = sorted(pii_stats.items(), key=lambda x: x[1]["detected"]/x[1]["total"] if x[1]["total"] > 0 else 0)
for pii_type, stats in sorted_pii[:3]:
    rate = stats["detected"] / stats["total"] if stats["total"] > 0 else 0
    print(f"  - {pii_type}: {rate*100:.1f}% ({stats['detected']}/{stats['total']})")

print(f"\n{'='*70}")
print(f"âœ… æµ‹è¯•å®Œæˆï¼")
print(f"{'='*70}")
