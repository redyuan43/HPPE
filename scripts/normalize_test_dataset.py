#!/usr/bin/env python3
"""
æ ‡å‡†åŒ–æµ‹è¯•æ•°æ®é›†çš„PIIç±»å‹åç§°

å°†æµ‹è¯•æ•°æ®ä¸­çš„æ—§ç±»å‹åç§°è½¬æ¢ä¸ºæ ‡å‡†ç±»å‹åç§°
"""

import json
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from hppe.models.pii_types import normalize_pii_type


def normalize_dataset(input_file: Path, output_file: Path) -> dict:
    """
    æ ‡å‡†åŒ–æ•°æ®é›†ä¸­çš„PIIç±»å‹åç§°

    Args:
        input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„

    Returns:
        ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    stats = {
        "total_cases": 0,
        "total_entities": 0,
        "normalized_count": 0,
        "type_mapping": {},
    }

    normalized_cases = []

    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            case = json.loads(line)
            stats["total_cases"] += 1

            # æ ‡å‡†åŒ–å®ä½“ç±»å‹
            if "entities" in case:
                for entity in case["entities"]:
                    stats["total_entities"] += 1
                    old_type = entity["type"]
                    new_type = normalize_pii_type(old_type)

                    if old_type != new_type:
                        stats["normalized_count"] += 1
                        # è®°å½•æ˜ å°„
                        if old_type not in stats["type_mapping"]:
                            stats["type_mapping"][old_type] = new_type
                        print(f"  æ ‡å‡†åŒ–: {old_type} -> {new_type}")

                    entity["type"] = new_type

            normalized_cases.append(case)

    # å†™å…¥æ ‡å‡†åŒ–åçš„æ•°æ®
    with open(output_file, 'w', encoding='utf-8') as f:
        for case in normalized_cases:
            f.write(json.dumps(case, ensure_ascii=False) + '\n')

    return stats


def main():
    """ä¸»å‡½æ•°"""
    # è¾“å…¥è¾“å‡ºæ–‡ä»¶
    input_file = project_root / "data" / "test_datasets" / "17pii_test_cases.jsonl"
    output_file = project_root / "data" / "test_datasets" / "17pii_test_cases_normalized.jsonl"

    print("=" * 70)
    print("ğŸ“‹ æ ‡å‡†åŒ–æµ‹è¯•æ•°æ®é›†PIIç±»å‹")
    print("=" * 70)

    print(f"\nè¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")

    if not input_file.exists():
        print(f"\nâŒ é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return 1

    # æ‰§è¡Œæ ‡å‡†åŒ–
    print("\nğŸ”„ å¼€å§‹æ ‡å‡†åŒ–...")
    stats = normalize_dataset(input_file, output_file)

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 70)
    print("ğŸ“Š æ ‡å‡†åŒ–ç»“æœç»Ÿè®¡")
    print("=" * 70)
    print(f"æ€»æµ‹è¯•ç”¨ä¾‹æ•°: {stats['total_cases']}")
    print(f"æ€»å®ä½“æ•°: {stats['total_entities']}")
    print(f"æ ‡å‡†åŒ–å®ä½“æ•°: {stats['normalized_count']}")

    if stats["type_mapping"]:
        print(f"\nç±»å‹æ˜ å°„å…³ç³»:")
        for old, new in sorted(stats["type_mapping"].items()):
            print(f"  {old:30s} -> {new}")
    else:
        print("\nâœ… æ‰€æœ‰ç±»å‹å·²æ˜¯æ ‡å‡†æ ¼å¼ï¼Œæ— éœ€ä¿®æ”¹")

    print(f"\nâœ… æ ‡å‡†åŒ–å®Œæˆï¼")
    print(f"   è¾“å‡ºæ–‡ä»¶: {output_file}")

    # å¤‡ä»½åŸæ–‡ä»¶
    backup_file = input_file.with_suffix('.jsonl.backup')
    if not backup_file.exists():
        import shutil
        shutil.copy2(input_file, backup_file)
        print(f"   åŸæ–‡ä»¶å¤‡ä»½: {backup_file}")

    # æ›¿æ¢åŸæ–‡ä»¶
    output_file.replace(input_file)
    print(f"   å·²æ›´æ–°åŸæ–‡ä»¶: {input_file}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
