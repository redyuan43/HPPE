#!/usr/bin/env python3
"""
GPU1éš”ç¦»éªŒè¯ - Epoch 1æ¨¡å‹å¿«é€Ÿæµ‹è¯•
ä¸¥æ ¼é™åˆ¶åªä½¿ç”¨GPU1ï¼Œä¸å½±å“GPU0è®­ç»ƒ
"""

import os
import sys

# ========== å…³é”®ï¼šå¿…é¡»åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½® ==========
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
print(f"ğŸ”’ GPUéš”ç¦»è®¾ç½®: CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']}")

import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm
from pathlib import Path

# é…ç½®
MODEL_PATH = "models/pii_qwen4b_unsloth/checkpoint-781"
TEST_DATA = "data/merged_pii_dataset_test.jsonl"
SAMPLE_SIZE = 50  # å¿«é€ŸéªŒè¯ï¼šåªç”¨50ä¸ªæ ·æœ¬
MAX_SEQ_LENGTH = 512

print(f"\n{'='*70}")
print(f"Epoch 1 å¿«é€ŸéªŒè¯ (GPU1éš”ç¦»)")
print(f"{'='*70}")
print(f"æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
print(f"æµ‹è¯•æ•°æ®: {TEST_DATA}")
print(f"æ ·æœ¬æ•°é‡: {SAMPLE_SIZE}")
print(f"GPUè®¾ç½®: ä»…ä½¿ç”¨GPU1 (ä¸å½±å“GPU0è®­ç»ƒ)")
print(f"{'='*70}\n")

# éªŒè¯GPUéš”ç¦»
print("ğŸ” éªŒè¯GPUéš”ç¦»...")
print(f"  PyTorchå¯è§è®¾å¤‡æ•°: {torch.cuda.device_count()}")
print(f"  å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
if torch.cuda.is_available():
    print(f"  è®¾å¤‡åç§°: {torch.cuda.get_device_name(0)}")  # æ³¨æ„ï¼šè¿™é‡Œçš„0æ˜¯ç›¸å¯¹ç´¢å¼•
print()

# åŠ è½½æ¨¡å‹ï¼ˆLoRA checkpoint éœ€è¦å…ˆåŠ è½½åŸºç¡€æ¨¡å‹ï¼‰
print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
try:
    from peft import PeftModel, PeftConfig

    # 1. åŠ è½½LoRAé…ç½®ï¼Œè·å–åŸºç¡€æ¨¡å‹è·¯å¾„
    peft_config = PeftConfig.from_pretrained(MODEL_PATH)
    base_model_path = peft_config.base_model_name_or_path
    print(f"  åŸºç¡€æ¨¡å‹: {base_model_path}")
    print(f"  LoRAé€‚é…å™¨: {MODEL_PATH}")

    # 2. åŠ è½½åŸºç¡€æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        device_map="auto",  # è‡ªåŠ¨æ˜ å°„åˆ°å¯è§GPU (åªæœ‰GPU1)
    )

    # 3. åŠ è½½LoRAæƒé‡
    model = PeftModel.from_pretrained(model, MODEL_PATH)
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)

    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (åŸºç¡€æ¨¡å‹ + LoRA)\n")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# åŠ è½½æµ‹è¯•æ•°æ®
print(f"ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
test_samples = []
with open(TEST_DATA, 'r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        if i >= SAMPLE_SIZE:
            break
        test_samples.append(json.loads(line))
print(f"âœ… åŠ è½½äº† {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬\n")

# éªŒè¯å‡½æ•°
def extract_entities(text):
    """æå–æ¨¡å‹é¢„æµ‹çš„å®ä½“"""
    try:
        # æŸ¥æ‰¾JSONéƒ¨åˆ†
        start_idx = text.find('{"entities"')
        if start_idx == -1:
            return []

        # ä¿®å¤ï¼šæŸ¥æ‰¾<|im_end|>æ ‡è®°ä½œä¸ºç»“æŸ
        end_marker = '<|im_end|>'
        end_idx = text.find(end_marker, start_idx)
        if end_idx == -1:
            # å¦‚æœæ²¡æœ‰ç»“æŸæ ‡è®°ï¼Œå°è¯•è§£æåˆ°æ–‡æœ¬æœ«å°¾
            json_str = text[start_idx:].strip()
        else:
            json_str = text[start_idx:end_idx].strip()

        # å°è¯•è§£æJSON
        result = json.loads(json_str)
        return result.get("entities", [])
    except Exception as e:
        # è°ƒè¯•ï¼šæ‰“å°å¤±è´¥çš„JSON
        # print(f"JSONè§£æå¤±è´¥: {json_str[:100]}")
        return []

# æ‰§è¡ŒéªŒè¯
print("ğŸš€ å¼€å§‹éªŒè¯...\n")
tp = fp = fn = 0

for sample in tqdm(test_samples, desc="éªŒè¯è¿›åº¦"):
    input_text = sample["input"]
    expected_entities = set(
        (e["type"], e["value"])
        for e in sample.get("output", {}).get("entities", [])
    )

    # æ„é€ prompt
    prompt = (
        f"<|im_start|>system\n"
        f"ä½ æ˜¯ PII æ£€æµ‹ä¸“å®¶ã€‚æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚<|im_end|>\n"
        f"<|im_start|>user\n{input_text}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )

    # æ¨ç†
    inputs = tokenizer(prompt, return_tensors="pt", max_length=MAX_SEQ_LENGTH, truncation=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
        )

    # è§£ç 
    response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    predicted_entities = extract_entities(response)
    predicted_set = set((e["type"], e["value"]) for e in predicted_entities)

    # è®¡ç®—æŒ‡æ ‡
    tp += len(expected_entities & predicted_set)
    fp += len(predicted_set - expected_entities)
    fn += len(expected_entities - predicted_set)

# è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
precision = tp / (tp + fp) if (tp + fp) > 0 else 0
recall = tp / (tp + fn) if (tp + fn) > 0 else 0
f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

print(f"\n{'='*70}")
print(f"ğŸ“Š Epoch 1 éªŒè¯ç»“æœ (åŸºäº {SAMPLE_SIZE} æ ·æœ¬)")
print(f"{'='*70}")
print(f"Precision: {precision*100:.2f}%")
print(f"Recall:    {recall*100:.2f}%")
print(f"F1-Score:  {f1*100:.2f}%")
print(f"\nè¯¦ç»†ç»Ÿè®¡:")
print(f"  TP (æ­£ç¡®æ£€å‡º): {tp}")
print(f"  FP (è¯¯æŠ¥):     {fp}")
print(f"  FN (æ¼æ£€):     {fn}")
print(f"{'='*70}\n")

# ä¿å­˜ç»“æœ
result_file = f"logs/epoch1_validation_gpu1_{SAMPLE_SIZE}samples.json"
with open(result_file, 'w', encoding='utf-8') as f:
    json.dump({
        "model": MODEL_PATH,
        "sample_size": SAMPLE_SIZE,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "tp": tp,
        "fp": fp,
        "fn": fn,
    }, f, indent=2, ensure_ascii=False)

print(f"âœ… ç»“æœå·²ä¿å­˜è‡³: {result_file}")
