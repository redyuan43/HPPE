#!/usr/bin/env python3
"""
Phase 2: è®­ç»ƒå®Œæ•´17ç§PIIæ£€æµ‹æ¨¡å‹
ä½¿ç”¨merged_17pii_train.jsonl (69,215æ ·æœ¬)
"""
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # ä½¿ç”¨GPU0

import json
import sys
from pathlib import Path
from datasets import Dataset
from transformers import TrainingArguments, Trainer
import torch
from datetime import datetime

print("="*70)
print("ğŸš€ Phase 2: å®Œæ•´17ç§PIIæ¨¡å‹è®­ç»ƒ")
print("="*70)
print(f"è®­ç»ƒPIIç±»å‹: 17ç§")
print(f"  - åŸ6ç§: ADDRESS, ORGANIZATION, PERSON_NAME, PHONE_NUMBER, EMAIL, ID_CARD")
print(f"  - æ–°11ç§: BANK_CARD, PASSPORT, HK_MACAU_PASS, POSTAL_CODE, IP_ADDRESS,")
print(f"           MAC_ADDRESS, VEHICLE_PLATE, DRIVER_LICENSE, SOCIAL_SECURITY_CARD,")
print(f"           UNIFIED_SOCIAL_CREDIT_CODE, IPV6_ADDRESS")
print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*70 + "\n")

from unsloth import FastLanguageModel

# é…ç½®
MODEL_NAME = "/home/ivan/.cache/modelscope/hub/Qwen/Qwen3-4B"
TRAIN_DATA = "data/merged_17pii_train.jsonl"
VAL_DATA = "data/merged_pii_dataset_validation.jsonl"
OUTPUT_DIR = "models/pii_qwen4b_17types_final"
MAX_SEQ_LENGTH = 512
LOAD_IN_4BIT = True

# LoRAé…ç½®
LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0

# è®­ç»ƒé…ç½®
BATCH_SIZE = 12
GRADIENT_ACCUMULATION = 3
EPOCHS = 2
LR = 1.5e-4

print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_NAME,
    max_seq_length=MAX_SEQ_LENGTH,
    dtype=None,
    load_in_4bit=LOAD_IN_4BIT,
)

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
print(f"   4bité‡åŒ–: {LOAD_IN_4BIT}")
print(f"   Max seq length: {MAX_SEQ_LENGTH}\n")

print("ğŸ”§ é…ç½®LoRA...")
model = FastLanguageModel.get_peft_model(
    model,
    r=LORA_R,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_alpha=LORA_ALPHA,
    lora_dropout=LORA_DROPOUT,
    bias="none",
    use_gradient_checkpointing="unsloth",
)

print("âœ… LoRAé…ç½®å®Œæˆ")
print(f"   r={LORA_R}, alpha={LORA_ALPHA}, dropout={LORA_DROPOUT}\n")

# åŠ è½½æ•°æ®å¹¶tokenize
print(f"ğŸ“‚ åŠ è½½è®­ç»ƒæ•°æ®: {TRAIN_DATA}")

def load_and_tokenize_jsonl(path: str):
    """åŠ è½½JSONLæ•°æ®å¹¶tokenize"""
    all_input_ids = []
    all_attention_mask = []
    all_labels = []

    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            sample = json.loads(line)

            # æå–è¾“å…¥å’Œå®ä½“
            text = sample["input"]
            entities = sample.get("output", {}).get("entities", [])

            # æ„é€ å®Œæ•´çš„è®­ç»ƒæ–‡æœ¬
            full_text = (
                f"<|im_start|>system\n"
                f"ä½ æ˜¯ PII æ£€æµ‹ä¸“å®¶ã€‚æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚<|im_end|>\n"
                f"<|im_start|>user\n{text}<|im_end|>\n"
                f"<|im_start|>assistant\n"
                f'{{"entities": {json.dumps(entities, ensure_ascii=False)}}}<|im_end|>'
            )

            # Tokenize
            tokenized = tokenizer(
                full_text,
                max_length=MAX_SEQ_LENGTH,
                truncation=True,
                padding="max_length",
                return_tensors=None,
            )

            # è®¾ç½® labels = input_ids
            tokenized["labels"] = tokenized["input_ids"].copy()

            all_input_ids.append(tokenized["input_ids"])
            all_attention_mask.append(tokenized["attention_mask"])
            all_labels.append(tokenized["labels"])

    print(f"âœ… åŠ è½½äº† {len(all_input_ids)} ä¸ªè®­ç»ƒæ ·æœ¬\n")

    return Dataset.from_dict({
        "input_ids": all_input_ids,
        "attention_mask": all_attention_mask,
        "labels": all_labels,
    })

train_dataset = load_and_tokenize_jsonl(TRAIN_DATA)
eval_dataset = load_and_tokenize_jsonl(VAL_DATA)

# è®¡ç®—æ€»steps
total_samples = len(train_dataset)
effective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION
steps_per_epoch = total_samples // effective_batch_size
total_steps = steps_per_epoch * EPOCHS

print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
print("ğŸ“Š è®­ç»ƒé…ç½®æ±‡æ€»")
print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
print(f"è®­ç»ƒæ ·æœ¬æ•°: {total_samples:,}")
print(f"éªŒè¯æ ·æœ¬æ•°: {len(eval_dataset):,}")
print(f"Batch size: {BATCH_SIZE}")
print(f"Gradient accumulation: {GRADIENT_ACCUMULATION}")
print(f"Effective batch size: {effective_batch_size}")
print(f"Learning rate: {LR}")
print(f"Epochs: {EPOCHS}")
print(f"Steps per epoch: {steps_per_epoch}")
print(f"Total steps: {total_steps}")
print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")

# è®­ç»ƒé…ç½®
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRADIENT_ACCUMULATION,
    num_train_epochs=EPOCHS,
    learning_rate=LR,
    fp16=False,  # å…³é—­fp16é¿å…dtypeå†²çª
    bf16=True,   # ä½¿ç”¨bf16ï¼ˆä¸Unslothå…¼å®¹ï¼‰
    logging_steps=10,
    save_strategy="epoch",
    eval_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=False,
    warmup_ratio=0.05,
    lr_scheduler_type="cosine",
    optim="adamw_8bit",
    weight_decay=0.01,
    max_grad_norm=1.0,
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
)

print("ğŸš€ å¼€å§‹è®­ç»ƒ...\n")
start_time = datetime.now()

trainer.train()

end_time = datetime.now()
duration = end_time - start_time

print("\n" + "="*70)
print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
print("="*70)
print(f"å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"æ€»è€—æ—¶: {duration}")
print("="*70 + "\n")

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
print("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
final_output = f"{OUTPUT_DIR}/final"
trainer.model.save_pretrained(final_output)
tokenizer.save_pretrained(final_output)

print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {final_output}")
print(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("="*70)
