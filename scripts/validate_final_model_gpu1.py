#!/usr/bin/env python3
"""
GPU1éš”ç¦»éªŒè¯ - æœ€ç»ˆæ¨¡å‹å¿«é€Ÿæµ‹è¯•
ä¸¥æ ¼é™åˆ¶åªä½¿ç”¨GPU1ï¼Œä¸å½±å“GPU0è®­ç»ƒ
"""

import os
import sys
import argparse

# ========== å…³é”®ï¼šå¿…é¡»åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½® ==========
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
print(f"ğŸ”’ GPUéš”ç¦»è®¾ç½®: CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']}")

import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm
from pathlib import Path

# è§£æå‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser()
parser.add_argument("--model", type=str, required=True, help="æ¨¡å‹è·¯å¾„")
parser.add_argument("--test-data", type=str, default="data/merged_pii_dataset_test.jsonl", help="æµ‹è¯•æ•°æ®è·¯å¾„")
parser.add_argument("--sample-size", type=int, default=50, help="æµ‹è¯•æ ·æœ¬æ•°é‡")
parser.add_argument("--max-seq-length", type=int, default=512, help="æœ€å¤§åºåˆ—é•¿åº¦")
args = parser.parse_args()

MODEL_PATH = args.model
TEST_DATA = args.test_data
SAMPLE_SIZE = args.sample_size
MAX_SEQ_LENGTH = args.max_seq_length

print(f"\n{'='*70}")
print(f"æœ€ç»ˆæ¨¡å‹éªŒè¯ (GPU1éš”ç¦»)")
print(f"{'='*70}")
print(f"æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
print(f"æµ‹è¯•æ•°æ®: {TEST_DATA}")
print(f"æ ·æœ¬æ•°é‡: {SAMPLE_SIZE}")
print(f"GPUè®¾ç½®: ä»…ä½¿ç”¨GPU1 (ä¸å½±å“GPU0è®­ç»ƒ)")
print(f"{'='*70}\n")

# éªŒè¯GPUéš”ç¦»
print("ğŸ” éªŒè¯GPUéš”ç¦»...")
print(f"  PyTorchå¯è§è®¾å¤‡æ•°: {torch.cuda.device_count()}")
print(f"  å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")
if torch.cuda.is_available():
    print(f"  è®¾å¤‡åç§°: {torch.cuda.get_device_name(0)}")  # æ³¨æ„ï¼šè¿™é‡Œçš„0æ˜¯ç›¸å¯¹ç´¢å¼•
print()

# åŠ è½½æ¨¡å‹
print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
try:
    # æ£€æŸ¥æ˜¯å¦æ˜¯LoRA checkpoint
    model_path = Path(MODEL_PATH)
    adapter_config = model_path / "adapter_config.json"

    if adapter_config.exists():
        # LoRA checkpoint
        from peft import PeftModel, PeftConfig

        peft_config = PeftConfig.from_pretrained(MODEL_PATH)
        base_model_path = peft_config.base_model_name_or_path
        print(f"  åŸºç¡€æ¨¡å‹: {base_model_path}")
        print(f"  LoRAé€‚é…å™¨: {MODEL_PATH}")

        model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        model = PeftModel.from_pretrained(model, MODEL_PATH)
        tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (åŸºç¡€æ¨¡å‹ + LoRA)\n")
    else:
        # å®Œæ•´æ¨¡å‹æˆ–mergedæ¨¡å‹
        print(f"  å®Œæ•´æ¨¡å‹: {MODEL_PATH}")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="auto",
        )
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (å®Œæ•´æ¨¡å‹)\n")

except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# åŠ è½½æµ‹è¯•æ•°æ®
print(f"ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
test_samples = []
with open(TEST_DATA, 'r', encoding='utf-8') as f:
    for i, line in enumerate(f):
        if i >= SAMPLE_SIZE:
            break
        test_samples.append(json.loads(line))
print(f"âœ… åŠ è½½äº† {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬\n")

# éªŒè¯å‡½æ•°
def extract_entities(text):
    """æå–æ¨¡å‹é¢„æµ‹çš„å®ä½“"""
    try:
        # æŸ¥æ‰¾JSONéƒ¨åˆ†
        start_idx = text.find('{"entities"')
        if start_idx == -1:
            return []

        # ä¿®å¤ï¼šæŸ¥æ‰¾<|im_end|>æ ‡è®°ä½œä¸ºç»“æŸ
        end_marker = '<|im_end|>'
        end_idx = text.find(end_marker, start_idx)
        if end_idx == -1:
            # å¦‚æœæ²¡æœ‰ç»“æŸæ ‡è®°ï¼Œå°è¯•è§£æåˆ°æ–‡æœ¬æœ«å°¾
            json_str = text[start_idx:].strip()
        else:
            json_str = text[start_idx:end_idx].strip()

        # å°è¯•è§£æJSON
        result = json.loads(json_str)
        return result.get("entities", [])
    except Exception as e:
        # è°ƒè¯•ï¼šæ‰“å°å¤±è´¥çš„JSON
        # print(f"JSONè§£æå¤±è´¥: {json_str[:100]}")
        return []

# æ‰§è¡ŒéªŒè¯
print("ğŸš€ å¼€å§‹éªŒè¯...\n")
tp = fp = fn = 0

for sample in tqdm(test_samples, desc="éªŒè¯è¿›åº¦"):
    input_text = sample["input"]
    expected_entities = set(
        (e["type"], e["value"])
        for e in sample.get("output", {}).get("entities", [])
    )

    # æ„é€ prompt
    prompt = (
        f"<|im_start|>system\n"
        f"ä½ æ˜¯ PII æ£€æµ‹ä¸“å®¶ã€‚æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚<|im_end|>\n"
        f"<|im_start|>user\n{input_text}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )

    # æ¨ç†
    inputs = tokenizer(prompt, return_tensors="pt", max_length=MAX_SEQ_LENGTH, truncation=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
        )

    # è§£ç 
    response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    predicted_entities = extract_entities(response)
    predicted_set = set((e["type"], e["value"]) for e in predicted_entities)

    # è®¡ç®—æŒ‡æ ‡
    tp += len(expected_entities & predicted_set)
    fp += len(predicted_set - expected_entities)
    fn += len(expected_entities - predicted_set)

# è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
precision = tp / (tp + fp) if (tp + fp) > 0 else 0
recall = tp / (tp + fn) if (tp + fn) > 0 else 0
f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

print(f"\n{'='*70}")
print(f"ğŸ“Š éªŒè¯ç»“æœ (åŸºäº {SAMPLE_SIZE} æ ·æœ¬)")
print(f"{'='*70}")
print(f"Precision: {precision*100:.2f}%")
print(f"Recall:    {recall*100:.2f}%")
print(f"F1-Score:  {f1*100:.2f}%")
print(f"\nè¯¦ç»†ç»Ÿè®¡:")
print(f"  TP (æ­£ç¡®æ£€å‡º): {tp}")
print(f"  FP (è¯¯æŠ¥):     {fp}")
print(f"  FN (æ¼æ£€):     {fn}")
print(f"{'='*70}\n")

# ä¿å­˜ç»“æœ
model_name = Path(MODEL_PATH).name
result_file = f"logs/validation_{model_name}_{SAMPLE_SIZE}samples.json"
with open(result_file, 'w', encoding='utf-8') as f:
    json.dump({
        "model": MODEL_PATH,
        "sample_size": SAMPLE_SIZE,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "tp": tp,
        "fp": fp,
        "fn": fn,
    }, f, indent=2, ensure_ascii=False)

print(f"âœ… ç»“æœå·²ä¿å­˜è‡³: {result_file}")
