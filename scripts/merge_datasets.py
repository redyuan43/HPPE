#!/usr/bin/env python3
"""
åˆå¹¶å¤šä¸ª PII æ•°æ®é›†ä¸ºç»Ÿä¸€çš„ SFT è®­ç»ƒæ ¼å¼

æ”¯æŒçš„æ•°æ®é›†æ ¼å¼ï¼š
1. ai4privacy - PIIè„±æ•æ•°æ®é›†
2. MSRA NER - BIOæ ‡æ³¨æ ¼å¼
3. åˆæˆæ•°æ® - å·²ç»æ˜¯SFTæ ¼å¼

è¾“å‡ºæ ¼å¼ï¼š
{
  "instruction": "æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚",
  "input": "æˆ‘å«å¼ ä¸‰ï¼Œç”µè¯13800138000ã€‚",
  "output": {
    "entities": [
      {"type": "PERSON_NAME", "value": "å¼ ä¸‰", "start": 2, "end": 4},
      {"type": "PHONE_NUMBER", "value": "13800138000", "start": 6, "end": 17}
    ]
  }
}

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # åˆå¹¶æ‰€æœ‰æ•°æ®é›†ï¼ŒæŒ‰æ¨èæ¯”ä¾‹
    python scripts/merge_datasets.py --all --output data/merged_pii_dataset.jsonl

    # è‡ªå®šä¹‰æ•°æ®é›†å’Œæ¯”ä¾‹
    python scripts/merge_datasets.py \\
        --datasets ai4privacy:0.3 msra:0.3 synthetic:0.4 \\
        --output data/custom_dataset.jsonl

    # æŒ‡å®šæ€»æ ·æœ¬æ•°
    python scripts/merge_datasets.py --all --total-samples 50000 --output data/pii_50k.jsonl
"""

import argparse
import json
import random
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional
from collections import defaultdict

try:
    from datasets import load_from_disk, Dataset
except ImportError:
    print("âŒ ç¼ºå°‘ datasets åº“ã€‚è¯·å…ˆå®‰è£…ï¼š")
    print("   pip install datasets")
    sys.exit(1)


# PIIç±»å‹æ˜ å°„ï¼ˆæ ‡å‡†åŒ–ä¸åŒæ•°æ®é›†çš„å®ä½“ç±»å‹ï¼‰
ENTITY_TYPE_MAPPING = {
    # MSRA NERæ ‡ç­¾åˆ°æ ‡å‡†ç±»å‹
    "PER": "PERSON_NAME",
    "nr": "PERSON_NAME",
    "LOC": "ADDRESS",
    "ns": "ADDRESS",
    "ORG": "ORGANIZATION",
    "nt": "ORGANIZATION",

    # ai4privacyæ ‡ç­¾åˆ°æ ‡å‡†ç±»å‹
    "NAME": "PERSON_NAME",
    "PERSON": "PERSON_NAME",
    "EMAIL_ADDRESS": "EMAIL",
    "PHONE": "PHONE_NUMBER",
    "PHONE_NUM": "PHONE_NUMBER",
    "LOCATION": "ADDRESS",
    "CITY": "ADDRESS",
    "STREET": "ADDRESS",

    # BigCodeæ ‡ç­¾åˆ°æ ‡å‡†ç±»å‹
    "KEY": "API_KEY",
    "IP": "IP_ADDRESS",
    "USERNAME": "USERNAME",

    # å·²ç»æ˜¯æ ‡å‡†ç±»å‹çš„
    "PERSON_NAME": "PERSON_NAME",
    "PHONE_NUMBER": "PHONE_NUMBER",
    "EMAIL": "EMAIL",
    "ID_CARD": "ID_CARD",
    "BANK_CARD": "BANK_CARD",
    "ADDRESS": "ADDRESS",
    "ORGANIZATION": "ORGANIZATION",
    "IP_ADDRESS": "IP_ADDRESS",
}


def convert_ai4privacy_sample(sample: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    è½¬æ¢ ai4privacy æ•°æ®é›†æ ·æœ¬ä¸º SFT æ ¼å¼

    ai4privacy æ ¼å¼ï¼š
    {
      "source_text": "My name is [NAME] and I live in [CITY]",
      "target_text": "My name is John Smith and I live in New York",
      "privacy_mask": {
        "[NAME]": "John Smith",
        "[CITY]": "New York"
      }
    }
    """
    try:
        target_text = sample.get("target_text", "")
        privacy_mask = sample.get("privacy_mask", {})

        if not target_text or not privacy_mask:
            return None

        # æå–å®ä½“
        entities = []
        for placeholder, value in privacy_mask.items():
            # ä»å ä½ç¬¦æ¨æ–­ç±»å‹
            placeholder_upper = placeholder.upper()

            if "NAME" in placeholder_upper:
                entity_type = "PERSON_NAME"
            elif "EMAIL" in placeholder_upper:
                entity_type = "EMAIL"
            elif "PHONE" in placeholder_upper:
                entity_type = "PHONE_NUMBER"
            elif "CITY" in placeholder_upper or "LOCATION" in placeholder_upper or "STREET" in placeholder_upper:
                entity_type = "ADDRESS"
            elif "ORG" in placeholder_upper or "COMPANY" in placeholder_upper:
                entity_type = "ORGANIZATION"
            else:
                # è·³è¿‡æœªçŸ¥ç±»å‹
                continue

            # åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾å®ä½“ä½ç½®
            start = target_text.find(value)
            if start != -1:
                entities.append({
                    "type": entity_type,
                    "value": value,
                    "start": start,
                    "end": start + len(value)
                })

        if not entities:
            return None

        return {
            "instruction": "æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚",
            "input": target_text,
            "output": {
                "entities": entities
            }
        }

    except Exception as e:
        return None


def convert_msra_sample(sample: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    è½¬æ¢ MSRA NER æ•°æ®é›†æ ·æœ¬ä¸º SFT æ ¼å¼

    MSRA æ ¼å¼ï¼ˆBIOæ ‡æ³¨ï¼‰ï¼š
    {
      "tokens": ["æˆ‘", "å«", "å¼ ", "ä¸‰"],
      "ner_tags": [0, 0, 1, 2]  # 0=O, 1=B-PER, 2=I-PER, ...
    }
    """
    try:
        tokens = sample.get("tokens", [])
        ner_tags = sample.get("ner_tags", [])

        if not tokens or not ner_tags or len(tokens) != len(ner_tags):
            return None

        # æ ‡ç­¾IDåˆ°åç§°çš„æ˜ å°„ï¼ˆMSRAä½¿ç”¨çš„æ ‡ç­¾ï¼‰
        # 0=O, 1=B-PER, 2=I-PER, 3=B-ORG, 4=I-ORG, 5=B-LOC, 6=I-LOC
        tag_names = {
            0: "O",
            1: "B-PER", 2: "I-PER",
            3: "B-ORG", 4: "I-ORG",
            5: "B-LOC", 6: "I-LOC"
        }

        # é‡å»ºæ–‡æœ¬
        text = "".join(tokens)

        # æå–å®ä½“
        entities = []
        current_entity = None
        current_start = 0

        for i, (token, tag_id) in enumerate(zip(tokens, ner_tags)):
            tag = tag_names.get(tag_id, "O")

            if tag.startswith("B-"):
                # ä¿å­˜ä¹‹å‰çš„å®ä½“
                if current_entity:
                    entity_type = ENTITY_TYPE_MAPPING.get(current_entity["raw_type"], current_entity["raw_type"])
                    entities.append({
                        "type": entity_type,
                        "value": current_entity["value"],
                        "start": current_entity["start"],
                        "end": current_entity["end"]
                    })

                # å¼€å§‹æ–°å®ä½“
                current_entity = {
                    "raw_type": tag[2:],  # PER, ORG, LOC
                    "value": token,
                    "start": current_start,
                    "end": current_start + len(token)
                }

            elif tag.startswith("I-") and current_entity:
                # ç»§ç»­å½“å‰å®ä½“
                current_entity["value"] += token
                current_entity["end"] = current_start + len(token)

            else:  # O
                # ä¿å­˜ä¹‹å‰çš„å®ä½“
                if current_entity:
                    entity_type = ENTITY_TYPE_MAPPING.get(current_entity["raw_type"], current_entity["raw_type"])
                    entities.append({
                        "type": entity_type,
                        "value": current_entity["value"],
                        "start": current_entity["start"],
                        "end": current_entity["end"]
                    })
                    current_entity = None

            current_start += len(token)

        # ä¿å­˜æœ€åçš„å®ä½“
        if current_entity:
            entity_type = ENTITY_TYPE_MAPPING.get(current_entity["raw_type"], current_entity["raw_type"])
            entities.append({
                "type": entity_type,
                "value": current_entity["value"],
                "start": current_entity["start"],
                "end": current_entity["end"]
            })

        if not entities:
            return None

        return {
            "instruction": "æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚",
            "input": text,
            "output": {
                "entities": entities
            }
        }

    except Exception as e:
        return None


def load_dataset_samples(
    dataset_path: Path,
    dataset_type: str,
    max_samples: Optional[int] = None
) -> List[Dict[str, Any]]:
    """
    åŠ è½½å¹¶è½¬æ¢æ•°æ®é›†

    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„
        dataset_type: æ•°æ®é›†ç±»å‹ (ai4privacy, msra, synthetic)
        max_samples: æœ€å¤§æ ·æœ¬æ•°

    Returns:
        è½¬æ¢åçš„æ ·æœ¬åˆ—è¡¨
    """
    print(f"\nåŠ è½½æ•°æ®é›†: {dataset_path.name} ({dataset_type})")

    samples = []

    if dataset_type == "synthetic":
        # åˆæˆæ•°æ®å·²ç»æ˜¯SFTæ ¼å¼
        if dataset_path.suffix == ".jsonl":
            with open(dataset_path, "r", encoding="utf-8") as f:
                for line in f:
                    sample = json.loads(line)
                    samples.append(sample)
                    if max_samples and len(samples) >= max_samples:
                        break
        else:
            with open(dataset_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                samples = data if isinstance(data, list) else [data]
                if max_samples:
                    samples = samples[:max_samples]

    elif dataset_type == "ai4privacy":
        # ai4privacy æ•°æ®é›†
        dataset = load_from_disk(str(dataset_path))

        # è·å–è®­ç»ƒé›†
        if hasattr(dataset, "keys"):
            split = dataset["train"] if "train" in dataset else list(dataset.values())[0]
        else:
            split = dataset

        # è½¬æ¢æ ·æœ¬
        for i, sample in enumerate(split):
            if max_samples and i >= max_samples:
                break

            converted = convert_ai4privacy_sample(sample)
            if converted:
                samples.append(converted)

    elif dataset_type == "msra":
        # MSRA NER æ•°æ®é›†
        dataset = load_from_disk(str(dataset_path))

        # è·å–è®­ç»ƒé›†
        if hasattr(dataset, "keys"):
            split = dataset["train"] if "train" in dataset else list(dataset.values())[0]
        else:
            split = dataset

        # è½¬æ¢æ ·æœ¬
        for i, sample in enumerate(split):
            if max_samples and i >= max_samples:
                break

            converted = convert_msra_sample(sample)
            if converted:
                samples.append(converted)

    print(f"  âœ“ åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")
    return samples


def merge_datasets(
    dataset_configs: List[Dict[str, Any]],
    total_samples: Optional[int] = None,
    output_path: Optional[Path] = None,
    train_val_test_split: tuple = (0.8, 0.1, 0.1),
    seed: int = 42
) -> Dict[str, List[Dict[str, Any]]]:
    """
    åˆå¹¶å¤šä¸ªæ•°æ®é›†

    Args:
        dataset_configs: æ•°æ®é›†é…ç½®åˆ—è¡¨ [{"path": Path, "type": str, "ratio": float}]
        total_samples: æ€»æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰æ ·æœ¬ï¼‰
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        train_val_test_split: è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†æ¯”ä¾‹
        seed: éšæœºç§å­

    Returns:
        åˆå¹¶åçš„æ•°æ®é›† {"train": [...], "validation": [...], "test": [...]}
    """
    random.seed(seed)

    print(f"\n{'='*70}")
    print("åˆå¹¶ PII æ•°æ®é›†")
    print(f"{'='*70}")
    print(f"æ•°æ®é›†é…ç½®:")
    for config in dataset_configs:
        print(f"  - {config['type']}: {config['ratio']*100:.0f}%")
    if total_samples:
        print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"åˆ’åˆ†æ¯”ä¾‹: è®­ç»ƒ{train_val_test_split[0]*100:.0f}% / "
          f"éªŒè¯{train_val_test_split[1]*100:.0f}% / "
          f"æµ‹è¯•{train_val_test_split[2]*100:.0f}%")
    print()

    # åŠ è½½æ‰€æœ‰æ•°æ®é›†
    all_samples = []

    for config in dataset_configs:
        # è®¡ç®—æ­¤æ•°æ®é›†éœ€è¦çš„æ ·æœ¬æ•°
        if total_samples:
            max_samples = int(total_samples * config["ratio"])
        else:
            max_samples = None

        # åŠ è½½æ ·æœ¬
        samples = load_dataset_samples(
            config["path"],
            config["type"],
            max_samples
        )

        all_samples.extend(samples)

    print(f"\nâœ“ æ€»å…±åŠ è½½ {len(all_samples)} ä¸ªæ ·æœ¬")

    # æ‰“ä¹±æ•°æ®
    print("\næ‰“ä¹±æ•°æ®...")
    random.shuffle(all_samples)

    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
    print("åˆ’åˆ†æ•°æ®é›†...")
    train_size = int(len(all_samples) * train_val_test_split[0])
    val_size = int(len(all_samples) * train_val_test_split[1])

    splits = {
        "train": all_samples[:train_size],
        "validation": all_samples[train_size:train_size + val_size],
        "test": all_samples[train_size + val_size:]
    }

    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\næ•°æ®é›†ç»Ÿè®¡:")
    for split_name, split_samples in splits.items():
        print(f"  {split_name}: {len(split_samples)} æ ·æœ¬")

    # ç»Ÿè®¡å®ä½“ç±»å‹åˆ†å¸ƒ
    print(f"\nå®ä½“ç±»å‹åˆ†å¸ƒï¼ˆè®­ç»ƒé›†ï¼‰:")
    entity_type_counts = defaultdict(int)
    total_entities = 0

    for sample in splits["train"]:
        for entity in sample["output"]["entities"]:
            entity_type_counts[entity["type"]] += 1
            total_entities += 1

    for entity_type, count in sorted(entity_type_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = count / total_entities * 100 if total_entities > 0 else 0
        print(f"  {entity_type}: {count} ({percentage:.1f}%)")

    # ä¿å­˜åˆ°æ–‡ä»¶
    if output_path:
        print(f"\nä¿å­˜åˆ°æ–‡ä»¶...")
        output_path.parent.mkdir(parents=True, exist_ok=True)

        if output_path.suffix == ".jsonl":
            # JSONLæ ¼å¼ - æ¯ä¸ªsplitåˆ†åˆ«ä¿å­˜
            for split_name, split_samples in splits.items():
                split_path = output_path.with_name(f"{output_path.stem}_{split_name}.jsonl")
                with open(split_path, "w", encoding="utf-8") as f:
                    for sample in split_samples:
                        f.write(json.dumps(sample, ensure_ascii=False) + "\n")
                print(f"  âœ“ {split_name}: {split_path}")

        else:
            # JSONæ ¼å¼ - ä¿å­˜ä¸ºä¸€ä¸ªæ–‡ä»¶
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(splits, f, ensure_ascii=False, indent=2)
            print(f"  âœ“ {output_path}")

    # æ˜¾ç¤ºç¤ºä¾‹
    print(f"\nç¤ºä¾‹æ•°æ®ï¼ˆè®­ç»ƒé›†ï¼‰:")
    example = splits["train"][0]
    print(f"  è¾“å…¥: {example['input'][:100]}...")
    print(f"  å®ä½“æ•°: {len(example['output']['entities'])}")
    print(f"  å®ä½“: {[(e['type'], e['value']) for e in example['output']['entities'][:3]]}")

    return splits


def main():
    parser = argparse.ArgumentParser(
        description="åˆå¹¶å¤šä¸ª PII æ•°æ®é›†ä¸ºç»Ÿä¸€çš„ SFT è®­ç»ƒæ ¼å¼",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨æ¨èé…ç½®åˆå¹¶æ‰€æœ‰æ•°æ®é›†
  python scripts/merge_datasets.py --all --output data/merged_pii_dataset.jsonl

  # è‡ªå®šä¹‰æ•°æ®é›†å’Œæ¯”ä¾‹
  python scripts/merge_datasets.py \\
      --datasets ai4privacy:0.3 msra:0.3 synthetic:0.4 \\
      --output data/custom_dataset.jsonl

  # æŒ‡å®šæ€»æ ·æœ¬æ•°
  python scripts/merge_datasets.py --all --total-samples 50000

  # è‡ªå®šä¹‰åˆ’åˆ†æ¯”ä¾‹
  python scripts/merge_datasets.py --all --split 0.7 0.2 0.1
        """
    )

    parser.add_argument(
        "--datasets",
        nargs="+",
        help="æ•°æ®é›†é…ç½®ï¼Œæ ¼å¼ï¼šname:ratio (ä¾‹å¦‚: ai4privacy:0.3 msra:0.3)"
    )

    parser.add_argument(
        "--all",
        action="store_true",
        help="ä½¿ç”¨æ¨èé…ç½® (ai4privacy:0.3 + msra:0.3 + synthetic:0.4)"
    )

    parser.add_argument(
        "--data-dir",
        type=str,
        default="data/pii_datasets",
        help="æ•°æ®é›†æ ¹ç›®å½• (é»˜è®¤: data/pii_datasets)"
    )

    parser.add_argument(
        "--output",
        type=str,
        default="data/merged_pii_dataset.jsonl",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: data/merged_pii_dataset.jsonl)"
    )

    parser.add_argument(
        "--total-samples",
        type=int,
        help="æ€»æ ·æœ¬æ•°ï¼ˆä¸æŒ‡å®šåˆ™ä½¿ç”¨æ‰€æœ‰æ ·æœ¬ï¼‰"
    )

    parser.add_argument(
        "--split",
        nargs=3,
        type=float,
        default=[0.8, 0.1, 0.1],
        help="è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†åˆ’åˆ†æ¯”ä¾‹ (é»˜è®¤: 0.8 0.1 0.1)"
    )

    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éšæœºç§å­ (é»˜è®¤: 42)"
    )

    args = parser.parse_args()

    # è§£ææ•°æ®é›†é…ç½®
    data_dir = Path(args.data_dir)

    if args.all:
        # æ¨èé…ç½®
        dataset_configs = [
            {
                "path": data_dir / "ai4privacy",
                "type": "ai4privacy",
                "ratio": 0.3
            },
            {
                "path": data_dir / "msra",
                "type": "msra",
                "ratio": 0.3
            },
            {
                "path": data_dir / "synthetic_pii.jsonl",
                "type": "synthetic",
                "ratio": 0.4
            }
        ]
    elif args.datasets:
        # è‡ªå®šä¹‰é…ç½®
        dataset_configs = []
        for config_str in args.datasets:
            name, ratio = config_str.split(":")
            ratio = float(ratio)

            if name == "synthetic":
                path = data_dir / "synthetic_pii.jsonl"
                dataset_type = "synthetic"
            else:
                path = data_dir / name
                dataset_type = name

            dataset_configs.append({
                "path": path,
                "type": dataset_type,
                "ratio": ratio
            })
    else:
        parser.print_help()
        return

    # éªŒè¯æ¯”ä¾‹æ€»å’Œ
    total_ratio = sum(config["ratio"] for config in dataset_configs)
    if abs(total_ratio - 1.0) > 0.01:
        print(f"âš ï¸  è­¦å‘Šï¼šæ¯”ä¾‹æ€»å’Œä¸º {total_ratio:.2f}ï¼Œä¸ç­‰äº 1.0")

    # åˆå¹¶æ•°æ®é›†
    output_path = Path(args.output)
    splits = merge_datasets(
        dataset_configs=dataset_configs,
        total_samples=args.total_samples,
        output_path=output_path,
        train_val_test_split=tuple(args.split),
        seed=args.seed
    )

    print(f"\nğŸ‰ æ•°æ®é›†åˆå¹¶å®Œæˆï¼")
    print(f"\nåç»­æ­¥éª¤:")
    print(f"  1. æŸ¥çœ‹ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶: {output_path}")
    print(f"  2. ä½¿ç”¨è®­ç»ƒè„šæœ¬å¼€å§‹è®­ç»ƒ")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\n\nâŒ å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
