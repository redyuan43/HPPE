#!/bin/bash

###############################################################################
# Qwen3-8B-AWQ æ¨¡å‹ä¸‹è½½è„šæœ¬
#
# ä¸“ä¸º RTX 3060 12GB ä¼˜åŒ–
# æ¨¡å‹: Qwen/Qwen3-8B-AWQ
# å¤§å°: ~5GB (4-bit é‡åŒ–)
# æ˜¾å­˜: ~6GB
#
# ä½¿ç”¨æ–¹æ³•:
#   ./scripts/download_qwen3_model.sh
#   æˆ–åå°è¿è¡Œ:
#   nohup ./scripts/download_qwen3_model.sh > qwen3_download.log 2>&1 &
###############################################################################

set -e

# é¢œè‰²è¾“å‡º
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
RED='\033[0;31m'
NC='\033[0m'

log() {
    echo -e "${BLUE}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} $1"
}

info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# é…ç½®
MODEL_NAME="Qwen/Qwen3-8B-AWQ"
MODEL_SIZE="~5GB"

echo ""
echo "======================================================================"
echo "                Qwen3-8B-AWQ æ¨¡å‹ä¸‹è½½å·¥å…·"
echo "======================================================================"
echo ""
echo "æ¨¡å‹ä¿¡æ¯:"
echo "  åç§°: $MODEL_NAME"
echo "  å¤§å°: $MODEL_SIZE (4-bit AWQ é‡åŒ–)"
echo "  æ˜¾å­˜: ~6GB"
echo "  ä¸Šä¸‹æ–‡: 32K-131K tokens"
echo "  ç‰¹æ€§: Thinking Mode (æ€è€ƒæ¨¡å¼)"
echo ""
echo "ç¡¬ä»¶è¦æ±‚:"
echo "  GPU: RTX 3060 12GB âœ“"
echo "  vLLM: >=0.8.5"
echo "  ç£ç›˜: è‡³å°‘ 10GB å¯ç”¨ç©ºé—´"
echo ""
echo "======================================================================"
echo ""

# æ£€æŸ¥ conda ç¯å¢ƒ
if ! command -v conda &> /dev/null; then
    error "conda æœªæ‰¾åˆ°"
    exit 1
fi

# æ¿€æ´»ç¯å¢ƒ
log "æ¿€æ´» conda ç¯å¢ƒ..."
source "$(conda info --base)/etc/profile.d/conda.sh"
conda activate hppe || {
    error "æ— æ³•æ¿€æ´» hppe ç¯å¢ƒï¼Œè¯·å…ˆè¿è¡Œ ./scripts/setup_vllm_env.sh"
    exit 1
}

info "âœ“ ç¯å¢ƒå·²æ¿€æ´»: hppe"

# æ£€æŸ¥ç£ç›˜ç©ºé—´
log "æ£€æŸ¥ç£ç›˜ç©ºé—´..."
AVAILABLE_SPACE=$(df -BG /home/ivan | tail -1 | awk '{print $4}' | sed 's/G//')
REQUIRED_SPACE=10

if [ "$AVAILABLE_SPACE" -lt "$REQUIRED_SPACE" ]; then
    warn "ç£ç›˜ç©ºé—´ä¸è¶³ï¼ˆå¯ç”¨: ${AVAILABLE_SPACE}GBï¼Œéœ€è¦: ${REQUIRED_SPACE}GBï¼‰"
    exit 1
fi

info "âœ“ ç£ç›˜ç©ºé—´å……è¶³: ${AVAILABLE_SPACE}GB"

# å®‰è£… huggingface-hub
log "æ£€æŸ¥ huggingface-hub..."
if ! python -c "import huggingface_hub" 2>/dev/null; then
    info "å®‰è£… huggingface-hub..."
    pip install -q -U huggingface-hub
fi
info "âœ“ huggingface-hub å·²å°±ç»ª"

echo ""
log "å¼€å§‹ä¸‹è½½ $MODEL_NAME..."
echo ""

# ä¸‹è½½æ¨¡å‹
python -c "
from huggingface_hub import snapshot_download
import sys

try:
    print('æ­£åœ¨ä¸‹è½½ $MODEL_NAME...')
    print('è¿™å¯èƒ½éœ€è¦ 5-15 åˆ†é’Ÿï¼ˆå–å†³äºç½‘ç»œé€Ÿåº¦ï¼‰')
    print('')

    model_path = snapshot_download(
        repo_id='$MODEL_NAME',
        resume_download=True,
        local_files_only=False
    )

    print('')
    print('âœ“ ä¸‹è½½å®Œæˆ!')
    print(f'æ¨¡å‹è·¯å¾„: {model_path}')

except KeyboardInterrupt:
    print('\\nä¸‹è½½è¢«ç”¨æˆ·ä¸­æ–­')
    sys.exit(130)
except Exception as e:
    print(f'âœ— ä¸‹è½½å¤±è´¥: {e}')
    sys.exit(1)
"

if [ $? -eq 0 ]; then
    echo ""
    log "éªŒè¯æ¨¡å‹..."

    # éªŒè¯æ¨¡å‹æ–‡ä»¶
    python -c "
from transformers import AutoConfig, AutoTokenizer

try:
    config = AutoConfig.from_pretrained('$MODEL_NAME')
    tokenizer = AutoTokenizer.from_pretrained('$MODEL_NAME')

    print('âœ“ æ¨¡å‹éªŒè¯é€šè¿‡')
    print(f'  æ€»å‚æ•°: {config.num_parameters:,}')
    print(f'  å±‚æ•°: {config.num_hidden_layers}')
    print(f'  è¯è¡¨å¤§å°: {len(tokenizer):,}')
    print(f'  ä¸Šä¸‹æ–‡é•¿åº¦: {config.max_position_embeddings:,}')
except Exception as e:
    print(f'âœ— éªŒè¯å¤±è´¥: {e}')
    exit(1)
"

    echo ""
    echo "======================================================================"
    log "Qwen3-8B-AWQ ä¸‹è½½å®Œæˆï¼"
    echo "======================================================================"
    echo ""

    # æ˜¾ç¤ºä½¿ç”¨æŒ‡å—
    info "ä¸‹ä¸€æ­¥æ“ä½œ:"
    echo ""
    echo "1. å¯åŠ¨ vLLM æœåŠ¡:"
    echo "   MODEL_NAME=Qwen/Qwen3-8B-AWQ ./scripts/start_vllm.sh"
    echo ""
    echo "2. æˆ–ä½¿ç”¨è‡ªå®šä¹‰é…ç½®:"
    echo "   conda activate hppe"
    echo "   python -m vllm.entrypoints.openai.api_server \\"
    echo "     --model Qwen/Qwen3-8B-AWQ \\"
    echo "     --quantization awq \\"
    echo "     --gpu-memory-utilization 0.85 \\"
    echo "     --max-model-len 32768 \\"
    echo "     --port 8000"
    echo ""
    echo "3. æµ‹è¯• LLM å¼•æ“:"
    echo "   PYTHONPATH=/home/ivan/HPPE/src python examples/llm_engine_example.py"
    echo ""
    echo "4. ä½¿ç”¨ Thinking Mode (æ€è€ƒæ¨¡å¼):"
    echo "   # åœ¨ prompt ä¸­æ·»åŠ  <|im_start|>thinking è§¦å‘"
    echo ""

    log "å®Œæˆï¼ğŸ‰"
else
    error "ä¸‹è½½å¤±è´¥"
    exit 1
fi
