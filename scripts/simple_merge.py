#!/usr/bin/env python3
"""ç®€å•åˆå¹¶SFTæ•°æ®é›†"""

import json
import random
from pathlib import Path

# é…ç½®
INPUT_FILES = [
    "data/pii_datasets/synthetic_pii.jsonl",
    "data/pii_datasets/msra/msra_train_sft.jsonl",
    "data/pii_datasets/peoples_daily/peoples_daily_train_sft.jsonl",
]

OUTPUT_DIR = Path("data")
TRAIN_RATIO = 0.8
VAL_RATIO = 0.1
TEST_RATIO = 0.1

# è¯»å–æ‰€æœ‰æ•°æ®
print("\nğŸ“š è¯»å–æ•°æ®é›†...")
all_samples = []

for file_path in INPUT_FILES:
    path = Path(file_path)
    if not path.exists():
        print(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        continue

    with open(path, "r", encoding="utf-8") as f:
        samples = [json.loads(line) for line in f if line.strip()]
        all_samples.extend(samples)
        print(f"  âœ“ {path.name}: {len(samples):,} æ ·æœ¬")

print(f"\næ€»è®¡: {len(all_samples):,} æ ·æœ¬")

# éšæœºæ‰“ä¹±
print("\nğŸ”€ éšæœºæ‰“ä¹±æ•°æ®...")
random.seed(42)
random.shuffle(all_samples)

# åˆ†å‰²æ•°æ®
train_size = int(len(all_samples) * TRAIN_RATIO)
val_size = int(len(all_samples) * VAL_RATIO)

train_samples = all_samples[:train_size]
val_samples = all_samples[train_size:train_size + val_size]
test_samples = all_samples[train_size + val_size:]

print(f"  è®­ç»ƒé›†: {len(train_samples):,} æ ·æœ¬")
print(f"  éªŒè¯é›†: {len(val_samples):,} æ ·æœ¬")
print(f"  æµ‹è¯•é›†: {len(test_samples):,} æ ·æœ¬")

# ä¿å­˜
print("\nğŸ’¾ ä¿å­˜æ•°æ®é›†...")
OUTPUT_DIR.mkdir(exist_ok=True)

datasets = [
    ("merged_pii_dataset_train.jsonl", train_samples),
    ("merged_pii_dataset_validation.jsonl", val_samples),
    ("merged_pii_dataset_test.jsonl", test_samples),
]

for filename, samples in datasets:
    output_path = OUTPUT_DIR / filename
    with open(output_path, "w", encoding="utf-8") as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")
    print(f"  âœ“ {filename}: {len(samples):,} æ ·æœ¬")

print("\nğŸ‰ åˆå¹¶å®Œæˆï¼")
