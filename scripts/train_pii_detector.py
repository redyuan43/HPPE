#!/usr/bin/env python3
"""
è®­ç»ƒ PII æ£€æµ‹æ¨¡å‹

æ”¯æŒåŠŸèƒ½ï¼š
1. LoRA å¾®è°ƒ - å‚æ•°é«˜æ•ˆè®­ç»ƒ
2. å¤šç§åŸºç¡€æ¨¡å‹ - Qwen2/Qwen3 ç³»åˆ—
3. è‡ªåŠ¨è¯„ä¼° - ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1åˆ†æ•°
4. æ‰¹å¤„ç†ä¼˜åŒ– - é€‚é…æ‰¹é‡è„±æ•åœºæ™¯

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # åŸºç¡€è®­ç»ƒï¼ˆä½¿ç”¨ LoRAï¼‰
    python scripts/train_pii_detector.py \\
        --model Qwen/Qwen2-1.5B \\
        --data data/merged_pii_dataset_train.jsonl \\
        --output models/pii_detector_qwen2_1.5b

    # é«˜çº§é…ç½®
    python scripts/train_pii_detector.py \\
        --model Qwen/Qwen2-1.5B \\
        --data data/merged_pii_dataset_train.jsonl \\
        --val-data data/merged_pii_dataset_validation.jsonl \\
        --lora-r 16 \\
        --lora-alpha 32 \\
        --batch-size 8 \\
        --gradient-accumulation 4 \\
        --learning-rate 2e-4 \\
        --epochs 3 \\
        --output models/pii_detector_custom

    # å…¨å‚æ•°å¾®è°ƒï¼ˆéœ€è¦å¤§æ˜¾å­˜ï¼‰
    python scripts/train_pii_detector.py \\
        --model Qwen/Qwen2-1.5B \\
        --data data/merged_pii_dataset_train.jsonl \\
        --full-finetune \\
        --output models/pii_detector_full
"""

import argparse
import json
import sys
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

try:
    import torch
    from transformers import (
        AutoTokenizer,
        AutoModelForCausalLM,
        TrainingArguments,
        Trainer,
        DataCollatorForLanguageModeling
    )
    from peft import LoraConfig, get_peft_model, PeftModel
    from datasets import Dataset
except ImportError:
    print("âŒ ç¼ºå°‘å¿…è¦çš„åº“ã€‚è¯·å…ˆå®‰è£…ï¼š")
    print("   pip install transformers peft datasets torch accelerate")
    sys.exit(1)


@dataclass
class PIITrainingConfig:
    """è®­ç»ƒé…ç½®"""
    model_name: str
    data_path: Path
    val_data_path: Optional[Path]
    output_dir: Path

    # LoRA é…ç½®
    use_lora: bool = True
    lora_r: int = 8
    lora_alpha: int = 16
    lora_dropout: float = 0.05
    lora_target_modules: List[str] = None

    # è®­ç»ƒè¶…å‚æ•°
    batch_size: int = 4
    gradient_accumulation_steps: int = 4
    learning_rate: float = 2e-4
    num_epochs: int = 3
    warmup_steps: int = 100
    max_length: int = 512

    # ä¼˜åŒ–é…ç½®
    fp16: bool = True
    gradient_checkpointing: bool = True

    # è¯„ä¼°é…ç½®
    eval_steps: int = 500
    save_steps: int = 500
    logging_steps: int = 100

    def __post_init__(self):
        if self.lora_target_modules is None:
            # Qwen2/Qwen3 çš„é»˜è®¤ LoRA ç›®æ ‡æ¨¡å—
            self.lora_target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]


def load_training_data(data_path: Path, max_samples: Optional[int] = None) -> List[Dict[str, Any]]:
    """
    åŠ è½½è®­ç»ƒæ•°æ®

    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSONæˆ–JSONLæ ¼å¼ï¼‰
        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰

    Returns:
        æ ·æœ¬åˆ—è¡¨
    """
    print(f"\nåŠ è½½è®­ç»ƒæ•°æ®: {data_path}")

    samples = []

    if data_path.suffix == ".jsonl":
        with open(data_path, "r", encoding="utf-8") as f:
            for i, line in enumerate(f):
                if max_samples and i >= max_samples:
                    break
                sample = json.loads(line)
                samples.append(sample)
    else:
        with open(data_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            if isinstance(data, dict):
                # å¯èƒ½æ˜¯ {"train": [...], "validation": [...]} æ ¼å¼
                samples = data.get("train", data)
            else:
                samples = data

            if max_samples:
                samples = samples[:max_samples]

    print(f"  âœ“ åŠ è½½äº† {len(samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    return samples


def format_sample_for_training(sample: Dict[str, Any]) -> str:
    """
    å°†æ ·æœ¬æ ¼å¼åŒ–ä¸ºè®­ç»ƒæ–‡æœ¬

    æ ¼å¼ï¼š
    <|im_start|>system
    ä½ æ˜¯ PII æ£€æµ‹ä¸“å®¶ã€‚æ£€æµ‹æ–‡æœ¬ä¸­çš„ PII å¹¶è¾“å‡º JSONã€‚<|im_end|>
    <|im_start|>user
    æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼šæˆ‘å«å¼ ä¸‰ï¼Œç”µè¯13800138000<|im_end|>
    <|im_start|>assistant
    {"entities": [{"type": "PERSON_NAME", "value": "å¼ ä¸‰", ...}]}<|im_end|>
    """
    instruction = sample.get("instruction", "æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚")
    input_text = sample["input"]
    output_data = sample["output"]

    # ç®€åŒ–è¾“å‡ºï¼ˆåªä¿ç•™å¿…è¦ä¿¡æ¯ï¼‰
    entities = [
        {
            "type": e["type"],
            "value": e["value"],
            "start": e["start"],
            "end": e["end"]
        }
        for e in output_data["entities"]
    ]

    output_json = json.dumps({"entities": entities}, ensure_ascii=False)

    # Qwen å¯¹è¯æ ¼å¼
    formatted_text = (
        f"<|im_start|>system\n"
        f"ä½ æ˜¯ PII æ£€æµ‹ä¸“å®¶ã€‚{instruction}<|im_end|>\n"
        f"<|im_start|>user\n"
        f"{input_text}<|im_end|>\n"
        f"<|im_start|>assistant\n"
        f"{output_json}<|im_end|>"
    )

    return formatted_text


def prepare_dataset(
    samples: List[Dict[str, Any]],
    tokenizer: Any,
    max_length: int = 512
) -> Dataset:
    """
    å‡†å¤‡è®­ç»ƒæ•°æ®é›†

    Args:
        samples: åŸå§‹æ ·æœ¬åˆ—è¡¨
        tokenizer: åˆ†è¯å™¨
        max_length: æœ€å¤§åºåˆ—é•¿åº¦

    Returns:
        Hugging Face Dataset
    """
    print(f"\nå‡†å¤‡æ•°æ®é›†...")
    print(f"  æ ·æœ¬æ•°é‡: {len(samples):,}")
    print(f"  æœ€å¤§é•¿åº¦: {max_length}")

    # æ ¼å¼åŒ–æ ·æœ¬ä¸ºæ–‡æœ¬
    print(f"  æ ¼å¼åŒ–æ ·æœ¬...")
    formatted_texts = []
    for sample in samples:
        text = format_sample_for_training(sample)
        formatted_texts.append(text)

    # åˆ›å»ºåŸå§‹æ•°æ®é›†
    raw_dataset = Dataset.from_dict({"text": formatted_texts})

    # å®šä¹‰åˆ†è¯å‡½æ•°ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=max_length,
            padding=False,
            return_tensors=None
        )

    # ä½¿ç”¨ map è¿›è¡Œæµå¼æ‰¹é‡åˆ†è¯
    print(f"  æµå¼åˆ†è¯ä¸­ï¼ˆæ‰¹é‡å¤„ç†ï¼ŒèŠ‚çœå†…å­˜ï¼‰...")
    dataset = raw_dataset.map(
        tokenize_function,
        batched=True,
        batch_size=1000,  # æ¯æ¬¡å¤„ç†1000ä¸ªæ ·æœ¬
        remove_columns=["text"],  # ç§»é™¤åŸå§‹æ–‡æœ¬ï¼ŒèŠ‚çœå†…å­˜
        desc="åˆ†è¯è¿›åº¦"
    )

    print(f"  âœ“ æ•°æ®é›†å‡†å¤‡å®Œæˆ: {len(dataset):,} æ ·æœ¬")
    return dataset


def setup_model_and_tokenizer(config: PIITrainingConfig):
    """
    è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨

    Args:
        config: è®­ç»ƒé…ç½®

    Returns:
        (model, tokenizer)
    """
    print(f"\n{'='*70}")
    print("åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨")
    print(f"{'='*70}")
    print(f"æ¨¡å‹: {config.model_name}")
    print(f"è®­ç»ƒæ–¹æ³•: {'LoRA' if config.use_lora else 'å…¨å‚æ•°å¾®è°ƒ'}")

    # åŠ è½½åˆ†è¯å™¨
    print(f"\nåŠ è½½åˆ†è¯å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(
        config.model_name,
        trust_remote_code=True,
        padding_side="right"
    )

    # ç¡®ä¿æœ‰ pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print(f"  âœ“ åˆ†è¯å™¨åŠ è½½å®Œæˆ")
    print(f"    è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
    print(f"    pad_token: {tokenizer.pad_token}")

    # åŠ è½½æ¨¡å‹
    print(f"\nåŠ è½½æ¨¡å‹...")
    # æ³¨æ„: ä½¿ç”¨ accelerate å¤š GPU è®­ç»ƒæ—¶ä¸èƒ½æŒ‡å®š device_map
    # accelerate ä¼šè‡ªåŠ¨ç®¡ç†è®¾å¤‡åˆ†é…
    model = AutoModelForCausalLM.from_pretrained(
        config.model_name,
        trust_remote_code=True,
        torch_dtype=torch.float16 if config.fp16 else torch.float32
    )

    print(f"  âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")

    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    print(f"    æ€»å‚æ•°: {total_params / 1e9:.2f}B")

    # é…ç½® LoRA
    if config.use_lora:
        print(f"\né…ç½® LoRA...")
        print(f"  r: {config.lora_r}")
        print(f"  alpha: {config.lora_alpha}")
        print(f"  dropout: {config.lora_dropout}")
        print(f"  target_modules: {config.lora_target_modules}")

        lora_config = LoraConfig(
            r=config.lora_r,
            lora_alpha=config.lora_alpha,
            lora_dropout=config.lora_dropout,
            target_modules=config.lora_target_modules,
            bias="none",
            task_type="CAUSAL_LM"
        )

        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()

    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    # æ³¨æ„: LoRA + accelerate å¤šGPU æ¨¡å¼ä¸‹ gradient_checkpointing å¯èƒ½å¯¼è‡´æ¢¯åº¦é—®é¢˜
    # ç”±äº LoRA å‚æ•°é‡å¾ˆå° (5.9M)ï¼Œä¸¤ä¸ª3060åº”è¯¥è¶³å¤Ÿï¼Œæš‚æ—¶ç¦ç”¨
    if config.gradient_checkpointing and not config.use_lora:
        model.gradient_checkpointing_enable()
        print(f"\n  âœ“ æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
    elif config.gradient_checkpointing and config.use_lora:
        print(f"\n  âš  è·³è¿‡æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆLoRA+accelerateå…¼å®¹æ€§è€ƒè™‘ï¼‰")

    return model, tokenizer


def train_model(
    model: Any,
    tokenizer: Any,
    train_dataset: Dataset,
    val_dataset: Optional[Dataset],
    config: PIITrainingConfig
):
    """
    è®­ç»ƒæ¨¡å‹

    Args:
        model: æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        train_dataset: è®­ç»ƒé›†
        val_dataset: éªŒè¯é›†
        config: è®­ç»ƒé…ç½®
    """
    print(f"\n{'='*70}")
    print("å¼€å§‹è®­ç»ƒ")
    print(f"{'='*70}")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    config.output_dir.mkdir(parents=True, exist_ok=True)

    # è®­ç»ƒå‚æ•°
    # æ³¨æ„: LoRA + accelerate å¤šGPUæ¨¡å¼ä¸‹å¿…é¡»ç¦ç”¨ gradient_checkpointing
    use_gradient_checkpointing = config.gradient_checkpointing and not config.use_lora

    training_args = TrainingArguments(
        output_dir=str(config.output_dir),
        num_train_epochs=config.num_epochs,
        per_device_train_batch_size=config.batch_size,
        per_device_eval_batch_size=config.batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        learning_rate=config.learning_rate,
        warmup_steps=config.warmup_steps,
        logging_steps=config.logging_steps,
        save_steps=config.save_steps,
        eval_steps=config.eval_steps if val_dataset else None,
        eval_strategy="steps" if val_dataset else "no",
        save_total_limit=3,
        fp16=config.fp16,
        gradient_checkpointing=use_gradient_checkpointing,
        report_to="none",  # ç¦ç”¨ wandb/tensorboard
        load_best_model_at_end=True if val_dataset else False,
        metric_for_best_model="eval_loss" if val_dataset else None,
    )

    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # å› æœè¯­è¨€æ¨¡å‹ï¼Œä¸ä½¿ç”¨ MLM
    )

    # åˆ›å»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
    )

    # å¼€å§‹è®­ç»ƒ
    print(f"\nè®­ç»ƒé…ç½®:")
    print(f"  Batch size: {config.batch_size}")
    print(f"  Gradient accumulation: {config.gradient_accumulation_steps}")
    print(f"  Effective batch size: {config.batch_size * config.gradient_accumulation_steps}")
    print(f"  Learning rate: {config.learning_rate}")
    print(f"  Epochs: {config.num_epochs}")
    print(f"  FP16: {config.fp16}")
    print()

    start_time = time.time()
    trainer.train()
    elapsed = time.time() - start_time

    print(f"\nâœ“ è®­ç»ƒå®Œæˆï¼")
    print(f"  è€—æ—¶: {elapsed / 60:.1f} åˆ†é’Ÿ")

    # ä¿å­˜æ¨¡å‹
    print(f"\nä¿å­˜æ¨¡å‹...")
    trainer.save_model(str(config.output_dir / "final"))
    tokenizer.save_pretrained(str(config.output_dir / "final"))
    print(f"  âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {config.output_dir / 'final'}")

    # ä¿å­˜è®­ç»ƒé…ç½®
    config_path = config.output_dir / "training_config.json"
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump({
            "model_name": config.model_name,
            "use_lora": config.use_lora,
            "lora_r": config.lora_r,
            "lora_alpha": config.lora_alpha,
            "batch_size": config.batch_size,
            "learning_rate": config.learning_rate,
            "num_epochs": config.num_epochs,
            "max_length": config.max_length,
        }, f, indent=2)
    print(f"  âœ“ è®­ç»ƒé…ç½®å·²ä¿å­˜åˆ°: {config_path}")


def main():
    parser = argparse.ArgumentParser(
        description="è®­ç»ƒ PII æ£€æµ‹æ¨¡å‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºç¡€ LoRA è®­ç»ƒ
  python scripts/train_pii_detector.py \\
      --model Qwen/Qwen2-1.5B \\
      --data data/merged_pii_dataset_train.jsonl \\
      --output models/pii_detector

  # å¸¦éªŒè¯é›†çš„è®­ç»ƒ
  python scripts/train_pii_detector.py \\
      --model Qwen/Qwen2-1.5B \\
      --data data/merged_pii_dataset_train.jsonl \\
      --val-data data/merged_pii_dataset_validation.jsonl \\
      --output models/pii_detector

  # é«˜çº§é…ç½®
  python scripts/train_pii_detector.py \\
      --model Qwen/Qwen2-1.5B \\
      --data data/merged_pii_dataset_train.jsonl \\
      --lora-r 16 --lora-alpha 32 \\
      --batch-size 8 --learning-rate 2e-4 \\
      --epochs 3 --output models/pii_detector_custom
        """
    )

    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="åŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„ (ä¾‹å¦‚: Qwen/Qwen2-1.5B)"
    )

    parser.add_argument(
        "--data",
        type=str,
        required=True,
        help="è®­ç»ƒæ•°æ®è·¯å¾„ (.json æˆ– .jsonl)"
    )

    parser.add_argument(
        "--val-data",
        type=str,
        help="éªŒè¯æ•°æ®è·¯å¾„ (.json æˆ– .jsonl)"
    )

    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="è¾“å‡ºç›®å½•"
    )

    # LoRA é…ç½®
    parser.add_argument(
        "--full-finetune",
        action="store_true",
        help="ä½¿ç”¨å…¨å‚æ•°å¾®è°ƒï¼ˆè€Œé LoRAï¼‰"
    )

    parser.add_argument(
        "--lora-r",
        type=int,
        default=8,
        help="LoRA rank (é»˜è®¤: 8)"
    )

    parser.add_argument(
        "--lora-alpha",
        type=int,
        default=16,
        help="LoRA alpha (é»˜è®¤: 16)"
    )

    # è®­ç»ƒè¶…å‚æ•°
    parser.add_argument(
        "--batch-size",
        type=int,
        default=4,
        help="æ‰¹æ¬¡å¤§å° (é»˜è®¤: 4)"
    )

    parser.add_argument(
        "--gradient-accumulation",
        type=int,
        default=4,
        help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (é»˜è®¤: 4)"
    )

    parser.add_argument(
        "--learning-rate",
        type=float,
        default=2e-4,
        help="å­¦ä¹ ç‡ (é»˜è®¤: 2e-4)"
    )

    parser.add_argument(
        "--epochs",
        type=int,
        default=3,
        help="è®­ç»ƒè½®æ•° (é»˜è®¤: 3)"
    )

    parser.add_argument(
        "--max-length",
        type=int,
        default=512,
        help="æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 512)"
    )

    parser.add_argument(
        "--max-samples",
        type=int,
        help="æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰"
    )

    args = parser.parse_args()

    # åˆ›å»ºè®­ç»ƒé…ç½®
    config = PIITrainingConfig(
        model_name=args.model,
        data_path=Path(args.data),
        val_data_path=Path(args.val_data) if args.val_data else None,
        output_dir=Path(args.output),
        use_lora=not args.full_finetune,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        learning_rate=args.learning_rate,
        num_epochs=args.epochs,
        max_length=args.max_length,
    )

    # åŠ è½½æ•°æ®
    train_samples = load_training_data(config.data_path, args.max_samples)

    val_samples = None
    if config.val_data_path:
        val_samples = load_training_data(config.val_data_path, args.max_samples)

    # è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = setup_model_and_tokenizer(config)

    # å‡†å¤‡æ•°æ®é›†
    train_dataset = prepare_dataset(train_samples, tokenizer, config.max_length)
    val_dataset = prepare_dataset(val_samples, tokenizer, config.max_length) if val_samples else None

    # è®­ç»ƒæ¨¡å‹
    train_model(model, tokenizer, train_dataset, val_dataset, config)

    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"\næ¨¡å‹ä½ç½®: {config.output_dir / 'final'}")
    print(f"\nåç»­æ­¥éª¤:")
    print(f"  1. æµ‹è¯•æ¨¡å‹: python examples/test_trained_model.py --model {config.output_dir / 'final'}")
    print(f"  2. é›†æˆåˆ°ç³»ç»Ÿ: ä¿®æ”¹é…ç½®æ–‡ä»¶æŒ‡å‘æ–°æ¨¡å‹")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\n\nâŒ å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
