#!/usr/bin/env python3
"""
ä¸‹è½½ PII æ£€æµ‹è®­ç»ƒæ•°æ®é›†

æ”¯æŒçš„æ•°æ®é›†ï¼š
1. ai4privacy/pii-masking-200k - 200k PIIè„±æ•æ•°æ®é›†
2. levow/msra_ner - MSRAä¸­æ–‡NERæ•°æ®é›†
3. bigcode/bigcode-pii-dataset - ä»£ç ä¸­çš„PIIæ•°æ®é›†

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # ä¸‹è½½æ‰€æœ‰æ¨èæ•°æ®é›†
    python scripts/download_datasets.py --all

    # ä¸‹è½½ç‰¹å®šæ•°æ®é›†
    python scripts/download_datasets.py --datasets ai4privacy msra

    # æŒ‡å®šè¾“å‡ºç›®å½•
    python scripts/download_datasets.py --all --output data/pii_datasets
"""

import argparse
import sys
from pathlib import Path
from typing import List, Optional

try:
    from datasets import load_dataset
    from huggingface_hub import login
except ImportError:
    print("âŒ ç¼ºå°‘ä¾èµ–åŒ…ã€‚è¯·å…ˆå®‰è£…ï¼š")
    print("   pip install datasets huggingface_hub")
    sys.exit(1)


DATASET_CONFIGS = {
    "ai4privacy": {
        "hf_path": "ai4privacy/pii-masking-200k",
        "description": "PIIè„±æ•æ•°æ®é›† (200kæ ·æœ¬)",
        "size_mb": 450,
        "languages": ["en", "zh"],
        "pii_types": ["NAME", "EMAIL", "PHONE", "ADDRESS", "ID_CARD"]
    },
    "msra": {
        "hf_path": "levow/msra_ner",
        "description": "MSRAä¸­æ–‡NERæ•°æ®é›† (50k+æ ·æœ¬)",
        "size_mb": 25,
        "languages": ["zh"],
        "pii_types": ["PERSON_NAME", "LOCATION", "ORGANIZATION"]
    },
    "bigcode": {
        "hf_path": "bigcode/bigcode-pii-dataset",
        "description": "BigCodeä»£ç PIIæ•°æ®é›† (12kæ ·æœ¬)",
        "size_mb": 120,
        "languages": ["code"],
        "pii_types": ["EMAIL", "USERNAME", "IP_ADDRESS", "KEY", "PASSWORD"]
    }
}


def download_dataset(
    dataset_name: str,
    output_dir: Path,
    show_progress: bool = True
) -> bool:
    """
    ä¸‹è½½å•ä¸ªæ•°æ®é›†

    Args:
        dataset_name: æ•°æ®é›†åç§° (ai4privacy, msra, bigcode)
        output_dir: è¾“å‡ºç›®å½•
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦

    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    if dataset_name not in DATASET_CONFIGS:
        print(f"âŒ æœªçŸ¥æ•°æ®é›†: {dataset_name}")
        print(f"   æ”¯æŒçš„æ•°æ®é›†: {', '.join(DATASET_CONFIGS.keys())}")
        return False

    config = DATASET_CONFIGS[dataset_name]
    hf_path = config["hf_path"]

    print(f"\n{'='*70}")
    print(f"ä¸‹è½½æ•°æ®é›†: {dataset_name}")
    print(f"{'='*70}")
    print(f"æè¿°: {config['description']}")
    print(f"å¤§å°: ~{config['size_mb']} MB")
    print(f"è¯­è¨€: {', '.join(config['languages'])}")
    print(f"PIIç±»å‹: {', '.join(config['pii_types'])}")
    print(f"Hugging Faceè·¯å¾„: {hf_path}")
    print()

    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        dataset_output_dir = output_dir / dataset_name
        dataset_output_dir.mkdir(parents=True, exist_ok=True)

        # ä¸‹è½½æ•°æ®é›†
        print(f"æ­£åœ¨ä¸‹è½½ {hf_path}...")
        dataset = load_dataset(hf_path)

        # ä¿å­˜åˆ°æœ¬åœ°
        print(f"ä¿å­˜åˆ°: {dataset_output_dir}")
        dataset.save_to_disk(str(dataset_output_dir))

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nâœ“ ä¸‹è½½å®Œæˆï¼")
        print(f"\næ•°æ®é›†ç»Ÿè®¡:")
        for split_name, split_data in dataset.items():
            print(f"  {split_name}: {len(split_data)} æ ·æœ¬")

        # æ˜¾ç¤ºç¤ºä¾‹
        if "train" in dataset:
            example = dataset["train"][0]
            print(f"\nç¤ºä¾‹æ•°æ®:")
            for key, value in list(example.items())[:3]:
                value_str = str(value)[:100]
                if len(str(value)) > 100:
                    value_str += "..."
                print(f"  {key}: {value_str}")

        return True

    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def download_multiple_datasets(
    dataset_names: List[str],
    output_dir: Path,
    show_progress: bool = True
) -> dict:
    """
    ä¸‹è½½å¤šä¸ªæ•°æ®é›†

    Returns:
        ä¸‹è½½ç»“æœç»Ÿè®¡ {dataset_name: success}
    """
    results = {}

    print(f"\n{'='*70}")
    print(f"æ‰¹é‡ä¸‹è½½æ•°æ®é›†")
    print(f"{'='*70}")
    print(f"ç›®æ ‡æ•°æ®é›†: {', '.join(dataset_names)}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print()

    for i, dataset_name in enumerate(dataset_names, 1):
        print(f"\n[{i}/{len(dataset_names)}] å¤„ç†: {dataset_name}")
        success = download_dataset(dataset_name, output_dir, show_progress)
        results[dataset_name] = success

    # æ€»ç»“
    print(f"\n{'='*70}")
    print("ä¸‹è½½æ€»ç»“")
    print(f"{'='*70}")

    for dataset_name, success in results.items():
        status = "âœ“ æˆåŠŸ" if success else "âœ— å¤±è´¥"
        print(f"{dataset_name}: {status}")

    successful = sum(1 for s in results.values() if s)
    total = len(results)
    print(f"\næˆåŠŸ: {successful}/{total}")

    if successful == total:
        print("\nğŸ‰ æ‰€æœ‰æ•°æ®é›†ä¸‹è½½å®Œæˆï¼")
    else:
        print(f"\nâš ï¸  {total - successful} ä¸ªæ•°æ®é›†ä¸‹è½½å¤±è´¥")

    return results


def list_available_datasets():
    """åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›†"""
    print(f"\n{'='*70}")
    print("å¯ç”¨çš„ PII æ•°æ®é›†")
    print(f"{'='*70}\n")

    for name, config in DATASET_CONFIGS.items():
        print(f"ğŸ“¦ {name}")
        print(f"   æè¿°: {config['description']}")
        print(f"   è·¯å¾„: {config['hf_path']}")
        print(f"   å¤§å°: ~{config['size_mb']} MB")
        print(f"   è¯­è¨€: {', '.join(config['languages'])}")
        print(f"   PIIç±»å‹: {', '.join(config['pii_types'])}")
        print()


def main():
    parser = argparse.ArgumentParser(
        description="ä¸‹è½½ PII æ£€æµ‹è®­ç»ƒæ•°æ®é›†",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä¸‹è½½æ‰€æœ‰æ¨èæ•°æ®é›†
  python scripts/download_datasets.py --all

  # ä¸‹è½½ç‰¹å®šæ•°æ®é›†
  python scripts/download_datasets.py --datasets ai4privacy msra

  # åˆ—å‡ºå¯ç”¨æ•°æ®é›†
  python scripts/download_datasets.py --list

  # æŒ‡å®šè¾“å‡ºç›®å½•
  python scripts/download_datasets.py --all --output data/pii_datasets
        """
    )

    parser.add_argument(
        "--datasets",
        nargs="+",
        choices=list(DATASET_CONFIGS.keys()),
        help="è¦ä¸‹è½½çš„æ•°æ®é›†åç§°"
    )

    parser.add_argument(
        "--all",
        action="store_true",
        help="ä¸‹è½½æ‰€æœ‰æ¨èæ•°æ®é›† (ai4privacy + msra)"
    )

    parser.add_argument(
        "--list",
        action="store_true",
        help="åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›†"
    )

    parser.add_argument(
        "--output",
        type=str,
        default="data/pii_datasets",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: data/pii_datasets)"
    )

    parser.add_argument(
        "--hf-token",
        type=str,
        help="Hugging Face token (æŸäº›æ•°æ®é›†éœ€è¦)"
    )

    args = parser.parse_args()

    # åˆ—å‡ºæ•°æ®é›†
    if args.list:
        list_available_datasets()
        return

    # ç¡®å®šè¦ä¸‹è½½çš„æ•°æ®é›†
    if args.all:
        # æ¨èçš„æ•°æ®é›†ç»„åˆ
        dataset_names = ["ai4privacy", "msra"]
    elif args.datasets:
        dataset_names = args.datasets
    else:
        parser.print_help()
        return

    # ç™»å½• Hugging Faceï¼ˆå¦‚æœæä¾›äº†tokenï¼‰
    if args.hf_token:
        print("ç™»å½• Hugging Face...")
        login(token=args.hf_token)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ä¸‹è½½æ•°æ®é›†
    results = download_multiple_datasets(dataset_names, output_dir)

    # é€€å‡ºç 
    all_successful = all(results.values())
    sys.exit(0 if all_successful else 1)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\n\nâŒ å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
