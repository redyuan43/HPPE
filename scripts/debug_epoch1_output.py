#!/usr/bin/env python3
"""
è°ƒè¯•Epoch 1æ¨¡å‹è¾“å‡º - æŸ¥çœ‹å®é™…ç”Ÿæˆå†…å®¹
"""

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig

MODEL_PATH = "models/pii_qwen4b_unsloth/checkpoint-781"
TEST_DATA = "data/merged_pii_dataset_test.jsonl"

print("ğŸ” åŠ è½½æ¨¡å‹...")
peft_config = PeftConfig.from_pretrained(MODEL_PATH)
base_model_path = peft_config.base_model_name_or_path

model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.float16,
    device_map="auto",
)
model = PeftModel.from_pretrained(model, MODEL_PATH)
tokenizer = AutoTokenizer.from_pretrained(base_model_path)

print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")

# æµ‹è¯•3ä¸ªæ ·æœ¬
print("ğŸ“Š æµ‹è¯•3ä¸ªæ ·æœ¬çš„å®é™…è¾“å‡º:\n")

with open(TEST_DATA, 'r') as f:
    for i, line in enumerate(f):
        if i >= 3:
            break

        sample = json.loads(line)
        input_text = sample["input"]
        expected_entities = sample.get("output", {}).get("entities", [])

        print(f"{'='*70}")
        print(f"æ ·æœ¬ {i+1}:")
        print(f"è¾“å…¥: {input_text[:100]}...")
        print(f"æœŸæœ›å®ä½“æ•°: {len(expected_entities)}")

        # æ„é€ prompt
        prompt = (
            f"<|im_start|>system\n"
            f"ä½ æ˜¯ PII æ£€æµ‹ä¸“å®¶ã€‚æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚<|im_end|>\n"
            f"<|im_start|>user\n{input_text}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )

        # æ¨ç†
        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id,
            )

        # è§£ç å®Œæ•´è¾“å‡º
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)

        # æå–assistantéƒ¨åˆ†
        assistant_start = full_response.rfind("<|im_start|>assistant\n")
        if assistant_start != -1:
            assistant_response = full_response[assistant_start + len("<|im_start|>assistant\n"):]
            print(f"\næ¨¡å‹åŸå§‹è¾“å‡º:")
            print(f"{assistant_response[:500]}")
        else:
            print(f"\nâš ï¸ æœªæ‰¾åˆ°assistantè¾“å‡ºæ ‡è®°")
            print(f"å®Œæ•´è¾“å‡º: {full_response[:500]}")

        print(f"\næœŸæœ›è¾“å‡ºæ ·ä¾‹:")
        if expected_entities:
            print(f'{{"entities": [{expected_entities[0]}]}}')
        print()

print(f"{'='*70}")
print("è°ƒè¯•å®Œæˆ")
