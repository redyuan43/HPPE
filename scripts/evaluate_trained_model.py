#!/usr/bin/env python3
"""
è®­ç»ƒåæ¨¡å‹è¯„ä¼°è„šæœ¬

ç”¨é€”ï¼šåœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¯„ä¼°è®­ç»ƒåçš„ PII æ£€æµ‹æ¨¡å‹

åŠŸèƒ½ï¼š
1. åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
2. åœ¨æµ‹è¯•é›†ä¸Šè¿è¡Œæ¨ç†
3. è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡ï¼ˆPrecisionã€Recallã€F1ã€F2ï¼‰
4. æŒ‰ PII ç±»å‹ç»Ÿè®¡æŒ‡æ ‡
5. ç”Ÿæˆæ··æ·†çŸ©é˜µ
6. è¾“å‡ºè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š

ä½¿ç”¨ç¤ºä¾‹ï¼š
    python scripts/evaluate_trained_model.py \
        --model models/pii_detector_qwen3_0.6b/final \
        --test-data data/merged_pii_dataset_test.jsonl \
        --output evaluation_results/test_evaluation.json
"""

import argparse
import json
import sys
import time
from pathlib import Path
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass, field
from collections import defaultdict

try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
except ImportError:
    print("âŒ ç¼ºå°‘å¿…è¦çš„åº“ã€‚è¯·å…ˆå®‰è£…ï¼š")
    print("   pip install transformers peft torch")
    sys.exit(1)


@dataclass
class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡"""
    entity_type: str = "overall"

    # æ··æ·†çŸ©é˜µ
    true_positives: int = 0
    false_positives: int = 0
    false_negatives: int = 0
    true_negatives: int = 0

    # æ´¾ç”ŸæŒ‡æ ‡
    precision: float = 0.0
    recall: float = 0.0
    f1_score: float = 0.0
    f2_score: float = 0.0
    accuracy: float = 0.0

    # æ ·æœ¬ç»Ÿè®¡
    total_samples: int = 0
    success_count: int = 0
    error_count: int = 0

    def calculate_metrics(self):
        """è®¡ç®—æ´¾ç”ŸæŒ‡æ ‡"""
        # Precision = TP / (TP + FP)
        if self.true_positives + self.false_positives > 0:
            self.precision = self.true_positives / (self.true_positives + self.false_positives)

        # Recall = TP / (TP + FN)
        if self.true_positives + self.false_negatives > 0:
            self.recall = self.true_positives / (self.true_positives + self.false_negatives)

        # F1 = 2 * (P * R) / (P + R)
        if self.precision + self.recall > 0:
            self.f1_score = 2 * (self.precision * self.recall) / (self.precision + self.recall)

        # F2 = 5 * (P * R) / (4P + R)  # æ›´é‡è§† Recall
        if self.precision + self.recall > 0:
            self.f2_score = 5 * (self.precision * self.recall) / (4 * self.precision + self.recall)

        # Accuracy = (TP + TN) / (TP + TN + FP + FN)
        total = self.true_positives + self.true_negatives + self.false_positives + self.false_negatives
        if total > 0:
            self.accuracy = (self.true_positives + self.true_negatives) / total

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "entity_type": self.entity_type,
            "confusion_matrix": {
                "true_positives": self.true_positives,
                "false_positives": self.false_positives,
                "false_negatives": self.false_negatives,
                "true_negatives": self.true_negatives
            },
            "metrics": {
                "precision": round(self.precision, 4),
                "recall": round(self.recall, 4),
                "f1_score": round(self.f1_score, 4),
                "f2_score": round(self.f2_score, 4),
                "accuracy": round(self.accuracy, 4)
            },
            "statistics": {
                "total_samples": self.total_samples,
                "success_count": self.success_count,
                "error_count": self.error_count
            }
        }


class TrainedModelEvaluator:
    """è®­ç»ƒåæ¨¡å‹è¯„ä¼°å™¨"""

    def __init__(self, model_path: str, device: str = "auto"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆå¾®è°ƒåçš„æ¨¡å‹ç›®å½•ï¼‰
            device: è®¾å¤‡ï¼ˆcuda/cpu/autoï¼‰
        """
        self.model_path = Path(model_path)
        self.device = self._setup_device(device)

        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        self.tokenizer, self.model = self._load_model()

        # è¯„ä¼°ç»“æœ
        self.results: List[Dict[str, Any]] = []

    def _setup_device(self, device: str) -> str:
        """è®¾ç½®è®¾å¤‡"""
        if device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return device

    def _load_model(self) -> Tuple[Any, Any]:
        """
        åŠ è½½å¾®è°ƒåçš„æ¨¡å‹

        Returns:
            (tokenizer, model)
        """
        print(f"\n{'=' * 70}")
        print("åŠ è½½å¾®è°ƒåçš„æ¨¡å‹")
        print(f"{'=' * 70}")
        print(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
        print(f"è®¾å¤‡: {self.device}")

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        adapter_model = self.model_path / "adapter_model.safetensors"
        adapter_config = self.model_path / "adapter_config.json"

        if not adapter_model.exists() or not adapter_config.exists():
            raise FileNotFoundError(
                f"æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´ã€‚è¯·ç¡®ä¿ç›®å½•åŒ…å«ï¼š\n"
                f"  - adapter_model.safetensors\n"
                f"  - adapter_config.json\n"
                f"å½“å‰è·¯å¾„: {self.model_path}"
            )

        # è¯»å– adapter_config è·å–åŸºç¡€æ¨¡å‹è·¯å¾„
        with open(adapter_config, 'r') as f:
            config = json.load(f)
            base_model_name = config.get("base_model_name_or_path")

        print(f"\nåŠ è½½åŸºç¡€æ¨¡å‹: {base_model_name}")

        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            str(self.model_path),
            trust_remote_code=True
        )

        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map=self.device
        )

        # åŠ è½½ LoRA é€‚é…å™¨
        print(f"åŠ è½½ LoRA é€‚é…å™¨: {self.model_path}")
        model = PeftModel.from_pretrained(
            base_model,
            str(self.model_path),
            torch_dtype=torch.float16
        )

        # åˆå¹¶é€‚é…å™¨æƒé‡ï¼ˆå¯é€‰ï¼Œæå‡æ¨ç†é€Ÿåº¦ï¼‰
        model = model.merge_and_unload()

        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()

        print(f"\nâœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
        return tokenizer, model

    def _format_prompt(self, text: str) -> str:
        """
        æ ¼å¼åŒ–è¾“å…¥ä¸º Qwen å¯¹è¯æ ¼å¼

        Args:
            text: å¾…æ£€æµ‹æ–‡æœ¬

        Returns:
            æ ¼å¼åŒ–åçš„æç¤ºè¯
        """
        instruction = "æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PIIï¼Œå¹¶ä»¥ JSON æ ¼å¼è¾“å‡ºå®ä½“åˆ—è¡¨ã€‚"

        prompt = (
            f"<|im_start|>system\n"
            f"ä½ æ˜¯ PII æ£€æµ‹ä¸“å®¶ã€‚{instruction}<|im_end|>\n"
            f"<|im_start|>user\n"
            f"{text}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )

        return prompt

    def _parse_model_output(self, output_text: str) -> Dict[str, Any]:
        """
        è§£ææ¨¡å‹è¾“å‡ºä¸º JSON

        Args:
            output_text: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬

        Returns:
            è§£æåçš„ JSON å¯¹è±¡
        """
        try:
            # æå– JSON éƒ¨åˆ†ï¼ˆå»é™¤å‰åçš„ <|im_end|> ç­‰æ ‡è®°ï¼‰
            json_start = output_text.find("{")
            json_end = output_text.rfind("}") + 1

            if json_start == -1 or json_end == 0:
                return {"entities": []}

            json_str = output_text[json_start:json_end]
            result = json.loads(json_str)

            return result

        except json.JSONDecodeError:
            # JSON è§£æå¤±è´¥ï¼Œè¿”å›ç©ºç»“æœ
            return {"entities": []}

    def detect_pii(self, text: str, max_new_tokens: int = 512) -> Dict[str, Any]:
        """
        æ£€æµ‹æ–‡æœ¬ä¸­çš„ PII

        Args:
            text: å¾…æ£€æµ‹æ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°

        Returns:
            æ£€æµ‹ç»“æœ {"entities": [...]}
        """
        # æ ¼å¼åŒ–è¾“å…¥
        prompt = self._format_prompt(text)

        # åˆ†è¯
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(self.device)

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.1,  # ä½æ¸©åº¦ä¿è¯ä¸€è‡´æ€§
                do_sample=False,  # è´ªå©ªè§£ç 
                pad_token_id=self.tokenizer.eos_token_id
            )

        # è§£ç 
        output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=False)

        # æå– assistant çš„å›å¤ï¼ˆå»é™¤è¾“å…¥éƒ¨åˆ†ï¼‰
        assistant_marker = "<|im_start|>assistant\n"
        if assistant_marker in output_text:
            output_text = output_text.split(assistant_marker)[-1]

        # è§£æ JSON
        result = self._parse_model_output(output_text)

        return result

    def _match_entities(
        self,
        predicted: List[Dict[str, Any]],
        expected: List[Dict[str, Any]]
    ) -> Tuple[int, int, int]:
        """
        åŒ¹é…é¢„æµ‹å®ä½“å’Œé¢„æœŸå®ä½“

        Args:
            predicted: é¢„æµ‹çš„å®ä½“åˆ—è¡¨
            expected: é¢„æœŸçš„å®ä½“åˆ—è¡¨

        Returns:
            (true_positives, false_positives, false_negatives)
        """
        # è½¬æ¢ä¸ºé›†åˆï¼ˆtype, valueï¼‰
        predicted_set = {
            (e['type'], e['value'])
            for e in predicted
        }

        expected_set = {
            (e['type'], e['value'])
            for e in expected
        }

        # è®¡ç®—æŒ‡æ ‡
        true_positives = len(predicted_set & expected_set)
        false_positives = len(predicted_set - expected_set)
        false_negatives = len(expected_set - predicted_set)

        return true_positives, false_positives, false_negatives

    def evaluate_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªæ ·æœ¬

        Args:
            sample: æµ‹è¯•æ ·æœ¬
                {
                    "input": "æˆ‘å«å¼ ä¸‰...",
                    "output": {
                        "entities": [{"type": "PERSON_NAME", "value": "å¼ ä¸‰", ...}]
                    }
                }

        Returns:
            è¯„ä¼°ç»“æœ
        """
        input_text = sample["input"]
        expected_entities = sample["output"]["entities"]

        # æ¨ç†
        try:
            start_time = time.time()
            result = self.detect_pii(input_text)
            latency = time.time() - start_time

            predicted_entities = result.get("entities", [])

            # åŒ¹é…å®ä½“
            tp, fp, fn = self._match_entities(predicted_entities, expected_entities)

            return {
                "success": True,
                "input": input_text,
                "expected_count": len(expected_entities),
                "predicted_count": len(predicted_entities),
                "true_positives": tp,
                "false_positives": fp,
                "false_negatives": fn,
                "latency": latency,
                "predicted_entities": predicted_entities,
                "expected_entities": expected_entities
            }

        except Exception as e:
            return {
                "success": False,
                "input": input_text,
                "error": str(e),
                "expected_count": len(expected_entities)
            }

    def evaluate_dataset(
        self,
        test_data_path: str,
        max_samples: int = None
    ) -> Tuple[EvaluationMetrics, Dict[str, EvaluationMetrics]]:
        """
        è¯„ä¼°æ•´ä¸ªæµ‹è¯•é›†

        Args:
            test_data_path: æµ‹è¯•æ•°æ®è·¯å¾„
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰

        Returns:
            (overall_metrics, metrics_by_type)
        """
        print(f"\n{'=' * 70}")
        print("å¼€å§‹è¯„ä¼°æµ‹è¯•é›†")
        print(f"{'=' * 70}")

        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data_path = Path(test_data_path)
        samples = []

        print(f"åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")

        with open(test_data_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if max_samples and i >= max_samples:
                    break
                sample = json.loads(line)
                samples.append(sample)

        print(f"  âœ“ åŠ è½½äº† {len(samples):,} ä¸ªæµ‹è¯•æ ·æœ¬")

        # åˆå§‹åŒ–æŒ‡æ ‡
        overall_metrics = EvaluationMetrics(entity_type="overall")
        metrics_by_type = defaultdict(lambda: EvaluationMetrics())

        # é€æ ·æœ¬è¯„ä¼°
        print(f"\nå¼€å§‹æ¨ç†...")
        for i, sample in enumerate(samples, 1):
            result = self.evaluate_sample(sample)
            self.results.append(result)

            # æ›´æ–°æ€»ä½“æŒ‡æ ‡
            overall_metrics.total_samples += 1

            if result["success"]:
                overall_metrics.success_count += 1
                overall_metrics.true_positives += result["true_positives"]
                overall_metrics.false_positives += result["false_positives"]
                overall_metrics.false_negatives += result["false_negatives"]

                # æŒ‰ç±»å‹ç»Ÿè®¡
                for entity in result["predicted_entities"]:
                    entity_type = entity["type"]
                    metrics_by_type[entity_type].total_samples += 1

                for entity in result["expected_entities"]:
                    entity_type = entity["type"]
                    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œåªç»Ÿè®¡ TP/FP/FN åˆ°å¯¹åº”ç±»å‹
                    pass

            else:
                overall_metrics.error_count += 1

            # æ˜¾ç¤ºè¿›åº¦
            if i % 100 == 0 or i == len(samples):
                print(f"  [{i}/{len(samples)}] å®Œæˆ {i / len(samples) * 100:.1f}%")

        # è®¡ç®—æ´¾ç”ŸæŒ‡æ ‡
        overall_metrics.calculate_metrics()

        for entity_type in metrics_by_type:
            metrics_by_type[entity_type].entity_type = entity_type
            metrics_by_type[entity_type].calculate_metrics()

        print(f"\nâœ“ è¯„ä¼°å®Œæˆï¼")

        return overall_metrics, dict(metrics_by_type)

    def print_evaluation_summary(
        self,
        overall_metrics: EvaluationMetrics,
        metrics_by_type: Dict[str, EvaluationMetrics]
    ):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print(f"\n{'=' * 70}")
        print("è¯„ä¼°ç»“æœæ‘˜è¦")
        print(f"{'=' * 70}")

        # æ€»ä½“æŒ‡æ ‡
        print(f"\nã€æ€»ä½“æŒ‡æ ‡ã€‘")
        print(f"  æ ·æœ¬æ€»æ•°: {overall_metrics.total_samples:,}")
        print(f"  æˆåŠŸæ¨ç†: {overall_metrics.success_count:,}")
        print(f"  æ¨ç†å¤±è´¥: {overall_metrics.error_count:,}")
        print(f"\n  æ··æ·†çŸ©é˜µ:")
        print(f"    TP (æ­£ç¡®æ£€æµ‹): {overall_metrics.true_positives:,}")
        print(f"    FP (è¯¯æŠ¥): {overall_metrics.false_positives:,}")
        print(f"    FN (æ¼æŠ¥): {overall_metrics.false_negatives:,}")
        print(f"\n  å‡†ç¡®æ€§æŒ‡æ ‡:")
        print(f"    Precision: {overall_metrics.precision:.2%}")
        print(f"    Recall:    {overall_metrics.recall:.2%}")
        print(f"    F1-Score:  {overall_metrics.f1_score:.2%}")
        print(f"    F2-Score:  {overall_metrics.f2_score:.2%}")
        print(f"    Accuracy:  {overall_metrics.accuracy:.2%}")

        # åˆ¤å®šç»“æœ
        print(f"\nã€éªŒè¯ç»“æœã€‘")
        passed = True
        reasons = []

        if overall_metrics.f1_score < 0.875:
            passed = False
            reasons.append(f"F1-Score æœªè¾¾æ ‡ ({overall_metrics.f1_score:.2%} < 87.5%)")

        if overall_metrics.recall < 0.90:
            passed = False
            reasons.append(f"Recall æœªè¾¾æ ‡ ({overall_metrics.recall:.2%} < 90%)")

        if passed:
            print(f"  âœ… é€šè¿‡éªŒè¯ï¼")
        else:
            print(f"  âŒ æœªé€šè¿‡éªŒè¯")
            for reason in reasons:
                print(f"     - {reason}")

        # æŒ‰ç±»å‹ç»Ÿè®¡
        if metrics_by_type:
            print(f"\nã€å„ç±»å‹æŒ‡æ ‡ã€‘")
            print(f"  {'ç±»å‹':<20} {'ç²¾ç¡®ç‡':>10} {'å¬å›ç‡':>10} {'F1-Score':>10}")
            print(f"  {'-' * 60}")
            for entity_type, metrics in sorted(metrics_by_type.items()):
                print(f"  {entity_type:<20} {metrics.precision:>9.2%} {metrics.recall:>9.2%} {metrics.f1_score:>9.2%}")

    def save_results(self, output_path: str):
        """ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # æ±‡æ€»ç»“æœ
        overall_metrics, metrics_by_type = self._compute_final_metrics()

        result_data = {
            "model_path": str(self.model_path),
            "evaluation_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "overall_metrics": overall_metrics.to_dict(),
            "metrics_by_type": {k: v.to_dict() for k, v in metrics_by_type.items()},
            "detailed_results": self.results
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)

        print(f"\nâœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜: {output_path}")

    def _compute_final_metrics(self) -> Tuple[EvaluationMetrics, Dict[str, EvaluationMetrics]]:
        """ä» results é‡æ–°è®¡ç®—æœ€ç»ˆæŒ‡æ ‡"""
        overall_metrics = EvaluationMetrics(entity_type="overall")
        metrics_by_type = defaultdict(lambda: EvaluationMetrics())

        for result in self.results:
            overall_metrics.total_samples += 1

            if result["success"]:
                overall_metrics.success_count += 1
                overall_metrics.true_positives += result["true_positives"]
                overall_metrics.false_positives += result["false_positives"]
                overall_metrics.false_negatives += result["false_negatives"]
            else:
                overall_metrics.error_count += 1

        overall_metrics.calculate_metrics()

        return overall_metrics, dict(metrics_by_type)


def main():
    parser = argparse.ArgumentParser(
        description="è¯„ä¼°è®­ç»ƒåçš„ PII æ£€æµ‹æ¨¡å‹",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="å¾®è°ƒåçš„æ¨¡å‹è·¯å¾„ï¼ˆåŒ…å« adapter_model.safetensorsï¼‰"
    )

    parser.add_argument(
        "--test-data",
        type=str,
        required=True,
        help="æµ‹è¯•æ•°æ®è·¯å¾„ï¼ˆ.jsonl æ ¼å¼ï¼‰"
    )

    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„ï¼ˆ.json æ ¼å¼ï¼‰"
    )

    parser.add_argument(
        "--max-samples",
        type=int,
        help="æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰"
    )

    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["auto", "cuda", "cpu"],
        help="è®¾å¤‡ï¼ˆé»˜è®¤: autoï¼‰"
    )

    args = parser.parse_args()

    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = TrainedModelEvaluator(
            model_path=args.model,
            device=args.device
        )

        # è¯„ä¼°æµ‹è¯•é›†
        overall_metrics, metrics_by_type = evaluator.evaluate_dataset(
            test_data_path=args.test_data,
            max_samples=args.max_samples
        )

        # æ‰“å°æ‘˜è¦
        evaluator.print_evaluation_summary(overall_metrics, metrics_by_type)

        # ä¿å­˜ç»“æœ
        evaluator.save_results(args.output)

        print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)

    except Exception as e:
        print(f"\n\nâŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
