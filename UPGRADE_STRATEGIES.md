# PII检测模型升级策略全景图

**当前状态**: Precision 97.78%, Recall 81.48%, F1 88.89%
**目标**: Recall ≥ 90% (差距8.52%)

---

## 🎯 策略分类

### 一、数据层面优化 (最有效) ⭐⭐⭐⭐⭐

#### 1.1 数据增强策略
**难度**: ⭐⭐
**预期提升**: Recall +5-10%
**时间成本**: 2-4小时

**具体方案**:
- **过采样低Recall类别**: 分析发现哪些PII类型漏检严重，3倍过采样
- **困难样本挖掘**: 从漏检案例中提取模式，生成相似样本
- **数据清洗**: 修正标注错误（可能存在假阴性）
- **负样本增强**: 增加不含PII但容易混淆的文本

**实施步骤**:
```bash
1. 运行漏检分析 (已启动)
2. 识别Recall<70%的PII类型
3. 针对性生成/过采样训练数据
4. 重新训练1-2 epoch
```

**优势**:
- 针对性强，直击痛点
- 成本低，效果显著
- 不需要更换模型

---

#### 1.2 主动学习 (Active Learning)
**难度**: ⭐⭐⭐
**预期提升**: Recall +3-8%
**时间成本**: 4-8小时

**流程**:
1. 用当前模型预测大量未标注数据
2. 选出模型不确定的样本（预测概率0.4-0.6）
3. 人工标注这些困难样本
4. 加入训练集重新训练

**工具**: 使用模型输出的logits作为不确定性度量

---

#### 1.3 数据合成 (LLM生成)
**难度**: ⭐⭐⭐
**预期提升**: Recall +4-7%
**时间成本**: 3-6小时

**方案**:
- 使用更大的LLM（如Qwen2.5-32B）生成包含特定PII类型的文本
- 自动标注并人工校验
- 针对漏检严重的类型生成500-1000条

---

### 二、模型层面优化 ⭐⭐⭐⭐

#### 2.1 继续训练当前模型
**难度**: ⭐
**预期提升**: Recall +2-5%
**时间成本**: 18小时/epoch

**方案A - 标准训练**:
- 再训练2-3 epoch
- 风险：过拟合，收益递减

**方案B - 低学习率微调**:
```python
learning_rate = 5e-5  # 降低10倍
num_epochs = 3-5
weight_decay = 0.01  # 防止过拟合
```
- 更温和的优化，减少过拟合风险

**方案C - 焦点损失 (Focal Loss)**:
- 针对困难样本（漏检类别）加大权重
```python
from torch.nn import CrossEntropyLoss
# 改用focal loss，gamma=2
```

---

#### 2.2 模型架构升级
**难度**: ⭐⭐⭐
**预期提升**: Recall +5-12%
**时间成本**: 24-48小时

**选项**:

| 模型 | 参数量 | 预期效果 | 显存需求 | 训练时间 |
|------|--------|----------|----------|----------|
| Qwen3-7B | 7B | Recall +8-12% | 16GB (4bit) | 30h/epoch |
| Qwen2.5-14B | 14B | Recall +10-15% | 24GB (需2卡) | 60h/epoch |
| Qwen3-14B | 14B | Recall +12-18% | 24GB (需2卡) | 65h/epoch |

**当前硬件**: 2×RTX 3060 (12GB)
- ✅ 可以训练Qwen3-7B (4bit量化)
- ⚠️ Qwen-14B需要双卡并行（可行但复杂）

---

#### 2.3 模型集成 (Ensemble)
**难度**: ⭐⭐
**预期提升**: Recall +3-6%
**时间成本**: 2小时部署

**方案**:
- 结合多个checkpoint (Epoch 1 + Epoch 2 + 继续训练的Epoch 3)
- 投票机制：任一模型检出即视为PII
- 优势：提升Recall，几乎不影响Precision

```python
predictions = []
for model in [epoch1_model, epoch2_model, epoch3_model]:
    predictions.append(model.predict(text))
final = union(predictions)  # 并集提升Recall
```

---

### 三、训练技术优化 ⭐⭐⭐

#### 3.1 LoRA参数调优
**难度**: ⭐⭐
**预期提升**: Recall +2-4%
**时间成本**: 18小时

**当前配置**:
```python
LORA_R = 16
LORA_ALPHA = 32
```

**优化尝试**:
```python
# 方案A: 增大LoRA秩
LORA_R = 32  # 从16增大
LORA_ALPHA = 64

# 方案B: 添加dropout防过拟合
LORA_DROPOUT = 0.05  # 当前是0

# 方案C: 扩展目标层
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                  "gate_proj", "up_proj", "down_proj"]  # 增加MLP层
```

---

#### 3.2 损失函数优化
**难度**: ⭐⭐⭐
**预期提升**: Recall +3-7%
**时间成本**: 6小时实现 + 18小时训练

**方案**:

**A. 类别加权损失**:
```python
# 给漏检严重的类别更高权重
class_weights = {
    "低Recall类型": 3.0,
    "中等Recall类型": 1.5,
    "高Recall类型": 1.0
}
```

**B. Recall优化损失**:
```python
# F-beta Loss (beta>1偏向Recall)
def f_beta_loss(pred, target, beta=2):
    # beta=2使Recall权重是Precision的4倍
```

---

#### 3.3 课程学习 (Curriculum Learning)
**难度**: ⭐⭐⭐⭐
**预期提升**: Recall +4-8%
**时间成本**: 8小时实现 + 30小时训练

**策略**:
1. **阶段1**: 训练简单样本（实体明显）
2. **阶段2**: 加入中等难度样本
3. **阶段3**: 重点训练困难样本（历史漏检）

---

### 四、后处理优化 ⭐⭐

#### 4.1 规则增强
**难度**: ⭐
**预期提升**: Recall +2-5%
**时间成本**: 2小时

**方案**:
- 正则表达式辅助（身份证、手机号、邮箱等格式固定的PII）
- 命名实体词典匹配（姓名、地名库）
- 模型输出 + 规则系统混合

```python
def hybrid_detection(text):
    ml_entities = model.predict(text)
    rule_entities = regex_matcher(text)
    return union(ml_entities, rule_entities)
```

---

#### 4.2 置信度阈值调优
**难度**: ⭐
**预期提升**: Recall +1-3%
**时间成本**: 1小时

**当前**: 使用greedy decoding (do_sample=False)
**优化**:
```python
# 降低生成阈值，保留更多候选
outputs = model.generate(
    num_beams=3,  # beam search
    num_return_sequences=3,
    output_scores=True
)
# 选择置信度>0.3的所有实体
```

---

### 五、高级策略 ⭐⭐⭐⭐⭐

#### 5.1 两阶段模型
**难度**: ⭐⭐⭐⭐
**预期提升**: Recall +8-15%
**时间成本**: 40小时

**架构**:
```
阶段1: 召回模型 (高Recall, 允许低Precision)
  ↓ 输出候选实体
阶段2: 精排模型 (高Precision, 过滤误报)
  ↓ 最终输出
```

**优势**: 召回率和精确率解耦优化

---

#### 5.2 对比学习
**难度**: ⭐⭐⭐⭐⭐
**预期提升**: Recall +6-12%
**时间成本**: 60小时

**原理**:
- 正样本：相同PII类型的不同表述
- 负样本：不同PII类型或非PII
- 学习更鲁棒的实体表示

---

#### 5.3 知识蒸馏
**难度**: ⭐⭐⭐⭐
**预期提升**: Recall +5-10%
**时间成本**: 48小时

**流程**:
1. 用超大模型（Qwen2.5-72B）生成软标签
2. 蒸馏到Qwen3-4B
3. 保持推理速度，提升性能

---

## 🎯 推荐优先级排序

### 立即可行 (48小时内)

#### 🥇 第一优先：数据增强 + 继续训练
**组合方案**:
1. **等待漏检分析完成** (10分钟内)
2. **过采样低Recall类型** (2小时)
3. **Focal Loss + 继续训练2 epoch** (36小时)

**预期结果**: Recall 81.48% → 88-92%
**成功概率**: 85%

---

#### 🥈 第二优先：模型集成 + 规则增强
**组合方案**:
1. **Ensemble** (Epoch1 + Epoch2) (1小时)
2. **正则规则辅助** (1小时)

**预期结果**: Recall 81.48% → 86-89%
**成功概率**: 90%
**优势**: 快速，几乎无风险

---

#### 🥉 第三优先：升级到Qwen3-7B
**前提**: 如果数据优化后仍未达标

**预期结果**: Recall → 89-93%
**成本**: 30小时训练
**风险**: 中等

---

## 📊 效果预估矩阵

| 策略 | Recall提升 | 时间成本 | 难度 | ROI | 推荐度 |
|------|-----------|----------|------|-----|--------|
| 数据增强 | +5-10% | 6h | ⭐⭐ | ⭐⭐⭐⭐⭐ | 强烈推荐 |
| 模型集成 | +3-6% | 2h | ⭐⭐ | ⭐⭐⭐⭐⭐ | 强烈推荐 |
| 规则增强 | +2-5% | 2h | ⭐ | ⭐⭐⭐⭐ | 推荐 |
| 继续训练 | +2-5% | 18h | ⭐ | ⭐⭐⭐ | 考虑 |
| Focal Loss | +3-7% | 18h | ⭐⭐⭐ | ⭐⭐⭐⭐ | 推荐 |
| LoRA调优 | +2-4% | 18h | ⭐⭐ | ⭐⭐⭐ | 考虑 |
| Qwen3-7B | +8-12% | 30h | ⭐⭐⭐ | ⭐⭐⭐⭐ | 备选 |
| 两阶段 | +8-15% | 40h | ⭐⭐⭐⭐ | ⭐⭐⭐ | 长期 |

---

## 🚀 我的终极建议

**三步走策略 (总计48小时)**:

### 第1步：快速提升 (2-4小时) ✅ 立即执行
1. ✅ 漏检分析 (进行中)
2. 模型集成 (Epoch1 + Epoch2)
3. 规则增强 (身份证/手机号/邮箱)
- **预期**: Recall → 86-88%

### 第2步：数据优化 (6-8小时) ⭐ 主力
1. 过采样低Recall类别 (3倍)
2. 困难样本增强
3. 数据清洗
- **准备**: 优化训练集

### 第3步：重新训练 (36小时) 🎯 冲刺
1. Focal Loss (gamma=2)
2. 降低学习率 (5e-5)
3. 训练2 epoch
- **目标**: Recall → 90-93%

**总预期**: Recall 81.48% → **90%+**
**成功率**: 80%+

---

## ❓ 您的选择

1. **激进路线**: 立即执行三步走（48小时冲刺90%）
2. **稳健路线**: 先试第1步集成+规则（4小时验证可行性）
3. **保守路线**: 接受当前81.48%，观察实际业务反馈
4. **长期路线**: 升级Qwen3-7B（需要1周时间）

**您倾向于哪个方案？我可以立即开始执行！**
