# Epic 2: LLM ä¸Šä¸‹æ–‡å¼•æ“é›†æˆ

**Epic ID:** EPIC-2
**ä¼˜å…ˆçº§:** P0
**çŠ¶æ€:** ğŸ”„ è¿›è¡Œä¸­
**é¢„è®¡å·¥ä½œé‡:** 2-3 å‘¨
**è´Ÿè´£äºº:** TBD
**åˆ›å»ºæ—¥æœŸ:** 2025-10-14

---

## Epic æ¦‚è¿°

### ç›®æ ‡
é›†æˆ Qwen3 8B å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç°éç»“æ„åŒ– PIIï¼ˆå¦‚å§“åã€åœ°å€ç­‰ï¼‰çš„æ™ºèƒ½æ£€æµ‹ï¼Œè¡¥å……æ­£åˆ™è¡¨è¾¾å¼å¼•æ“æ— æ³•è¦†ç›–çš„åœºæ™¯ã€‚

### ä¸šåŠ¡ä»·å€¼
- **æå‡å¬å›ç‡:** æ£€æµ‹æ­£åˆ™è¡¨è¾¾å¼éš¾ä»¥åŒ¹é…çš„ PIIï¼ˆå¦‚ä¸­æ–‡å§“åã€è‡ªç”±æ ¼å¼åœ°å€ï¼‰
- **ä¸Šä¸‹æ–‡ç†è§£:** åˆ©ç”¨ LLM çš„è¯­ä¹‰ç†è§£èƒ½åŠ›ï¼Œå‡å°‘è¯¯æŠ¥
- **çµæ´»æ‰©å±•:** é€šè¿‡æç¤ºè¯å·¥ç¨‹å¿«é€Ÿæ”¯æŒæ–°çš„ PII ç±»å‹
- **æœ¬åœ°éƒ¨ç½²:** åŸºäºç”¨æˆ·çš„ 2 Ã— RTX 3060 GPUï¼Œæ— éœ€äº‘æœåŠ¡

### ç¡¬ä»¶é…ç½®
**ç”¨æˆ·ç¡¬ä»¶:**
- 2 Ã— NVIDIA RTX 3060 (12GB VRAM æ¯å¼ )
- æ€» VRAM: 24GB
- é€‚åˆè¿è¡Œé‡åŒ–æ¨¡å‹

**éƒ¨ç½²ç­–ç•¥:**
- **æ–¹æ¡ˆ 1:** å•å¡éƒ¨ç½²ï¼ˆæ¨èï¼‰
  - ä½¿ç”¨ 1 å¼  3060 è¿è¡Œ Qwen3 8B (4-bit é‡åŒ–)
  - å¦ 1 å¼ ä¿ç•™ç»™æœªæ¥æ‰©å±•æˆ–æ‰¹å¤„ç†
  - VRAM å ç”¨: ~5-6GB
  - æ¨ç†å»¶è¿Ÿ: ~100-200ms

- **æ–¹æ¡ˆ 2:** å¼ é‡å¹¶è¡Œ
  - ä½¿ç”¨ 2 å¼  3060 åšå¼ é‡å¹¶è¡Œ
  - åŠ é€Ÿæ¨ç†ï¼Œé™ä½å•æ¬¡å»¶è¿Ÿ
  - VRAM å ç”¨: å‡åˆ†æ¨¡å‹å‚æ•°
  - æ¨ç†å»¶è¿Ÿ: ~50-100ms

### ä¾èµ–æ¡ä»¶
- âœ… Epic 1 å®Œæˆï¼ˆæ­£åˆ™å¼•æ“å·²å°±ç»ªï¼‰
- âœ… CUDA å’Œ GPU é©±åŠ¨å·²å®‰è£…
- ğŸ”„ vLLM å®‰è£…ï¼ˆå¾…å®Œæˆï¼‰
- ğŸ”„ Qwen3 8B æ¨¡å‹ä¸‹è½½ï¼ˆå¾…å®Œæˆï¼‰

---

## Stories åˆ—è¡¨

### Story 2.1: vLLM æ¨ç†æœåŠ¡å™¨éƒ¨ç½²
**ä¼˜å…ˆçº§:** P0
**å·¥ä½œé‡:** 3 å¤©

**ç”¨æˆ·æ•…äº‹:**
> ä½œä¸ºå¼€å‘è€…ï¼Œæˆ‘éœ€è¦åœ¨æœ¬åœ° GPU ä¸Šéƒ¨ç½² vLLM æ¨ç†æœåŠ¡ï¼Œä»¥ä¾¿é«˜æ•ˆåœ°è¿è¡Œ Qwen3 æ¨¡å‹ã€‚

**éªŒæ”¶æ ‡å‡†:**
1. âœ… æˆåŠŸå®‰è£… vLLM (æ”¯æŒ CUDA)
2. âœ… å¯åŠ¨ vLLM æœåŠ¡å™¨ï¼Œç›‘å¬æœ¬åœ°ç«¯å£
3. âœ… éªŒè¯ GPU å¯è§æ€§å’Œå†…å­˜ç®¡ç†
4. âœ… å®ç°å¥åº·æ£€æŸ¥ç«¯ç‚¹
5. âœ… é…ç½®å¹¶å‘è¯·æ±‚å¤„ç†ï¼ˆæ”¯æŒå¤šå®¢æˆ·ç«¯ï¼‰
6. âœ… å®ç°æ—¥å¿—å’Œæ€§èƒ½ç›‘æ§

**æŠ€æœ¯è¦æ±‚:**
- ä½¿ç”¨ vLLM 0.6+ ç‰ˆæœ¬
- æ”¯æŒ OpenAI å…¼å®¹ API
- é…ç½® GPU memory utilization (~0.9)
- æ–‡ä»¶ä½ç½®: `src/hppe/engines/llm/vllm_server.py`

**ç¯å¢ƒé…ç½®:**
```bash
# å®‰è£… vLLM
pip install vllm

# éªŒè¯ GPU
nvidia-smi

# å¯åŠ¨æœåŠ¡
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --tensor-parallel-size 1 \
  --gpu-memory-utilization 0.85 \
  --max-model-len 4096 \
  --port 8000
```

**æ€§èƒ½ç›®æ ‡:**
- é¦–æ¬¡åŠ è½½æ—¶é—´: < 60ç§’
- æ¨ç†å»¶è¿Ÿ (P50): < 200ms
- æ¨ç†å»¶è¿Ÿ (P99): < 500ms
- ååé‡: > 10 RPS (å•å¡)

---

### Story 2.2: Qwen3 æ¨¡å‹é›†æˆ
**ä¼˜å…ˆçº§:** P0
**å·¥ä½œé‡:** 2 å¤©

**ç”¨æˆ·æ•…äº‹:**
> ä½œä¸ºå¼€å‘è€…ï¼Œæˆ‘éœ€è¦é›†æˆ Qwen3 8B æ¨¡å‹ï¼Œå¹¶æä¾›ç»Ÿä¸€çš„æ¨ç†æ¥å£ï¼Œä»¥ä¾¿åœ¨ HPPE ä¸­è°ƒç”¨ã€‚

**éªŒæ”¶æ ‡å‡†:**
1. âœ… ä¸‹è½½å¹¶éªŒè¯ Qwen3 8B æ¨¡å‹
2. âœ… å®ç°æ¨¡å‹åŠ è½½å’Œåˆå§‹åŒ–
3. âœ… åˆ›å»º LLMEngine æŠ½è±¡ç±»
4. âœ… å®ç° QwenEngine å…·ä½“ç±»
5. âœ… æ”¯æŒæµå¼å’Œéæµå¼å“åº”
6. âœ… å®ç°é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

**æŠ€æœ¯è¦æ±‚:**
- æ¨¡å‹: Qwen/Qwen2.5-7B-Instruct (æˆ–æ›´æ–°ç‰ˆæœ¬)
- é‡åŒ–: 4-bit (GPTQ/AWQ) å¯é€‰
- æ¥å£: OpenAI å…¼å®¹ API
- æ–‡ä»¶ä½ç½®: `src/hppe/engines/llm/qwen_engine.py`

**API è®¾è®¡:**
```python
from hppe.engines.llm import QwenEngine

# åˆå§‹åŒ–å¼•æ“
engine = QwenEngine(
    model_name="Qwen/Qwen2.5-7B-Instruct",
    base_url="http://localhost:8000/v1",
    temperature=0.1,
    max_tokens=512
)

# è°ƒç”¨æ¨ç†
response = engine.generate(
    prompt="æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ PII: æˆ‘æ˜¯å¼ ä¸‰ï¼Œç”µè¯13812345678",
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éšç§ä¿¡æ¯æ£€æµ‹åŠ©æ‰‹ã€‚"
)
```

**æ¨¡å‹é€‰æ‹©:**
- **Qwen2.5-7B-Instruct**: 7B å‚æ•°ï¼ŒæŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬
- **é‡åŒ–ç‰ˆæœ¬**: Qwen2.5-7B-Instruct-GPTQ-Int4 (æ¨è)
- **VRAM éœ€æ±‚**:
  - FP16: ~14GB (ä¸é€‚åˆå•å¼  3060)
  - 4-bit: ~4-5GB (é€‚åˆå•å¼  3060) âœ…

**ä¾èµ–:**
- Story 2.1 å®Œæˆ

---

### Story 2.3: é›¶æ ·æœ¬ PII æ£€æµ‹å®ç°
**ä¼˜å…ˆçº§:** P0
**å·¥ä½œé‡:** 4 å¤©

**ç”¨æˆ·æ•…äº‹:**
> ä½œä¸ºå¼€å‘è€…ï¼Œæˆ‘éœ€è¦ä½¿ç”¨ LLM å®ç°é›¶æ ·æœ¬ PII æ£€æµ‹ï¼Œä»¥ä¾¿è¯†åˆ«éç»“æ„åŒ–çš„ PIIï¼ˆå§“åã€åœ°å€ç­‰ï¼‰ã€‚

**éªŒæ”¶æ ‡å‡†:**
1. âœ… è®¾è®¡é›¶æ ·æœ¬æ£€æµ‹æç¤ºæ¨¡æ¿
2. âœ… å®ç° LLMRecognizer ç±»ï¼ˆç»§æ‰¿ BaseRecognizerï¼‰
3. âœ… æ”¯æŒä»¥ä¸‹ PII ç±»å‹:
   - ä¸­æ–‡å§“å (PERSON_NAME_ZH)
   - è‹±æ–‡å§“å (PERSON_NAME)
   - ä¸­æ–‡åœ°å€ (ADDRESS_ZH)
   - è‹±æ–‡åœ°å€ (ADDRESS)
   - ç»„ç»‡åç§° (ORGANIZATION)
4. âœ… è§£æ LLM è¾“å‡ºä¸ºç»“æ„åŒ–å®ä½“
5. âœ… å®ç°ç½®ä¿¡åº¦è¯„åˆ†æœºåˆ¶
6. âœ… å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 85%

**æŠ€æœ¯è¦æ±‚:**
- ä½¿ç”¨ JSON Mode æˆ–ç»“æ„åŒ–è¾“å‡º
- æ”¯æŒæ‰¹é‡æ£€æµ‹ï¼ˆä¸€æ¬¡è°ƒç”¨å¤„ç†å¤šä¸ª PIIï¼‰
- å®ç°è¾“å‡ºè§£æå®¹é”™æœºåˆ¶
- æ–‡ä»¶ä½ç½®: `src/hppe/engines/llm/recognizers.py`

**æç¤ºè¯è®¾è®¡åŸåˆ™:**
- æ¸…æ™°çš„ä»»åŠ¡æè¿°
- å…·ä½“çš„è¾“å‡ºæ ¼å¼è¦æ±‚
- å°‘é‡ç¤ºä¾‹ï¼ˆFew-shotï¼Œå¯é€‰ï¼‰
- æ˜ç¡®çš„çº¦æŸæ¡ä»¶

**è¾“å‡ºæ ¼å¼:**
```json
{
  "entities": [
    {
      "type": "PERSON_NAME_ZH",
      "value": "å¼ ä¸‰",
      "start_pos": 5,
      "end_pos": 7,
      "confidence": 0.95
    }
  ]
}
```

**æ€§èƒ½ç›®æ ‡:**
- å•æ¬¡æ£€æµ‹å»¶è¿Ÿ: < 300ms
- å‡†ç¡®ç‡ (Precision): > 80%
- å¬å›ç‡ (Recall): > 85%

**ä¾èµ–:**
- Story 2.2 å®Œæˆ

---

### Story 2.4: æç¤ºå·¥ç¨‹å’Œä¼˜åŒ–
**ä¼˜å…ˆçº§:** P0
**å·¥ä½œé‡:** 3 å¤©

**ç”¨æˆ·æ•…äº‹:**
> ä½œä¸ºå¼€å‘è€…ï¼Œæˆ‘éœ€è¦ä¼˜åŒ– LLM æç¤ºè¯å’Œæ¨ç†å‚æ•°ï¼Œä»¥ä¾¿æé«˜æ£€æµ‹å‡†ç¡®æ€§å’Œæ€§èƒ½ã€‚

**éªŒæ”¶æ ‡å‡†:**
1. âœ… è®¾è®¡å¹¶æµ‹è¯•è‡³å°‘ 3 ç§æç¤ºè¯å˜ä½“
2. âœ… å®ç° A/B æµ‹è¯•æ¡†æ¶
3. âœ… ä¼˜åŒ–æ¨ç†å‚æ•°ï¼ˆtemperature, top_p, max_tokensï¼‰
4. âœ… å®ç°æç¤ºè¯æ¨¡æ¿ç®¡ç†ç³»ç»Ÿ
5. âœ… å»ºç«‹è¯„ä¼°æ•°æ®é›†ï¼ˆä¸­æ–‡ 100 æ¡ï¼Œè‹±æ–‡ 100 æ¡ï¼‰
6. âœ… å¯¹æ¯”æµ‹è¯•ï¼Œé€‰æ‹©æœ€ä½³é…ç½®

**æŠ€æœ¯è¦æ±‚:**
- æ”¯æŒå¤šè¯­è¨€æç¤ºæ¨¡æ¿
- æç¤ºè¯ç‰ˆæœ¬æ§åˆ¶
- å¯é…ç½®çš„æ¨ç†å‚æ•°
- æ–‡ä»¶ä½ç½®: `src/hppe/engines/llm/prompts/`

**æç¤ºè¯ä¼˜åŒ–ç­–ç•¥:**

**ç­–ç•¥ 1: ç›´æ¥æŒ‡ä»¤å¼**
```
ä½ æ˜¯ä¸€ä¸ªéšç§ä¿¡æ¯æ£€æµ‹ä¸“å®¶ã€‚è¯·æ£€æµ‹ä»¥ä¸‹æ–‡æœ¬ä¸­çš„ä¸ªäººèº«ä»½ä¿¡æ¯ï¼ˆPIIï¼‰ã€‚

æ–‡æœ¬: {text}

è¯·è¯†åˆ«æ‰€æœ‰çš„å§“åã€åœ°å€ã€ç”µè¯ã€é‚®ç®±ç­‰æ•æ„Ÿä¿¡æ¯ï¼Œå¹¶ä»¥ JSON æ ¼å¼è¿”å›ã€‚
```

**ç­–ç•¥ 2: è§’è‰²æ‰®æ¼”å¼**
```
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æ•°æ®åˆè§„ä¸“å®¶ï¼Œè´Ÿè´£å®¡æŸ¥æ–‡æ¡£ä¸­çš„éšç§é£é™©ã€‚
è¯·ä»”ç»†åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼Œè¯†åˆ«æ‰€æœ‰å¯èƒ½å¯¼è‡´éšç§æ³„éœ²çš„ä¸ªäººèº«ä»½ä¿¡æ¯ã€‚

æ–‡æœ¬: {text}

è¾“å‡ºæ ¼å¼: JSON
```

**ç­–ç•¥ 3: Few-shot ç¤ºä¾‹å¼**
```
ä»»åŠ¡: æ£€æµ‹æ–‡æœ¬ä¸­çš„ PII

ç¤ºä¾‹ 1:
è¾“å…¥: "æˆ‘æ˜¯å¼ ä¸‰ï¼Œä½åœ¨åŒ—äº¬å¸‚æµ·æ·€åŒº"
è¾“å‡º: {"entities": [{"type": "å§“å", "value": "å¼ ä¸‰"}, {"type": "åœ°å€", "value": "åŒ—äº¬å¸‚æµ·æ·€åŒº"}]}

ç¤ºä¾‹ 2:
è¾“å…¥: "è”ç³»æ–¹å¼: john@example.com"
è¾“å‡º: {"entities": [{"type": "é‚®ç®±", "value": "john@example.com"}]}

ç°åœ¨è¯·å¤„ç†:
è¾“å…¥: {text}
è¾“å‡º:
```

**è¯„ä¼°æŒ‡æ ‡:**
- å‡†ç¡®ç‡ (Precision)
- å¬å›ç‡ (Recall)
- F1-Score
- æ¨ç†å»¶è¿Ÿ
- Token ä½¿ç”¨é‡

**ä¾èµ–:**
- Story 2.3 å®Œæˆ

---

## æŠ€æœ¯æ¶æ„

### 1. ç»„ä»¶æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HPPE Core System                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Regex Engine   â”‚        â”‚   LLM Engine    â”‚        â”‚
â”‚  â”‚  (Epic 1)       â”‚        â”‚   (Epic 2)      â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚           â”‚                          â”‚                  â”‚
â”‚           â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚                  â”‚
â”‚           â””â”€â”€â”€â”€â”¤ Detection Pipeline â”œâ”˜                  â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                          â”‚                              â”‚
â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚                â”‚  Result Merger     â”‚                   â”‚
â”‚                â”‚  (Epic 3)          â”‚                   â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              vLLM Inference Server (Local)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  Qwen3 8B Model (4-bit Quantized)    â”‚               â”‚
â”‚  â”‚  VRAM: ~5GB                           â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  OpenAI-compatible API                â”‚               â”‚
â”‚  â”‚  Endpoint: http://localhost:8000/v1   â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  GPU Hardware (Local)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  RTX 3060 #1    â”‚        â”‚  RTX 3060 #2    â”‚        â”‚
â”‚  â”‚  12GB VRAM      â”‚        â”‚  12GB VRAM      â”‚        â”‚
â”‚  â”‚  (Active)       â”‚        â”‚  (Reserved)     â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. æ•°æ®æµ

```
ç”¨æˆ·è¾“å…¥æ–‡æœ¬
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Splitter  â”‚  â† åˆ†å—ï¼ˆé•¿æ–‡æœ¬ï¼‰
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Regex Engineâ”‚    â”‚ LLM Engine â”‚
â”‚  (Fast)    â”‚    â”‚  (Smart)   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      â”‚                  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Entity Merger   â”‚  â† å»é‡ã€åˆå¹¶
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Result   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. LLM Engine ç±»è®¾è®¡

```python
# src/hppe/engines/llm/base.py
from abc import ABC, abstractmethod
from typing import List, Optional
from hppe.models.entity import Entity

class BaseLLMEngine(ABC):
    """LLM å¼•æ“æŠ½è±¡åŸºç±»"""

    @abstractmethod
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: int = 512
    ) -> str:
        """ç”Ÿæˆæ–‡æœ¬å“åº”"""
        pass

    @abstractmethod
    def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        pass


# src/hppe/engines/llm/qwen_engine.py
class QwenEngine(BaseLLMEngine):
    """Qwen3 æ¨¡å‹å¼•æ“å®ç°"""

    def __init__(
        self,
        base_url: str = "http://localhost:8000/v1",
        model_name: str = "Qwen/Qwen2.5-7B-Instruct",
        timeout: int = 30
    ):
        self.base_url = base_url
        self.model_name = model_name
        self.timeout = timeout
        self.client = OpenAI(base_url=base_url)

    def generate(self, prompt: str, **kwargs) -> str:
        """è°ƒç”¨ vLLM API ç”Ÿæˆå“åº”"""
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": kwargs.get("system_prompt", "")},
                {"role": "user", "content": prompt}
            ],
            temperature=kwargs.get("temperature", 0.1),
            max_tokens=kwargs.get("max_tokens", 512)
        )
        return response.choices[0].message.content


# src/hppe/engines/llm/recognizers.py
class LLMRecognizer(BaseRecognizer):
    """åŸºäº LLM çš„ PII è¯†åˆ«å™¨"""

    def __init__(self, config: Dict, llm_engine: BaseLLMEngine):
        super().__init__(config)
        self.llm_engine = llm_engine
        self.prompt_template = self._load_prompt_template()

    def detect(self, text: str) -> List[Entity]:
        """ä½¿ç”¨ LLM æ£€æµ‹ PII"""
        # 1. æ„å»ºæç¤ºè¯
        prompt = self.prompt_template.format(text=text)

        # 2. è°ƒç”¨ LLM
        response = self.llm_engine.generate(
            prompt=prompt,
            temperature=0.1,
            max_tokens=512
        )

        # 3. è§£æå“åº”
        entities = self._parse_llm_response(response, text)

        # 4. éªŒè¯å’Œè¿‡æ»¤
        validated_entities = [
            e for e in entities
            if self.validate(e)
        ]

        return validated_entities

    def _parse_llm_response(
        self,
        response: str,
        original_text: str
    ) -> List[Entity]:
        """è§£æ LLM JSON è¾“å‡ºä¸º Entity å¯¹è±¡"""
        try:
            data = json.loads(response)
            entities = []

            for item in data.get("entities", []):
                entity = Entity(
                    entity_type=item["type"],
                    value=item["value"],
                    start_pos=item.get("start_pos", 0),
                    end_pos=item.get("end_pos", 0),
                    confidence=item.get("confidence", 0.8),
                    detection_method="llm",
                    recognizer_name=self.recognizer_name,
                    metadata={"llm_model": self.llm_engine.model_name}
                )
                entities.append(entity)

            return entities

        except json.JSONDecodeError:
            # å®¹é”™ï¼šå°è¯•ä¿®å¤ JSON
            return []
```

---

## æµ‹è¯•ç­–ç•¥

### 1. å•å…ƒæµ‹è¯•
- **LLM Engine æµ‹è¯•**: Mock API è°ƒç”¨
- **Recognizer æµ‹è¯•**: ä½¿ç”¨å›ºå®šçš„ LLM å“åº”
- **æç¤ºè¯æµ‹è¯•**: éªŒè¯æ¨¡æ¿æ¸²æŸ“

### 2. é›†æˆæµ‹è¯•
- **ç«¯åˆ°ç«¯æµ‹è¯•**: å¯åŠ¨çœŸå® vLLM æœåŠ¡
- **å¤šè¯­è¨€æµ‹è¯•**: ä¸­æ–‡å’Œè‹±æ–‡æ–‡æœ¬
- **è¾¹ç•Œæµ‹è¯•**: é•¿æ–‡æœ¬ã€ç‰¹æ®Šå­—ç¬¦

### 3. æ€§èƒ½æµ‹è¯•
- **å»¶è¿Ÿæµ‹è¯•**: P50, P95, P99
- **ååé‡æµ‹è¯•**: RPS
- **GPU åˆ©ç”¨ç‡**: nvidia-smi ç›‘æ§

### 4. å‡†ç¡®æ€§æµ‹è¯•
- **åŸºå‡†æ•°æ®é›†**: 200 æ¡æ ‡æ³¨æ•°æ®
- **æŒ‡æ ‡è®¡ç®—**: Precision, Recall, F1
- **å¯¹æ¯”æµ‹è¯•**: LLM vs Regex

---

## æ€§èƒ½ç›®æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | æµ‹é‡æ–¹æ³• |
|------|--------|----------|
| æ¨¡å‹åŠ è½½æ—¶é—´ | < 60ç§’ | é¦–æ¬¡å¯åŠ¨è®¡æ—¶ |
| æ¨ç†å»¶è¿Ÿ (P50) | < 200ms | å•æ¬¡è¯·æ±‚æ—¶é—´ |
| æ¨ç†å»¶è¿Ÿ (P99) | < 500ms | 99åˆ†ä½å»¶è¿Ÿ |
| GPU å†…å­˜å ç”¨ | < 8GB | nvidia-smi |
| å‡†ç¡®ç‡ (Precision) | > 80% | æµ‹è¯•é›†è¯„ä¼° |
| å¬å›ç‡ (Recall) | > 85% | æµ‹è¯•é›†è¯„ä¼° |
| F1-Score | > 82% | è°ƒå’Œå¹³å‡æ•° |

---

## é£é™©ä¸ç¼“è§£

### é£é™© 1: GPU å†…å­˜ä¸è¶³
**é£é™©çº§åˆ«:** ä¸­
**æè¿°:** Qwen3 8B FP16 ç‰ˆæœ¬éœ€è¦ ~14GBï¼Œè¶…è¿‡å•å¼  3060 å®¹é‡

**ç¼“è§£æªæ–½:**
- âœ… ä½¿ç”¨ 4-bit é‡åŒ–ç‰ˆæœ¬ï¼ˆé™è‡³ ~5GBï¼‰
- âœ… é…ç½® `gpu-memory-utilization=0.85`
- âœ… é™åˆ¶ `max-model-len=4096`

### é£é™© 2: æ¨ç†å»¶è¿Ÿè¿‡é«˜
**é£é™©çº§åˆ«:** ä¸­
**æè¿°:** LLM æ¨ç†å¯èƒ½å¯¼è‡´ç«¯åˆ°ç«¯å»¶è¿Ÿè¶…è¿‡ 500ms

**ç¼“è§£æªæ–½:**
- ä½¿ç”¨ vLLM çš„ PagedAttention ä¼˜åŒ–
- å‡å°‘ `max_tokens` åˆ° 512
- å®ç°è¯·æ±‚æ‰¹å¤„ç†
- ç¼“å­˜å¸¸è§æŸ¥è¯¢

### é£é™© 3: æ¨¡å‹è¾“å‡ºä¸ç¨³å®š
**é£é™©çº§åˆ«:** é«˜
**æè¿°:** LLM å¯èƒ½è¾“å‡ºæ ¼å¼ä¸ä¸€è‡´æˆ–åŒ…å«å¹»è§‰

**ç¼“è§£æªæ–½:**
- ä½¿ç”¨ä½ temperature (0.1)
- å®ç°è¾“å‡ºè§£æå®¹é”™
- æ·»åŠ åå¤„ç†éªŒè¯
- ä½¿ç”¨ JSON Modeï¼ˆå¦‚æœæ”¯æŒï¼‰

### é£é™© 4: ä¸­æ–‡å§“åæ£€æµ‹å‡†ç¡®ç‡
**é£é™©çº§åˆ«:** ä¸­
**æè¿°:** ä¸­æ–‡å§“åå¤šæ ·æ€§é«˜ï¼Œå¯èƒ½è¯¯æŠ¥æˆ–æ¼æŠ¥

**ç¼“è§£æªæ–½:**
- ä¼˜åŒ–ä¸­æ–‡æç¤ºè¯
- å¢åŠ  Few-shot ç¤ºä¾‹
- ç»“åˆä¸Šä¸‹æ–‡è¯ï¼ˆ"å…ˆç”Ÿ"ã€"å¥³å£«"ç­‰ï¼‰
- å»ºç«‹ä¸­æ–‡å§“åæ•°æ®é›†

---

## äº¤ä»˜ç‰©æ¸…å•

### ä»£ç æ–‡ä»¶
1. `src/hppe/engines/llm/__init__.py` - LLM å¼•æ“æ¨¡å—
2. `src/hppe/engines/llm/base.py` - æŠ½è±¡åŸºç±»
3. `src/hppe/engines/llm/qwen_engine.py` - Qwen å¼•æ“å®ç°
4. `src/hppe/engines/llm/recognizers.py` - LLM è¯†åˆ«å™¨
5. `src/hppe/engines/llm/prompts/` - æç¤ºè¯æ¨¡æ¿ç›®å½•
6. `src/hppe/engines/llm/utils.py` - å·¥å…·å‡½æ•°

### æµ‹è¯•æ–‡ä»¶
1. `tests/unit/test_llm_engine.py` - å¼•æ“å•å…ƒæµ‹è¯•
2. `tests/unit/test_llm_recognizers.py` - è¯†åˆ«å™¨å•å…ƒæµ‹è¯•
3. `tests/integration/test_vllm_integration.py` - vLLM é›†æˆæµ‹è¯•
4. `tests/benchmark/test_llm_performance.py` - æ€§èƒ½åŸºå‡†æµ‹è¯•

### é…ç½®æ–‡ä»¶
1. `configs/llm_config.yaml` - LLM é…ç½®
2. `configs/prompts.yaml` - æç¤ºè¯é…ç½®
3. `scripts/start_vllm.sh` - vLLM å¯åŠ¨è„šæœ¬

### æ–‡æ¡£
1. `docs/llm-integration-guide.md` - é›†æˆæŒ‡å—
2. `docs/prompt-engineering-guide.md` - æç¤ºè¯å·¥ç¨‹æŒ‡å—
3. `examples/llm_detection_example.py` - ä½¿ç”¨ç¤ºä¾‹
4. `EPIC_2_COMPLETION_REPORT.md` - Epic å®ŒæˆæŠ¥å‘Š

---

## æ—¶é—´è§„åˆ’

| Week | Story | ä¸»è¦ä»»åŠ¡ | é‡Œç¨‹ç¢‘ |
|------|-------|----------|--------|
| Week 1 | 2.1 | vLLM éƒ¨ç½² | vLLM æœåŠ¡å¯ç”¨ |
| Week 2 | 2.2, 2.3 | æ¨¡å‹é›†æˆ + é›¶æ ·æœ¬æ£€æµ‹ | åŸºç¡€æ£€æµ‹åŠŸèƒ½ |
| Week 3 | 2.4 | æç¤ºè¯ä¼˜åŒ– | è¾¾åˆ°å‡†ç¡®ç‡ç›®æ ‡ |

---

## ä¾èµ–ä¸å‰ç½®æ¡ä»¶

### å‰ç½®æ¡ä»¶
- âœ… Epic 1 å®Œæˆ
- âœ… 2 Ã— NVIDIA RTX 3060 GPU
- âœ… CUDA 11.8+ å®‰è£…
- âœ… Python 3.10+

### å¤–éƒ¨ä¾èµ–
- vLLM åº“
- Qwen3 8B æ¨¡å‹
- OpenAI Python SDK
- GPU é©±åŠ¨

---

## éªŒæ”¶æ ‡å‡†

Epic 2 å®Œæˆéœ€è¦æ»¡è¶³ä»¥ä¸‹æ‰€æœ‰æ¡ä»¶:

1. âœ… æ‰€æœ‰ 4 ä¸ª Stories (2.1-2.4) å®Œæˆ
2. âœ… vLLM æœåŠ¡æˆåŠŸéƒ¨ç½²å¹¶è¿è¡Œ
3. âœ… æ”¯æŒè‡³å°‘ 5 ç§éç»“æ„åŒ– PII æ£€æµ‹
4. âœ… å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%
5. âœ… é›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡
6. âœ… å‡†ç¡®ç‡è¾¾åˆ°ç›®æ ‡ (Precision > 80%, Recall > 85%)
7. âœ… æ€§èƒ½è¾¾åˆ°ç›®æ ‡ (P50 < 200ms)
8. âœ… å®Œæˆæ–‡æ¡£å’Œç¤ºä¾‹
9. âœ… ä¸ Epic 1 æ— ç¼é›†æˆ

---

**æ–‡æ¡£çŠ¶æ€:** âœ… å·²å®Œæˆ
**ä¸‹ä¸€æ­¥:** å¼€å§‹ Story 2.1 å®ç°
**æ›´æ–°æ—¥æœŸ:** 2025-10-14
