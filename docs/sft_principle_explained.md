# SFT 微调原理深度解析

## 📚 目录
1. [什么是 SFT](#什么是-sft)
2. [为什么需要 SFT](#为什么需要-sft)
3. [SFT vs 预训练](#sft-vs-预训练)
4. [LoRA 技术详解](#lora-技术详解)
5. [实际应用流程](#实际应用流程)
6. [参数调优指南](#参数调优指南)

---

## 什么是 SFT

### 定义

**SFT (Supervised Fine-Tuning)** = 监督微调

用少量**标注数据**教会大模型做**特定任务**的过程。

### 核心思想

```
预训练模型（通用能力）+ 任务数据（专业知识）= 专业模型（特定任务高手）
```

### 类比理解

| 阶段 | 类比 | 能力 | 数据量 |
|------|------|------|--------|
| **预训练** | 大学通识教育 | 广泛但浅层 | TB级（万亿token） |
| **SFT微调** | 岗位专业培训 | 专精且深入 | GB级（千万-亿token） |

---

## 为什么需要 SFT

### 问题 1：预训练模型不够专业

**例子：PII 检测**

**预训练模型（Qwen3-8B zero-shot）：**
```
输入："我叫张三，电话13800138000"
输出："好的，我看到您提供了姓名和电话号码。
       姓名是张三，电话是13800138000。
       这些是个人信息，请注意保护隐私..."
```
❌ 输出格式不统一
❌ 包含多余信息
❌ 不是结构化JSON

**SFT微调后（Qwen2-1.5B fine-tuned）：**
```
输入："我叫张三，电话13800138000"
输出：{"entities": [
    {"type": "PERSON_NAME", "value": "张三", "start": 2, "end": 4},
    {"type": "PHONE_NUMBER", "value": "13800138000", "start": 6, "end": 17}
]}
```
✅ 格式固定
✅ 结果准确
✅ 可直接使用

### 问题 2：预训练模型效率低

**推理速度对比：**
- Qwen3-8B (未微调): 8-12秒/样本
- Qwen2-1.5B (微调后): **2-4秒/样本** (3-6倍提速)

**显存占用对比：**
- Qwen3-8B: 12GB
- Qwen2-1.5B: **6-8GB** (节省一半)

### 问题 3：特定领域知识不足

预训练模型的训练数据：
- 来源：互联网文本、书籍、代码
- 特点：通用，但可能缺少**你的特定场景**

**举例：**
- 医疗领域：专业术语、诊断模式
- 法律领域：法条引用、判例分析
- **PII检测**：各种上下文中的敏感信息模式

通过 SFT，可以让模型学习**你的领域特定知识**。

---

## SFT vs 预训练

### 对比表

| 维度 | 预训练 (Pre-training) | SFT微调 (Fine-tuning) |
|------|---------------------|---------------------|
| **目标** | 学习通用语言能力 | 学习特定任务 |
| **数据** | TB级，未标注 | GB级，已标注 |
| **样本数** | 万亿级 token | 千万-亿级 token |
| **训练时间** | 数周到数月 | 数小时到数天 |
| **计算资源** | 数百-数千GPU | 1-8 GPU |
| **成本** | 百万美元级 | 数百-数千美元 |
| **产出** | 通用基础模型 | 任务专用模型 |

### 训练数据对比

**预训练数据（Qwen3）：**
```
来源：
- 互联网网页：60%
- 书籍文献：20%
- 代码仓库：15%
- 其他：5%

特点：
- 未标注
- 自监督学习（预测下一个词）
- 数量：~15T tokens
```

**SFT数据（PII检测）：**
```
来源：
- MSRA NER：15k 标注样本
- ai4privacy：60k 标注样本
- 合成数据：30k 标注样本

特点：
- 人工标注或规则生成
- 有明确的输入-输出对应
- 数量：~100k 样本，~50M tokens
```

---

## LoRA 技术详解

### 为什么需要 LoRA

**全参数微调的问题：**

假设模型有 **15 亿个参数**（Qwen2-1.5B）：

```
原始参数矩阵：[1500M 参数]

全参数微调：
- 需要训练所有 1500M 参数
- 显存需求：48GB+（存储参数、梯度、优化器状态）
- 训练时间：数天
- 存储空间：6GB（模型权重）
```

**大多数参数其实不需要改！**

研究发现：
- 微调时，只需调整**少量参数**就能适应新任务
- 大部分预训练知识应该**保持不变**

### LoRA 核心思想

**Low-Rank Adaptation** = 低秩适配

**数学原理：**

```
原始权重矩阵：W ∈ R^(d×k)  (例如：4096×4096)

不直接修改 W，而是添加一个低秩分解：
ΔW = B × A

其中：
- A ∈ R^(d×r)  (例如：4096×8)
- B ∈ R^(r×k)  (例如：8×4096)
- r << d, k  (r=8, d=k=4096)

新的权重：W' = W + ΔW = W + B×A
```

**参数量对比：**

```
全参数微调：
- 可训练参数：4096 × 4096 = 16,777,216 参数（每层）

LoRA微调：
- 可训练参数：(4096 × 8) + (8 × 4096) = 65,536 参数（每层）
- 减少：256倍！
```

### LoRA 可视化

```
传统微调：
┌────────────────┐
│  原始模型参数   │  ← 全部需要训练
│  [15亿参数]    │  ← 显存: 48GB+
└────────────────┘

LoRA微调：
┌────────────────┐
│  原始模型参数   │  ← 冻结（不训练）
│  [15亿参数]    │  ← 显存: 12GB
└────────────────┘
        +
┌────────────────┐
│  LoRA适配器    │  ← 只训练这部分！
│  [600万参数]   │  ← 显存: +2GB
│  (0.4%)        │
└────────────────┘
```

### LoRA 超参数

#### 1. rank (r)

**含义：** 低秩矩阵的维度

```
较小的 r (4-8)：
- 参数少，训练快
- 适合简单任务
- 可能欠拟合

较大的 r (16-32)：
- 参数多，表达能力强
- 适合复杂任务
- 可能过拟合
```

**推荐值：**
- 简单任务：r = 4-8
- 中等任务：r = 8-16
- 复杂任务：r = 16-32

#### 2. alpha (α)

**含义：** 缩放因子

```
LoRA 的实际影响 = (α / r) × ΔW

常用配置：
- α = 2 × r  (最常见)
  - r=8, α=16
  - r=16, α=32

效果：
- α 小：保守更新，接近原模型
- α 大：激进更新，可能偏离原模型
```

#### 3. target_modules

**含义：** 在哪些层应用 LoRA

**Transformer 结构：**
```
┌─────────────────────┐
│  Embedding Layer    │
└─────────────────────┘
         │
┌─────────────────────┐
│  Attention Block    │
│  ┌───────────────┐  │
│  │ Q projection  │ ← 可以加LoRA
│  │ K projection  │ ← 可以加LoRA
│  │ V projection  │ ← 可以加LoRA
│  │ O projection  │ ← 可以加LoRA
│  └───────────────┘  │
└─────────────────────┘
         │
┌─────────────────────┐
│  FFN Block          │
│  ┌───────────────┐  │
│  │ up projection │ ← 可以加LoRA
│  │ down proj.    │ ← 可以加LoRA
│  └───────────────┘  │
└─────────────────────┘
```

**常用配置：**

```python
# 配置 1：仅Attention（最常见）
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
# 优点：参数少，训练快
# 适合：大多数任务

# 配置 2：Attention + FFN（全面）
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                  "gate_proj", "up_proj", "down_proj"]
# 优点：表达能力强
# 适合：复杂任务

# 配置 3：仅 Q、V（轻量）
target_modules = ["q_proj", "v_proj"]
# 优点：超轻量
# 适合：简单任务或资源受限
```

---

## 实际应用流程

### 完整流程图

```
┌─────────────────────────────────────────────────────────┐
│                     1. 数据准备                          │
├─────────────────────────────────────────────────────────┤
│ 原始数据 → 清洗 → 标注 → 格式化 → 划分train/val/test  │
│                                                           │
│ 示例：                                                    │
│ {"input": "我叫张三", "output": {...}}                   │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│                     2. 模型准备                          │
├─────────────────────────────────────────────────────────┤
│ 下载基础模型 → 加载 → 配置LoRA → 冻结原始参数          │
│                                                           │
│ 基础模型：Qwen/Qwen2-1.5B                               │
│ LoRA配置：r=8, alpha=16                                  │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│                     3. 训练循环                          │
├─────────────────────────────────────────────────────────┤
│ for epoch in range(num_epochs):                          │
│     for batch in train_data:                             │
│         # 前向传播                                        │
│         output = model(input)                            │
│         # 计算损失                                        │
│         loss = criterion(output, target)                 │
│         # 反向传播（只更新LoRA参数）                      │
│         loss.backward()                                  │
│         optimizer.step()                                 │
│     # 验证                                                │
│     eval_loss = evaluate(model, val_data)                │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│                     4. 模型保存                          │
├─────────────────────────────────────────────────────────┤
│ 保存：原始模型 + LoRA适配器                              │
│                                                           │
│ 文件结构：                                                │
│ ├── adapter_config.json  (LoRA配置)                     │
│ ├── adapter_model.bin    (LoRA权重，~20MB)              │
│ └── tokenizer files                                      │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│                     5. 推理使用                          │
├─────────────────────────────────────────────────────────┤
│ 加载原始模型 → 加载LoRA适配器 → 推理                    │
│                                                           │
│ model = AutoModel.from_pretrained("Qwen2-1.5B")         │
│ model = PeftModel.from_pretrained(model, "lora_path")   │
└─────────────────────────────────────────────────────────┘
```

### 训练过程详解

#### Step 1: 前向传播

```python
# 输入文本
input_text = "我叫张三，电话13800138000"

# 分词
input_ids = tokenizer.encode(input_text)
# → [123, 456, 789, ...]

# 模型推理
logits = model(input_ids)  # 预测每个位置的下一个token概率
# → [vocab_size] 的概率分布

# 解码
predicted_ids = torch.argmax(logits, dim=-1)
output_text = tokenizer.decode(predicted_ids)
# → '{"entities": [{"type": "PERSON_NAME", ...}]}'
```

#### Step 2: 计算损失

```python
# 真实输出（ground truth）
target_text = '{"entities": [{"type": "PERSON_NAME", "value": "张三", ...}]}'
target_ids = tokenizer.encode(target_text)

# 计算交叉熵损失（对比预测和真实）
loss = CrossEntropyLoss(logits, target_ids)
# → 损失值（越小越好）

# 损失含义：
# - 初始（未训练）：loss ≈ 10
# - 训练中：loss ≈ 2-4
# - 训练完成：loss ≈ 0.5-1.5
```

#### Step 3: 反向传播

```python
# 计算梯度（哪些参数需要怎么调整）
loss.backward()

# 更新参数（只更新LoRA部分）
for name, param in model.named_parameters():
    if 'lora' in name:  # 只更新LoRA参数
        param.data -= learning_rate * param.grad

optimizer.step()
```

---

## 参数调优指南

### 学习率 (Learning Rate)

**作用：** 控制参数更新的步长

```
学习率太大：
- 训练不稳定
- 可能跳过最优点
- 损失震荡或发散

学习率太小：
- 训练太慢
- 可能卡在局部最优
- 需要更多轮次
```

**推荐值：**
- 全参数微调：1e-5 ~ 5e-5
- LoRA微调：1e-4 ~ 5e-4
- PII检测任务：**2e-4**

**学习率调度：**
```python
# Warmup + Cosine Decay
Epoch 1-100: 0 → 2e-4  (warmup)
Epoch 100-3000: 2e-4 → 1e-5  (cosine decay)
```

### 批次大小 (Batch Size)

**作用：** 每次训练使用多少样本

```
批次大小 = 单GPU批次 × 梯度累积 × GPU数量

例如：
- per_device_batch_size: 4
- gradient_accumulation_steps: 4
- num_gpus: 2
- 有效批次大小 = 4 × 4 × 2 = 32
```

**推荐值：**
- RTX 3060 12GB：单GPU批次 = 4-8
- 有效批次大小：32-64

**权衡：**
```
大批次（64-128）：
✅ 训练稳定
✅ 梯度估计准确
❌ 显存占用大
❌ 可能泛化性差

小批次（8-32）：
✅ 显存友好
✅ 泛化性好
❌ 训练噪声大
❌ 需要更多轮次
```

### 训练轮数 (Epochs)

**作用：** 数据集被完整遍历的次数

```
数据集大小：40,000 样本
批次大小：32
每轮步数：40,000 / 32 = 1,250 步

训练3轮 = 3,750 步
```

**推荐值：**
- 小数据集（<10k）：5-10 epochs
- 中等数据集（10k-50k）：3-5 epochs
- 大数据集（>50k）：2-3 epochs

**过拟合检测：**
```
训练损失 vs 验证损失：

正常：
Epoch 1: train_loss=2.0, val_loss=2.2
Epoch 2: train_loss=1.5, val_loss=1.6
Epoch 3: train_loss=1.0, val_loss=1.1  ← 都在下降

过拟合：
Epoch 1: train_loss=2.0, val_loss=2.2
Epoch 2: train_loss=1.5, val_loss=1.6
Epoch 3: train_loss=0.5, val_loss=2.0  ← 验证损失上升
Epoch 4: train_loss=0.3, val_loss=2.5  ← 继续恶化

解决：减少训练轮数，或增加数据量
```

---

## 常见问题

### Q1: SFT 会不会"遗忘"预训练知识？

**是的，可能会！** 这叫做"灾难性遗忘"（Catastrophic Forgetting）

**原因：**
```
过度训练特定任务 → 通用能力下降

例如：
训练前：既能检测PII，也能写诗、聊天
训练后：只会检测PII，其他能力退化
```

**解决方法：**
1. 使用 LoRA（不修改原始参数）
2. 控制训练轮数（不要过度训练）
3. 混合通用数据（10-20%通用对话数据）

### Q2: 为什么不直接用 Prompt Engineering？

**对比：**

| 方法 | 优点 | 缺点 | 适用场景 |
|------|------|------|---------|
| **Prompt** | 无需训练<br>即时可用 | 效果不稳定<br>Token消耗大 | 快速原型<br>一次性任务 |
| **SFT** | 效果稳定<br>推理快速 | 需要训练时间<br>需要标注数据 | 生产部署<br>高频任务 |

**示例：**
```
Prompt方式（每次推理需要）：
系统提示词：~200 tokens
任务说明：~100 tokens
示例样本：~500 tokens
输入文本：~50 tokens
─────────────────
总计：850 tokens/次

SFT方式：
输入文本：~50 tokens
─────────────────
总计：50 tokens/次

节省：94% tokens
```

### Q3: 数据量多少才够？

**经验法则：**

```
任务复杂度 → 所需数据量

简单任务（二分类）：1k-5k 样本
中等任务（多分类）：5k-20k 样本
复杂任务（序列标注）：20k-100k 样本

PII检测：
- 最少：10k 样本（准确率 ~90%）
- 推荐：50k 样本（准确率 ~95-99%）
- 理想：100k+ 样本（准确率 >99%）
```

**数据质量 > 数据数量：**
```
10k 高质量样本 > 50k 低质量样本

高质量标准：
- 标注准确
- 覆盖多样场景
- 标注一致性高
```

---

## 总结

### SFT 的本质

**用少量专业数据，让通用大模型变成领域专家。**

### 关键要点

1. **LoRA** - 用 0.4% 的参数，达到全参数微调 95% 的效果
2. **数据第一** - 质量 > 数量，多样性很重要
3. **防止过拟合** - 监控验证损失，适时停止
4. **参数调优** - 从推荐值开始，逐步调整

### 您的 PII 检测场景

**推荐配置：**
```python
model: Qwen/Qwen2-1.5B
data: 50,000 样本（30% MSRA + 30% ai4privacy + 40% 合成）
lora_r: 8-16
lora_alpha: 16-32
batch_size: 4（单GPU）× 4（梯度累积）× 2（GPU）= 32
learning_rate: 2e-4
epochs: 3-5
训练时间: 2-3 小时
```

**预期效果：**
- 准确率：95-99%
- 推理速度：2-4秒/样本（比原8B模型快3-6倍）
- 显存占用：6-8GB（节省一半）

---

**更新日期：** 2025-10-14
**参考资料：**
- LoRA 论文：https://arxiv.org/abs/2106.09685
- Qwen2 技术报告：https://arxiv.org/abs/2407.10671
