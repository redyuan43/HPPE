# Story 2.2: Qwen3 模型集成 - 下载指南

## 📋 文档信息

- **Story**: 2.2 Qwen3 模型集成
- **状态**: 🔄 准备中
- **创建时间**: 2025-10-14
- **前置条件**: Story 2.1 vLLM 基础设施（✅ 已完成）

---

## 🎯 目标

下载并配置 Qwen2.5-7B-Instruct 模型，用于本地 PII 上下文检测。

---

## 📊 模型选择指南

### 支持的模型版本

| 版本 | 大小 | 显存需求 | 推理速度 | 质量 | 推荐场景 |
|------|------|---------|---------|------|---------|
| **Qwen2.5-7B-Instruct (FP16)** | ~14GB | ~14GB | 快 | 最高 | 大显存 GPU (>16GB) |
| **Qwen2.5-7B-Instruct-AWQ** | ~4.5GB | ~6GB | 很快 | 高 | **RTX 3060 推荐** ✅ |
| **Qwen2.5-7B-Instruct-GPTQ-Int4** | ~4.5GB | ~6GB | 快 | 高 | RTX 3060 备选 |

### 硬件匹配

**用户配置**：
- 2 × NVIDIA RTX 3060 (12GB VRAM)
- CUDA 13.0（兼容 CUDA 12.1）

**推荐配置**：
```yaml
模型: Qwen2.5-7B-Instruct-AWQ (4-bit)
GPU: GPU 0 (单卡部署)
显存占用: ~5-6GB
剩余显存: ~6GB (用于推理缓存)
并发请求: 8-10
延迟: 100-200ms
```

**原因**：
- ✅ AWQ 量化质量优于 GPTQ（更接近 FP16）
- ✅ 4-bit 量化显存需求适合 RTX 3060
- ✅ 保留足够显存用于 KV 缓存和并发
- ✅ 推理速度快（优化的 CUDA kernel）

---

## 🚀 快速开始

### 方法 1: 使用下载脚本（推荐）

```bash
# 激活 conda 环境
conda activate hppe

# 下载 AWQ 4-bit 量化模型（推荐）
python scripts/download_model.py --quantization awq --bits 4

# 或下载标准 FP16 模型
python scripts/download_model.py
```

### 方法 2: 手动使用 huggingface-cli

```bash
# 激活环境
conda activate hppe

# 安装 huggingface-hub
pip install -U huggingface-hub

# 下载 AWQ 模型
huggingface-cli download Qwen/Qwen2.5-7B-Instruct-AWQ \
  --resume-download \
  --local-dir ./models/Qwen2.5-7B-Instruct-AWQ

# 或下载标准模型
huggingface-cli download Qwen/Qwen2.5-7B-Instruct \
  --resume-download \
  --local-dir ./models/Qwen2.5-7B-Instruct
```

### 方法 3: 使用 Python API

```python
from huggingface_hub import snapshot_download

# 下载 AWQ 模型
model_path = snapshot_download(
    repo_id="Qwen/Qwen2.5-7B-Instruct-AWQ",
    resume_download=True,
    cache_dir="./models"
)

print(f"模型路径: {model_path}")
```

---

## 📦 下载详细步骤

### 步骤 1: 检查磁盘空间

```bash
# 检查可用空间（需要至少 10GB）
df -h /home/ivan

# 查看 HuggingFace 缓存目录
du -sh ~/.cache/huggingface/hub 2>/dev/null || echo "缓存目录不存在"
```

**所需空间**：
- AWQ 4-bit: ~5GB（下载） + ~5GB（缓存） = **10GB**
- FP16: ~15GB（下载） + ~15GB（缓存） = **30GB**

### 步骤 2: 配置 HuggingFace（可选）

```bash
# 如果需要访问私有模型或提高下载速度
huggingface-cli login

# 或设置镜像站点（中国用户）
export HF_ENDPOINT=https://hf-mirror.com
```

### 步骤 3: 运行下载脚本

```bash
# 进入项目目录
cd /home/ivan/HPPE

# 激活环境
conda activate hppe

# 查看支持的模型
python scripts/download_model.py --list-models

# 下载推荐模型（AWQ 4-bit）
python scripts/download_model.py \
  --quantization awq \
  --bits 4 \
  --verify
```

**预期输出**：
```
======================================================================
Qwen3 模型下载工具
======================================================================

模型名称: Qwen/Qwen2.5-7B-Instruct-AWQ
预计大小: 4.5 GB
描述: Qwen2.5 7B AWQ 4-bit 量化版

✓ 使用 AWQ 4-bit 量化
✓ 显存需求: ~4.5 GB
✓ 适合 RTX 3060 12GB  [推荐]

开始下载...

2025-10-14 12:00:00 - INFO - 模型: Qwen/Qwen2.5-7B-Instruct-AWQ
2025-10-14 12:00:00 - INFO - 开始下载模型...
2025-10-14 12:00:00 - INFO - 这可能需要较长时间（取决于网络速度和模型大小）
Downloading: 100%|████████████████████| 4.5G/4.5G [05:30<00:00, 13.6MB/s]
2025-10-14 12:05:30 - INFO - ✓ 模型下载完成！
2025-10-14 12:05:30 - INFO - 模型路径: /home/ivan/.cache/huggingface/hub/...
2025-10-14 12:05:30 - INFO - 验证模型文件...
2025-10-14 12:05:31 - INFO - ✓ 验证通过！找到 2 个权重文件

======================================================================
✓ 下载并验证成功！
======================================================================

下一步:

1. 激活 conda 环境:
   conda activate hppe

2. 启动 vLLM 服务:
   ./scripts/start_vllm.sh

3. 测试 LLM 引擎:
   PYTHONPATH=/home/ivan/HPPE/src python examples/llm_engine_example.py
```

### 步骤 4: 验证下载

```bash
# 方法 1: 使用下载脚本验证
python scripts/download_model.py --verify

# 方法 2: 手动检查文件
ls -lh ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct-AWQ/

# 方法 3: 使用 Python 验证
python -c "
from transformers import AutoTokenizer, AutoConfig
model_name = 'Qwen/Qwen2.5-7B-Instruct-AWQ'
try:
    config = AutoConfig.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    print(f'✓ 模型验证成功')
    print(f'  词表大小: {len(tokenizer)}')
    print(f'  隐藏层大小: {config.hidden_size}')
    print(f'  注意力头数: {config.num_attention_heads}')
except Exception as e:
    print(f'✗ 验证失败: {e}')
"
```

**预期输出**：
```
✓ 模型验证成功
  词表大小: 151851
  隐藏层大小: 3584
  注意力头数: 28
```

---

## 📂 模型存储位置

### 默认位置

HuggingFace 默认缓存目录：
```bash
~/.cache/huggingface/hub/
```

模型完整路径示例：
```
~/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct-AWQ/
├── snapshots/
│   └── <commit-hash>/
│       ├── config.json
│       ├── tokenizer.json
│       ├── tokenizer_config.json
│       ├── model.safetensors.index.json
│       ├── model-00001-of-00002.safetensors
│       └── model-00002-of-00002.safetensors
└── refs/
```

### 自定义位置

```bash
# 下载到指定目录
python scripts/download_model.py \
  --quantization awq \
  --output-dir /data/models

# 或设置环境变量
export HF_HOME=/data/huggingface
python scripts/download_model.py --quantization awq
```

---

## ⚙️ 配置 vLLM 使用模型

### 更新配置文件

编辑 `configs/llm_config.yaml`:

```yaml
model:
  name: "Qwen/Qwen2.5-7B-Instruct-AWQ"  # 修改为 AWQ 模型
  quantization:
    enabled: true
    method: "awq"
    bits: 4
  gpu:
    device_id: 0
    tensor_parallel_size: 1
    memory_utilization: 0.85
```

### 更新启动脚本

编辑 `scripts/start_vllm.sh`（如果需要）:

```bash
# 设置模型名称
MODEL_NAME=${MODEL_NAME:-"Qwen/Qwen2.5-7B-Instruct-AWQ"}

# 启动 vLLM
python -m vllm.entrypoints.openai.api_server \
  --model "$MODEL_NAME" \
  --quantization awq \
  --gpu-memory-utilization 0.85 \
  --max-model-len 4096 \
  --host 0.0.0.0 \
  --port 8000
```

---

## 🧪 测试模型

### 方法 1: 使用 curl

```bash
# 测试 chat completions API
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-7B-Instruct-AWQ",
    "messages": [
      {"role": "system", "content": "你是一个有帮助的AI助手。"},
      {"role": "user", "content": "你好，请介绍一下你自己。"}
    ],
    "temperature": 0.7,
    "max_tokens": 200
  }'
```

### 方法 2: 使用 Python SDK

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"
)

response = client.chat.completions.create(
    model="Qwen/Qwen2.5-7B-Instruct-AWQ",
    messages=[
        {"role": "system", "content": "你是一个有帮助的AI助手。"},
        {"role": "user", "content": "你好，请介绍一下你自己。"}
    ],
    temperature=0.7,
    max_tokens=200
)

print(response.choices[0].message.content)
```

### 方法 3: 使用 HPPE QwenEngine

```python
from hppe.engines.llm import QwenEngine

# 初始化引擎
engine = QwenEngine(
    model_name="Qwen/Qwen2.5-7B-Instruct-AWQ",
    base_url="http://localhost:8000/v1"
)

# 健康检查
if not engine.health_check():
    print("✗ vLLM 服务未启动")
    exit(1)

print("✓ vLLM 服务运行正常")

# 测试生成
response = engine.generate(
    prompt="你好，请介绍一下你自己。",
    system_prompt="你是一个有帮助的AI助手。",
    temperature=0.7,
    max_tokens=200
)

print(f"响应: {response}")
```

---

## 🔧 故障排查

### 问题 1: 下载速度慢

**原因**:
- 网络限制
- HuggingFace 服务器在国外

**解决方法**:
```bash
# 使用镜像站点（中国用户）
export HF_ENDPOINT=https://hf-mirror.com

# 重新下载
python scripts/download_model.py --quantization awq
```

### 问题 2: 磁盘空间不足

**检查空间**:
```bash
df -h /home/ivan
du -sh ~/.cache/huggingface/hub
```

**清理缓存**:
```bash
# 删除旧模型
rm -rf ~/.cache/huggingface/hub/models--*

# 或移动到其他磁盘
mv ~/.cache/huggingface/hub /data/huggingface/hub
ln -s /data/huggingface/hub ~/.cache/huggingface/hub
```

### 问题 3: 下载中断

**解决方法**:
```bash
# 下载脚本支持断点续传，直接重新运行
python scripts/download_model.py --quantization awq

# 或使用 huggingface-cli
huggingface-cli download Qwen/Qwen2.5-7B-Instruct-AWQ --resume-download
```

### 问题 4: 模型验证失败

**检查步骤**:
```bash
# 1. 检查模型文件
ls -lh ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct-AWQ/snapshots/*/

# 2. 检查必需文件
for file in config.json tokenizer.json tokenizer_config.json; do
  find ~/.cache/huggingface -name "$file" | head -1
done

# 3. 重新下载
python scripts/download_model.py --quantization awq --verify
```

### 问题 5: vLLM 无法加载模型

**原因**:
- 模型路径不正确
- 量化格式不匹配
- 显存不足

**检查**:
```bash
# 1. 确认模型路径
python -c "from transformers import AutoConfig; print(AutoConfig.from_pretrained('Qwen/Qwen2.5-7B-Instruct-AWQ'))"

# 2. 检查 GPU 状态
nvidia-smi

# 3. 测试加载
python -m vllm.entrypoints.openai.api_server \
  --model "Qwen/Qwen2.5-7B-Instruct-AWQ" \
  --quantization awq \
  --gpu-memory-utilization 0.85
```

---

## 📊 性能预期

### AWQ 4-bit 模型（推荐）

| 指标 | 数值 |
|------|------|
| 模型大小 | ~4.5GB |
| 显存占用 | ~5-6GB (包含 KV 缓存) |
| 首 token 延迟 | ~100-150ms |
| 后续 token 延迟 | ~20-30ms |
| 吞吐量 | ~30-40 tokens/s |
| 并发请求 | 8-10 |
| 质量损失 | <3% (相比 FP16) |

### FP16 标准模型

| 指标 | 数值 |
|------|------|
| 模型大小 | ~14GB |
| 显存占用 | ~14-16GB (⚠️ 超出 RTX 3060) |
| 首 token 延迟 | ~80-120ms |
| 后续 token 延迟 | ~15-25ms |
| 吞吐量 | ~40-50 tokens/s |
| 并发请求 | 6-8 (显存受限) |
| 质量 | 基准 |

---

## ✅ 验收标准

### 必需项

- [ ] 模型成功下载（AWQ 4-bit 或 FP16）
- [ ] 文件完整性验证通过
- [ ] 模型配置正确加载（config.json, tokenizer）
- [ ] vLLM 服务可成功启动
- [ ] 健康检查 API 返回 200
- [ ] Chat completions API 正常响应
- [ ] QwenEngine 可正常使用

### 性能验证

- [ ] 首 token 延迟 < 200ms
- [ ] 推理速度 > 20 tokens/s
- [ ] 显存占用 < 8GB (AWQ) 或 < 16GB (FP16)
- [ ] 可支持 5+ 并发请求

### 质量验证

- [ ] 基础对话能力正常
- [ ] 中文理解准确
- [ ] 指令遵循良好
- [ ] 输出格式符合预期（JSON, 列表等）

---

## 📈 下一步

完成模型下载后：

1. **启动 vLLM 服务** → 运行 `./scripts/start_vllm.sh`
2. **测试 LLM 引擎** → 运行 `examples/llm_engine_example.py`
3. **设计 PII Prompt 模板** → Story 2.3 准备工作
4. **创建测试数据集** → 用于 PII 检测评估

---

## 📚 参考资源

- [Qwen2.5 官方文档](https://github.com/QwenLM/Qwen2.5)
- [vLLM 文档](https://docs.vllm.ai/)
- [AWQ 量化论文](https://arxiv.org/abs/2306.00978)
- [HuggingFace Hub CLI](https://huggingface.co/docs/huggingface_hub/guides/cli)

---

**创建时间**: 2025-10-14
**状态**: 📖 准备文档完成，等待 vLLM 环境安装完成后执行下载
