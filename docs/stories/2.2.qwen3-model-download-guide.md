# Story 2.2: Qwen3 æ¨¡å‹é›†æˆ - ä¸‹è½½æŒ‡å—

## ğŸ“‹ æ–‡æ¡£ä¿¡æ¯

- **Story**: 2.2 Qwen3 æ¨¡å‹é›†æˆ
- **çŠ¶æ€**: ğŸ”„ å‡†å¤‡ä¸­
- **åˆ›å»ºæ—¶é—´**: 2025-10-14
- **å‰ç½®æ¡ä»¶**: Story 2.1 vLLM åŸºç¡€è®¾æ–½ï¼ˆâœ… å·²å®Œæˆï¼‰

---

## ğŸ¯ ç›®æ ‡

ä¸‹è½½å¹¶é…ç½® Qwen2.5-7B-Instruct æ¨¡å‹ï¼Œç”¨äºæœ¬åœ° PII ä¸Šä¸‹æ–‡æ£€æµ‹ã€‚

---

## ğŸ“Š æ¨¡å‹é€‰æ‹©æŒ‡å—

### æ”¯æŒçš„æ¨¡å‹ç‰ˆæœ¬

| ç‰ˆæœ¬ | å¤§å° | æ˜¾å­˜éœ€æ±‚ | æ¨ç†é€Ÿåº¦ | è´¨é‡ | æ¨èåœºæ™¯ |
|------|------|---------|---------|------|---------|
| **Qwen2.5-7B-Instruct (FP16)** | ~14GB | ~14GB | å¿« | æœ€é«˜ | å¤§æ˜¾å­˜ GPU (>16GB) |
| **Qwen2.5-7B-Instruct-AWQ** | ~4.5GB | ~6GB | å¾ˆå¿« | é«˜ | **RTX 3060 æ¨è** âœ… |
| **Qwen2.5-7B-Instruct-GPTQ-Int4** | ~4.5GB | ~6GB | å¿« | é«˜ | RTX 3060 å¤‡é€‰ |

### ç¡¬ä»¶åŒ¹é…

**ç”¨æˆ·é…ç½®**ï¼š
- 2 Ã— NVIDIA RTX 3060 (12GB VRAM)
- CUDA 13.0ï¼ˆå…¼å®¹ CUDA 12.1ï¼‰

**æ¨èé…ç½®**ï¼š
```yaml
æ¨¡å‹: Qwen2.5-7B-Instruct-AWQ (4-bit)
GPU: GPU 0 (å•å¡éƒ¨ç½²)
æ˜¾å­˜å ç”¨: ~5-6GB
å‰©ä½™æ˜¾å­˜: ~6GB (ç”¨äºæ¨ç†ç¼“å­˜)
å¹¶å‘è¯·æ±‚: 8-10
å»¶è¿Ÿ: 100-200ms
```

**åŸå› **ï¼š
- âœ… AWQ é‡åŒ–è´¨é‡ä¼˜äº GPTQï¼ˆæ›´æ¥è¿‘ FP16ï¼‰
- âœ… 4-bit é‡åŒ–æ˜¾å­˜éœ€æ±‚é€‚åˆ RTX 3060
- âœ… ä¿ç•™è¶³å¤Ÿæ˜¾å­˜ç”¨äº KV ç¼“å­˜å’Œå¹¶å‘
- âœ… æ¨ç†é€Ÿåº¦å¿«ï¼ˆä¼˜åŒ–çš„ CUDA kernelï¼‰

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³• 1: ä½¿ç”¨ä¸‹è½½è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# æ¿€æ´» conda ç¯å¢ƒ
conda activate hppe

# ä¸‹è½½ AWQ 4-bit é‡åŒ–æ¨¡å‹ï¼ˆæ¨èï¼‰
python scripts/download_model.py --quantization awq --bits 4

# æˆ–ä¸‹è½½æ ‡å‡† FP16 æ¨¡å‹
python scripts/download_model.py
```

### æ–¹æ³• 2: æ‰‹åŠ¨ä½¿ç”¨ huggingface-cli

```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate hppe

# å®‰è£… huggingface-hub
pip install -U huggingface-hub

# ä¸‹è½½ AWQ æ¨¡å‹
huggingface-cli download Qwen/Qwen2.5-7B-Instruct-AWQ \
  --resume-download \
  --local-dir ./models/Qwen2.5-7B-Instruct-AWQ

# æˆ–ä¸‹è½½æ ‡å‡†æ¨¡å‹
huggingface-cli download Qwen/Qwen2.5-7B-Instruct \
  --resume-download \
  --local-dir ./models/Qwen2.5-7B-Instruct
```

### æ–¹æ³• 3: ä½¿ç”¨ Python API

```python
from huggingface_hub import snapshot_download

# ä¸‹è½½ AWQ æ¨¡å‹
model_path = snapshot_download(
    repo_id="Qwen/Qwen2.5-7B-Instruct-AWQ",
    resume_download=True,
    cache_dir="./models"
)

print(f"æ¨¡å‹è·¯å¾„: {model_path}")
```

---

## ğŸ“¦ ä¸‹è½½è¯¦ç»†æ­¥éª¤

### æ­¥éª¤ 1: æ£€æŸ¥ç£ç›˜ç©ºé—´

```bash
# æ£€æŸ¥å¯ç”¨ç©ºé—´ï¼ˆéœ€è¦è‡³å°‘ 10GBï¼‰
df -h /home/ivan

# æŸ¥çœ‹ HuggingFace ç¼“å­˜ç›®å½•
du -sh ~/.cache/huggingface/hub 2>/dev/null || echo "ç¼“å­˜ç›®å½•ä¸å­˜åœ¨"
```

**æ‰€éœ€ç©ºé—´**ï¼š
- AWQ 4-bit: ~5GBï¼ˆä¸‹è½½ï¼‰ + ~5GBï¼ˆç¼“å­˜ï¼‰ = **10GB**
- FP16: ~15GBï¼ˆä¸‹è½½ï¼‰ + ~15GBï¼ˆç¼“å­˜ï¼‰ = **30GB**

### æ­¥éª¤ 2: é…ç½® HuggingFaceï¼ˆå¯é€‰ï¼‰

```bash
# å¦‚æœéœ€è¦è®¿é—®ç§æœ‰æ¨¡å‹æˆ–æé«˜ä¸‹è½½é€Ÿåº¦
huggingface-cli login

# æˆ–è®¾ç½®é•œåƒç«™ç‚¹ï¼ˆä¸­å›½ç”¨æˆ·ï¼‰
export HF_ENDPOINT=https://hf-mirror.com
```

### æ­¥éª¤ 3: è¿è¡Œä¸‹è½½è„šæœ¬

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /home/ivan/HPPE

# æ¿€æ´»ç¯å¢ƒ
conda activate hppe

# æŸ¥çœ‹æ”¯æŒçš„æ¨¡å‹
python scripts/download_model.py --list-models

# ä¸‹è½½æ¨èæ¨¡å‹ï¼ˆAWQ 4-bitï¼‰
python scripts/download_model.py \
  --quantization awq \
  --bits 4 \
  --verify
```

**é¢„æœŸè¾“å‡º**ï¼š
```
======================================================================
Qwen3 æ¨¡å‹ä¸‹è½½å·¥å…·
======================================================================

æ¨¡å‹åç§°: Qwen/Qwen2.5-7B-Instruct-AWQ
é¢„è®¡å¤§å°: 4.5 GB
æè¿°: Qwen2.5 7B AWQ 4-bit é‡åŒ–ç‰ˆ

âœ“ ä½¿ç”¨ AWQ 4-bit é‡åŒ–
âœ“ æ˜¾å­˜éœ€æ±‚: ~4.5 GB
âœ“ é€‚åˆ RTX 3060 12GB  [æ¨è]

å¼€å§‹ä¸‹è½½...

2025-10-14 12:00:00 - INFO - æ¨¡å‹: Qwen/Qwen2.5-7B-Instruct-AWQ
2025-10-14 12:00:00 - INFO - å¼€å§‹ä¸‹è½½æ¨¡å‹...
2025-10-14 12:00:00 - INFO - è¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆå–å†³äºç½‘ç»œé€Ÿåº¦å’Œæ¨¡å‹å¤§å°ï¼‰
Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.5G/4.5G [05:30<00:00, 13.6MB/s]
2025-10-14 12:05:30 - INFO - âœ“ æ¨¡å‹ä¸‹è½½å®Œæˆï¼
2025-10-14 12:05:30 - INFO - æ¨¡å‹è·¯å¾„: /home/ivan/.cache/huggingface/hub/...
2025-10-14 12:05:30 - INFO - éªŒè¯æ¨¡å‹æ–‡ä»¶...
2025-10-14 12:05:31 - INFO - âœ“ éªŒè¯é€šè¿‡ï¼æ‰¾åˆ° 2 ä¸ªæƒé‡æ–‡ä»¶

======================================================================
âœ“ ä¸‹è½½å¹¶éªŒè¯æˆåŠŸï¼
======================================================================

ä¸‹ä¸€æ­¥:

1. æ¿€æ´» conda ç¯å¢ƒ:
   conda activate hppe

2. å¯åŠ¨ vLLM æœåŠ¡:
   ./scripts/start_vllm.sh

3. æµ‹è¯• LLM å¼•æ“:
   PYTHONPATH=/home/ivan/HPPE/src python examples/llm_engine_example.py
```

### æ­¥éª¤ 4: éªŒè¯ä¸‹è½½

```bash
# æ–¹æ³• 1: ä½¿ç”¨ä¸‹è½½è„šæœ¬éªŒè¯
python scripts/download_model.py --verify

# æ–¹æ³• 2: æ‰‹åŠ¨æ£€æŸ¥æ–‡ä»¶
ls -lh ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct-AWQ/

# æ–¹æ³• 3: ä½¿ç”¨ Python éªŒè¯
python -c "
from transformers import AutoTokenizer, AutoConfig
model_name = 'Qwen/Qwen2.5-7B-Instruct-AWQ'
try:
    config = AutoConfig.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    print(f'âœ“ æ¨¡å‹éªŒè¯æˆåŠŸ')
    print(f'  è¯è¡¨å¤§å°: {len(tokenizer)}')
    print(f'  éšè—å±‚å¤§å°: {config.hidden_size}')
    print(f'  æ³¨æ„åŠ›å¤´æ•°: {config.num_attention_heads}')
except Exception as e:
    print(f'âœ— éªŒè¯å¤±è´¥: {e}')
"
```

**é¢„æœŸè¾“å‡º**ï¼š
```
âœ“ æ¨¡å‹éªŒè¯æˆåŠŸ
  è¯è¡¨å¤§å°: 151851
  éšè—å±‚å¤§å°: 3584
  æ³¨æ„åŠ›å¤´æ•°: 28
```

---

## ğŸ“‚ æ¨¡å‹å­˜å‚¨ä½ç½®

### é»˜è®¤ä½ç½®

HuggingFace é»˜è®¤ç¼“å­˜ç›®å½•ï¼š
```bash
~/.cache/huggingface/hub/
```

æ¨¡å‹å®Œæ•´è·¯å¾„ç¤ºä¾‹ï¼š
```
~/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct-AWQ/
â”œâ”€â”€ snapshots/
â”‚   â””â”€â”€ <commit-hash>/
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ tokenizer.json
â”‚       â”œâ”€â”€ tokenizer_config.json
â”‚       â”œâ”€â”€ model.safetensors.index.json
â”‚       â”œâ”€â”€ model-00001-of-00002.safetensors
â”‚       â””â”€â”€ model-00002-of-00002.safetensors
â””â”€â”€ refs/
```

### è‡ªå®šä¹‰ä½ç½®

```bash
# ä¸‹è½½åˆ°æŒ‡å®šç›®å½•
python scripts/download_model.py \
  --quantization awq \
  --output-dir /data/models

# æˆ–è®¾ç½®ç¯å¢ƒå˜é‡
export HF_HOME=/data/huggingface
python scripts/download_model.py --quantization awq
```

---

## âš™ï¸ é…ç½® vLLM ä½¿ç”¨æ¨¡å‹

### æ›´æ–°é…ç½®æ–‡ä»¶

ç¼–è¾‘ `configs/llm_config.yaml`:

```yaml
model:
  name: "Qwen/Qwen2.5-7B-Instruct-AWQ"  # ä¿®æ”¹ä¸º AWQ æ¨¡å‹
  quantization:
    enabled: true
    method: "awq"
    bits: 4
  gpu:
    device_id: 0
    tensor_parallel_size: 1
    memory_utilization: 0.85
```

### æ›´æ–°å¯åŠ¨è„šæœ¬

ç¼–è¾‘ `scripts/start_vllm.sh`ï¼ˆå¦‚æœéœ€è¦ï¼‰:

```bash
# è®¾ç½®æ¨¡å‹åç§°
MODEL_NAME=${MODEL_NAME:-"Qwen/Qwen2.5-7B-Instruct-AWQ"}

# å¯åŠ¨ vLLM
python -m vllm.entrypoints.openai.api_server \
  --model "$MODEL_NAME" \
  --quantization awq \
  --gpu-memory-utilization 0.85 \
  --max-model-len 4096 \
  --host 0.0.0.0 \
  --port 8000
```

---

## ğŸ§ª æµ‹è¯•æ¨¡å‹

### æ–¹æ³• 1: ä½¿ç”¨ curl

```bash
# æµ‹è¯• chat completions API
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-7B-Instruct-AWQ",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
      {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
    ],
    "temperature": 0.7,
    "max_tokens": 200
  }'
```

### æ–¹æ³• 2: ä½¿ç”¨ Python SDK

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"
)

response = client.chat.completions.create(
    model="Qwen/Qwen2.5-7B-Instruct-AWQ",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
    ],
    temperature=0.7,
    max_tokens=200
)

print(response.choices[0].message.content)
```

### æ–¹æ³• 3: ä½¿ç”¨ HPPE QwenEngine

```python
from hppe.engines.llm import QwenEngine

# åˆå§‹åŒ–å¼•æ“
engine = QwenEngine(
    model_name="Qwen/Qwen2.5-7B-Instruct-AWQ",
    base_url="http://localhost:8000/v1"
)

# å¥åº·æ£€æŸ¥
if not engine.health_check():
    print("âœ— vLLM æœåŠ¡æœªå¯åŠ¨")
    exit(1)

print("âœ“ vLLM æœåŠ¡è¿è¡Œæ­£å¸¸")

# æµ‹è¯•ç”Ÿæˆ
response = engine.generate(
    prompt="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
    system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚",
    temperature=0.7,
    max_tokens=200
)

print(f"å“åº”: {response}")
```

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: ä¸‹è½½é€Ÿåº¦æ…¢

**åŸå› **:
- ç½‘ç»œé™åˆ¶
- HuggingFace æœåŠ¡å™¨åœ¨å›½å¤–

**è§£å†³æ–¹æ³•**:
```bash
# ä½¿ç”¨é•œåƒç«™ç‚¹ï¼ˆä¸­å›½ç”¨æˆ·ï¼‰
export HF_ENDPOINT=https://hf-mirror.com

# é‡æ–°ä¸‹è½½
python scripts/download_model.py --quantization awq
```

### é—®é¢˜ 2: ç£ç›˜ç©ºé—´ä¸è¶³

**æ£€æŸ¥ç©ºé—´**:
```bash
df -h /home/ivan
du -sh ~/.cache/huggingface/hub
```

**æ¸…ç†ç¼“å­˜**:
```bash
# åˆ é™¤æ—§æ¨¡å‹
rm -rf ~/.cache/huggingface/hub/models--*

# æˆ–ç§»åŠ¨åˆ°å…¶ä»–ç£ç›˜
mv ~/.cache/huggingface/hub /data/huggingface/hub
ln -s /data/huggingface/hub ~/.cache/huggingface/hub
```

### é—®é¢˜ 3: ä¸‹è½½ä¸­æ–­

**è§£å†³æ–¹æ³•**:
```bash
# ä¸‹è½½è„šæœ¬æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œç›´æ¥é‡æ–°è¿è¡Œ
python scripts/download_model.py --quantization awq

# æˆ–ä½¿ç”¨ huggingface-cli
huggingface-cli download Qwen/Qwen2.5-7B-Instruct-AWQ --resume-download
```

### é—®é¢˜ 4: æ¨¡å‹éªŒè¯å¤±è´¥

**æ£€æŸ¥æ­¥éª¤**:
```bash
# 1. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
ls -lh ~/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct-AWQ/snapshots/*/

# 2. æ£€æŸ¥å¿…éœ€æ–‡ä»¶
for file in config.json tokenizer.json tokenizer_config.json; do
  find ~/.cache/huggingface -name "$file" | head -1
done

# 3. é‡æ–°ä¸‹è½½
python scripts/download_model.py --quantization awq --verify
```

### é—®é¢˜ 5: vLLM æ— æ³•åŠ è½½æ¨¡å‹

**åŸå› **:
- æ¨¡å‹è·¯å¾„ä¸æ­£ç¡®
- é‡åŒ–æ ¼å¼ä¸åŒ¹é…
- æ˜¾å­˜ä¸è¶³

**æ£€æŸ¥**:
```bash
# 1. ç¡®è®¤æ¨¡å‹è·¯å¾„
python -c "from transformers import AutoConfig; print(AutoConfig.from_pretrained('Qwen/Qwen2.5-7B-Instruct-AWQ'))"

# 2. æ£€æŸ¥ GPU çŠ¶æ€
nvidia-smi

# 3. æµ‹è¯•åŠ è½½
python -m vllm.entrypoints.openai.api_server \
  --model "Qwen/Qwen2.5-7B-Instruct-AWQ" \
  --quantization awq \
  --gpu-memory-utilization 0.85
```

---

## ğŸ“Š æ€§èƒ½é¢„æœŸ

### AWQ 4-bit æ¨¡å‹ï¼ˆæ¨èï¼‰

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ¨¡å‹å¤§å° | ~4.5GB |
| æ˜¾å­˜å ç”¨ | ~5-6GB (åŒ…å« KV ç¼“å­˜) |
| é¦– token å»¶è¿Ÿ | ~100-150ms |
| åç»­ token å»¶è¿Ÿ | ~20-30ms |
| ååé‡ | ~30-40 tokens/s |
| å¹¶å‘è¯·æ±‚ | 8-10 |
| è´¨é‡æŸå¤± | <3% (ç›¸æ¯” FP16) |

### FP16 æ ‡å‡†æ¨¡å‹

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æ¨¡å‹å¤§å° | ~14GB |
| æ˜¾å­˜å ç”¨ | ~14-16GB (âš ï¸ è¶…å‡º RTX 3060) |
| é¦– token å»¶è¿Ÿ | ~80-120ms |
| åç»­ token å»¶è¿Ÿ | ~15-25ms |
| ååé‡ | ~40-50 tokens/s |
| å¹¶å‘è¯·æ±‚ | 6-8 (æ˜¾å­˜å—é™) |
| è´¨é‡ | åŸºå‡† |

---

## âœ… éªŒæ”¶æ ‡å‡†

### å¿…éœ€é¡¹

- [ ] æ¨¡å‹æˆåŠŸä¸‹è½½ï¼ˆAWQ 4-bit æˆ– FP16ï¼‰
- [ ] æ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡
- [ ] æ¨¡å‹é…ç½®æ­£ç¡®åŠ è½½ï¼ˆconfig.json, tokenizerï¼‰
- [ ] vLLM æœåŠ¡å¯æˆåŠŸå¯åŠ¨
- [ ] å¥åº·æ£€æŸ¥ API è¿”å› 200
- [ ] Chat completions API æ­£å¸¸å“åº”
- [ ] QwenEngine å¯æ­£å¸¸ä½¿ç”¨

### æ€§èƒ½éªŒè¯

- [ ] é¦– token å»¶è¿Ÿ < 200ms
- [ ] æ¨ç†é€Ÿåº¦ > 20 tokens/s
- [ ] æ˜¾å­˜å ç”¨ < 8GB (AWQ) æˆ– < 16GB (FP16)
- [ ] å¯æ”¯æŒ 5+ å¹¶å‘è¯·æ±‚

### è´¨é‡éªŒè¯

- [ ] åŸºç¡€å¯¹è¯èƒ½åŠ›æ­£å¸¸
- [ ] ä¸­æ–‡ç†è§£å‡†ç¡®
- [ ] æŒ‡ä»¤éµå¾ªè‰¯å¥½
- [ ] è¾“å‡ºæ ¼å¼ç¬¦åˆé¢„æœŸï¼ˆJSON, åˆ—è¡¨ç­‰ï¼‰

---

## ğŸ“ˆ ä¸‹ä¸€æ­¥

å®Œæˆæ¨¡å‹ä¸‹è½½åï¼š

1. **å¯åŠ¨ vLLM æœåŠ¡** â†’ è¿è¡Œ `./scripts/start_vllm.sh`
2. **æµ‹è¯• LLM å¼•æ“** â†’ è¿è¡Œ `examples/llm_engine_example.py`
3. **è®¾è®¡ PII Prompt æ¨¡æ¿** â†’ Story 2.3 å‡†å¤‡å·¥ä½œ
4. **åˆ›å»ºæµ‹è¯•æ•°æ®é›†** â†’ ç”¨äº PII æ£€æµ‹è¯„ä¼°

---

## ğŸ“š å‚è€ƒèµ„æº

- [Qwen2.5 å®˜æ–¹æ–‡æ¡£](https://github.com/QwenLM/Qwen2.5)
- [vLLM æ–‡æ¡£](https://docs.vllm.ai/)
- [AWQ é‡åŒ–è®ºæ–‡](https://arxiv.org/abs/2306.00978)
- [HuggingFace Hub CLI](https://huggingface.co/docs/huggingface_hub/guides/cli)

---

**åˆ›å»ºæ—¶é—´**: 2025-10-14
**çŠ¶æ€**: ğŸ“– å‡†å¤‡æ–‡æ¡£å®Œæˆï¼Œç­‰å¾… vLLM ç¯å¢ƒå®‰è£…å®Œæˆåæ‰§è¡Œä¸‹è½½
